
@inproceedings{UpperBoundResolution,
  title = {An {{Upper Bound}} for {{Resolution Size}}: {{Characterization}} of {{Tractable SAT Instances}}}
}

@inproceedings{ImanishiUpperBoundResolution2017,
  series = {Lecture Notes in Computer Science},
  title = {An {{Upper Bound}} for {{Resolution Size}}: {{Characterization}} of {{Tractable SAT Instances}}},
  isbn = {978-3-319-53924-9 978-3-319-53925-6},
  shorttitle = {An {{Upper Bound}} for {{Resolution Size}}},
  doi = {10.1007/978-3-319-53925-6_28},
  abstract = {We show the first upper bound for resolution size of a SAT instance by pathwidth of its incidence graph. Namely, we prove that if an incidence graph of an unsatisfiable CNF formula has pathwidth pwpw\{$\backslash$mathrm \{pw\}\}, the formula can be refuted by a resolution proof with at most O${_\ast}$(3pw)O${_\ast}$(3pw)O\^*(3\^\{$\backslash$mathrm \{pw\}\}) clauses. It is known that modern practical SAT-solvers run efficiently for instances which have small and narrow resolution refutations. Resolution size is one of the parameters which make SAT tractable, whereas it is shown that even linearly approximating the resolution size is NP-hard. In contrast, computing graph based parameters such as treewidth or pathwidth is fixed-parameter tractable, and also efficient FPT algorithms for SAT of bounded such parameters are widely researched. However, few explicit connection between these parameters and resolutions or SAT-solvers are known. In this paper, we provide an FPT algorithm for SAT on path decomposition of its incidence graph. The algorithm can construct resolution refutations for unsatisfiable formulas, and analyzing the size of constructed proof gives the new bound.},
  language = {en},
  booktitle = {{{WALCOM}}: {{Algorithms}} and {{Computation}}},
  publisher = {{Springer, Cham}},
  author = {Imanishi, Kensuke},
  month = mar,
  year = {2017},
  pages = {359-369},
  file = {/Users/billlai/Folder/Papers/An Upper Bound for Resolution Size Characterization of Tractable SAT Instances.pdf;/Users/billlai/Zotero/storage/J3YU3WVP/Imanishi - 2017 - An Upper Bound for Resolution Size Characterizati.pdf;/Users/billlai/Zotero/storage/3V4T2VT2/978-3-319-53925-6_28.html}
}

@inproceedings{BeameUnderstandingPowerClause2003,
  address = {San Francisco, CA, USA},
  series = {IJCAI'03},
  title = {Understanding the {{Power}} of {{Clause Learning}}},
  booktitle = {Proceedings of the 18th {{International Joint Conference}} on {{Artificial Intelligence}}},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  author = {Beame, Paul and Kautz, Henry and Sabharwal, Ashish},
  year = {2003},
  pages = {1194--1201},
  file = {/Users/billlai/Folder/Papers/Understanding the power of clause learning.pdf;/Users/billlai/Folder/Papers/Understanding the power of clause learning.ppt}
}

@article{Ben-SassonShortProofsAre2001,
  title = {Short {{Proofs Are Narrow}}\textemdash{{Resolution Made Simple}}},
  volume = {48},
  issn = {0004-5411},
  doi = {10.1145/375827.375835},
  abstract = {The widthof a Resolution proof is defined to be the maximal number of literals in any clause of the proof. In this paper, we relate proof width to proof length (=size), in both general Resolution, and its tree-like variant. The following consequences of these relations reveal width as a crucial ``resource'' of Resolution proofs.
In one direction, the relations allow us to give simple, unified proofs for almost all known exponential lower bounds on size of resolution proofs, as well as several interesting new ones. They all follow from width lower bounds, and we show how these follow from natural expansion property of clauses of the input tautology.
In the other direction, the width-size relations naturally suggest a  simple dynamic programming procedure for automated theorem proving\textemdash{}one which simply searches for small width proofs. This relation guarantees that the runnuing time (and thus the size of the produced proof) is at most quasi-polynomial in the smallest tree-like proof. This algorithm is never much worse than any of the recursive automated provers (such as DLL) used in practice. In contrast, we present a family of tautologies on which it is exponentially faster.},
  number = {2},
  journal = {J. ACM},
  author = {{Ben-Sasson}, Eli and Wigderson, Avi},
  month = mar,
  year = {2001},
  pages = {149--169}
}

@article{Ben-Sasson*OptimalSeparationTreeLike2004,
  title = {Near {{Optimal Separation Of Tree}}-{{Like And General Resolution}}},
  volume = {24},
  issn = {0209-9683, 1439-6912},
  doi = {10.1007/s00493-004-0036-5},
  abstract = {We present the best known separation between tree-like and general resolution, improving on the recent exp(n$\in$) separation of [2]. This is done by constructing a natural family of contradictions, of size n, that have O(n)-size resolution refutations, but only exp($\Omega$(n/log n))- size tree-like refutations. This result implies that the most commonly used automated theorem procedures, which produce tree-like resolution refutations, will perform badly on some inputs, while other simple procedures, that produce general resolution refutations, will have polynomial run-time on these very same inputs. We show, furthermore that the gap we present is nearly optimal. Specifically, if S (ST) is the minimal size of a (tree-like) refutation, we prove that ST = exp(O(S log log S/log S)).},
  language = {en},
  number = {4},
  journal = {Combinatorica},
  author = {{Ben-Sasson*}, Eli and Impagliazzo\textdagger, Russell and Wigderson\textdaggerdbl, Avi},
  month = sep,
  year = {2004},
  pages = {585-603},
  file = {/Users/billlai/Folder/Papers/NEAR OPTIMAL SEPARATION OF TREE-LIKE AND GENERAL RESOLUTION.pdf;/Users/billlai/Zotero/storage/FJJJKEHM/Ben-Sasson 等。 - 2004 - Near Optimal Separation Of Tree-Like And General R.pdf}
}

@article{GoerdtRegularResolutionUnrestricted1993,
  title = {Regular {{Resolution Versus Unrestricted Resolution}}},
  volume = {22},
  issn = {0097-5397},
  doi = {10.1137/0222044},
  number = {4},
  journal = {SIAM J. Comput.},
  author = {Goerdt, Andreas},
  month = aug,
  year = {1993},
  keywords = {propositional logic,regular resolution,resolution theorem proving},
  pages = {661--683}
}

@inproceedings{PipatsrisawatPowerClauseLearningSAT2009,
  series = {Lecture Notes in Computer Science},
  title = {On the {{Power}} of {{Clause}}-{{Learning SAT Solvers}} with {{Restarts}}},
  isbn = {978-3-642-04243-0 978-3-642-04244-7},
  doi = {10.1007/978-3-642-04244-7_51},
  abstract = {In this work, we improve on existing work that studied the relationship between the proof system of modern SAT solvers and general resolution. Previous contributions such as those by Beame et al (2004), Hertel et al (2008), and Buss et al (2008) demonstrated that variations on modern clause-learning SAT solvers were as powerful as general resolution. However, the models used in these studies required either extra degrees of non-determinism or a preprocessing step that are not utilized by any state-of-the-art SAT solvers in practice. In this paper, we prove that modern SAT solvers that learn asserting clauses indeed p-simulate general resolution without the need for any additional techniques.},
  language = {en},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}} - {{CP}} 2009},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Pipatsrisawat, Knot and Darwiche, Adnan},
  month = sep,
  year = {2009},
  pages = {654-668},
  file = {/Users/billlai/Folder/Papers/On the Power of Clause-Learning SAT Solvers with Restarts.pdf;/Users/billlai/Zotero/storage/IUQ84BJI/10.html}
}

@inproceedings{AudemardRestrictionExtendedResolution2010,
  address = {Atlanta, Georgia},
  series = {AAAI'10},
  title = {A {{Restriction}} of {{Extended Resolution}} for {{Clause Learning Sat Solvers}}},
  abstract = {Modern complete SAT solvers almost uniformly implement variations of the clause learning framework introduced by Grasp and Chaff. The success of these solvers has been theoretically explained by showing that the clause learning framework is an implementation of a proof system which is as poweful as resolution. However, exponential lower bounds are known for resolution, which suggests that significant advances in SAT solving must come from implementations of more powerful proof systems. We present a clause learning SAT solver that uses extended resolution. It is based on a restriction of the application of the extension rule. This solver outperforms existing solvers on application instances from recent SAT competitions as well as on instances that are provably hard for resolution, such as XOR-SAT instances.},
  booktitle = {Proceedings of the {{Twenty}}-{{Fourth AAAI Conference}} on {{Artificial Intelligence}}},
  publisher = {{AAAI Press}},
  author = {Audemard, Gilles and Katsirelos, George and Simon, Laurent},
  year = {2010},
  pages = {15--20},
  file = {/Users/billlai/Folder/Papers/A Restriction of Extended Resolution for Clause Learning SAT Solvers.pdf}
}

@misc{KolokolovaComplexityExpanderBasedReasoning,
  title = {Complexity of {{Expander}}-{{Based Reasoning}} and the {{Power}} of {{Monotone Proofs}} | {{Simons Institute}} for the {{Theory}} of {{Computing}}},
  howpublished = {https://simons.berkeley.edu/talks/antonina-kolokolova-12-15-2016},
  author = {Kolokolova, Antonina},
  keywords = {RTR},
  file = {/Users/billlai/Folder/Papers/Power of reasoning over richer domains - Complexity of Expander-Based Reasoning and the Power of Monotone Proofs.pdf;/Users/billlai/Zotero/storage/A8EQPKTA/antonina-kolokolova-12-15-2016.html}
}

@misc{TalkProofComplexity,
  title = {Talk: {{Proof Complexity Mini}}-Tutorial},
  howpublished = {https://www.math.ucsd.edu/\textasciitilde{}sbuss/ResearchWeb/Banff2014/},
  keywords = {RTR},
  file = {/Users/billlai/Folder/Papers/Mini-tutorial on proof complexity.pdf;/Users/billlai/Zotero/storage/9T2H9WV3/Banff2014.html}
}

@book{CookLogicalfoundationsproof,
  title = {Logical Foundations of Proof Complexity},
  abstract = {This book treats bounded arithmetic and propositional proof complexity from the point of view of computational complexity. The first seven chapters include the necessary logical background for the material and are suitable for a graduate course. Associated with each of many complexity classes are both a two-sorted predicate calculus theory, with induction restricted to concepts in the class, and a propositional proof system. The complexity classes range from AC0 for the weakest theory up to the polynomial hierarchy. Each bounded theorem in a theory translates into a family of (quantified) propositional tautologies with polynomial size proofs in the corresponding proof system. The theory proves the soundness of the associated proof system. The result is a uniform treatment of many systems in the literature, including Buss's theories for the polynomial hierarchy and many disparate systems for complexity classes such as AC0, AC0(m), TC0, NC1, L, NL, NC, and P.
Read more at http://www.cambridge.org/cn/academic/subjects/mathematics/logic-categories-and-sets/logical-foundations-proof-complexity\#pS8kF092DKSSYAK8.99},
  author = {Cook, Stephen and Phuong, Nguyen},
  file = {/Users/billlai/Folder/Books/SAT/Logical Foundations of Proof Complexity.pdf;/Users/billlai/Zotero/storage/GGFRVVBL/logical-foundations-proof-complexity.html}
}

@misc{ConsistencySearchExtended,
  title = {Consistency {{Search}} ({{Extended}}) {{Frege}}},
  abstract = {We study consistency search problems for Frege and extended Frege proofs, namely the NP search problems of finding syntactic errors in Frege and extended Frege proofs of contradictions. The input is a polynomial time function, or an oracle, describing a proof of a contradiction; the output is the location of a syntactic error in the proof. The consistency search problems for Frege and extended Frege systems are shown to be many-one complete for the provably total NP search problems of the second order bounded arithmetic theories U\^1\_2 and V\^1\_2, respectively.},
  howpublished = {https://www.math.ucsd.edu/\textasciitilde{}sbuss/ResearchWeb/NPsearchFrege/},
  file = {/Users/billlai/Zotero/storage/IYNB9X65/NPsearchFrege.html}
}

@misc{BussPropositionalProofsFrege2015,
  title = {Propositional {{Proofs}} in {{Frege}} and {{Extended Frege Systems}} ({{Abstract}})},
  abstract = {We discuss recent results on the propositional proof complexity of Frege proof systems, including some recently discovered quasipolynomial size proofs for the pigeonhole principle and the Kneser-Lov$\backslash$'asz theorem. These are closely related to formalizability in bounded arithmetic.},
  howpublished = {http://link.springer.com/10.1007/978-3-319-20297-6\_1},
  journal = {Computer Science -- Theory and Applications},
  author = {Buss, Sam},
  collaborator = {Beklemishev, Lev D. and Musatov, Daniil V.},
  year = {2015},
  file = {/Users/billlai/Folder/Papers/Propositional Proofs in Frege and Extended Frege Systems (Abstract).pdf},
  doi = {10.1007/978-3-319-20297-6_1}
}

@misc{ProofSystemsSAT,
  title = {Proof {{Systems SAT Solvers}}},
  howpublished = {https://www.math.ucsd.edu/\textasciitilde{}sbuss/ResearchWeb/Dagstuhl\_2015/},
  file = {/Users/billlai/Folder/Papers/Proof Complexity & Complexity of SAT Solvers.pdf;/Users/billlai/Zotero/storage/7QKVN2KX/Dagstuhl_2015.html}
}

@misc{BeyersdorffProofComplexityOlaf2013,
  title = {Proof {{Complexity}} by {{Olaf Beyersdorff}}},
  abstract = {Tour of proof systems and the relation to other aspects},
  author = {Beyersdorff, Olaf},
  month = jun,
  year = {2013},
  file = {/Users/billlai/Folder/Papers/Proof Complexity - Beyersdorff.pdf}
}

@article{RazborovResolutionLowerBounds2003,
  title = {Resolution {{Lower Bounds}} for the {{Weak Functional Pigeonhole Principle}}},
  volume = {303},
  issn = {0304-3975},
  doi = {10.1016/S0304-3975(02)00453-X},
  abstract = {We show that every resolution proof of the functional version FPHPnm of the pigeonhole principle (in which one pigeon may not split between several holes) must have size exp($\Omega$(n/(log m)2)). This implies an exp($\Omega$(n1/3)) bound when the number of pigeons m is arbitrary.},
  number = {1},
  journal = {Theor. Comput. Sci.},
  author = {Razborov, Alexander A.},
  month = jun,
  year = {2003},
  pages = {233--243},
  file = {/Users/billlai/Folder/Papers/Resolution lower bounds for the weak functional pigeonhole principle.pdf}
}

@article{KrajicekPropositionalproofsystems1989,
  title = {Propositional Proof Systems, the Consistency of First Order Theories and the Complexity of Computations},
  volume = {54},
  issn = {0022-4812, 1943-5886},
  doi = {10.2307/2274765},
  abstract = {AbstractWe consider the problem about the length of proofs of the sentences  saying that there is no proof of contradiction in S whose length is $<$ n. We show the relation of this problem to some problems about propositional proof systems.},
  number = {3},
  journal = {The Journal of Symbolic Logic},
  author = {Kraj{\'\i}{\v c}ek, Jan and Pudl{\'a}k, Pavel},
  month = sep,
  year = {1989},
  pages = {1063-1079},
  file = {/Users/billlai/Folder/Papers/Propositional proof systems, the consistency of first order theories and the complexity of computations.pdf;/Users/billlai/Zotero/storage/5XIRFEU5/2E0129971839631F3AE97FFCC21395E7.html}
}

@article{UrquhartComplexityPropositionalProofs1995,
  title = {The {{Complexity}} of {{Propositional Proofs}}},
  volume = {1},
  issn = {1079-8986},
  doi = {10.2307/421131},
  number = {4},
  journal = {The Bulletin of Symbolic Logic},
  author = {Urquhart, Alasdair},
  year = {1995},
  pages = {425-467},
  file = {/Users/billlai/Folder/Papers/The complexity of propositional proofs.pdf}
}

@article{AjtaicomplexityPigeonholePrinciple1994,
  title = {The Complexity of the {{Pigeonhole Principle}}},
  volume = {14},
  issn = {0209-9683, 1439-6912},
  doi = {10.1007/BF01302964},
  abstract = {The Pigeonhole Principle forn is the statement that there is no one-to-one function between a set of sizen and a set of sizen-1. This statement can be formulated as an unlimited fan-in constant depth polynomial size Boolean formulaPHPn inn(n-1) variables. We may think that the truth-value of the variablexi,j will be true iff the function maps thei-th element of the first set to thej-th element of the second (see Cook and Rechkow [5]).PHPn can be proved in the propositional calculus. That is, a sequence of Boolean formulae can be given so that each one is either an axiom of the propositional calculus or a consequence of some of the previous ones according to an inference rule of the propositional calculus, and the last one isPHPn. Our main result is that the Pigeonhole Principle cannot be proved this way, if the size of the proof (the total number or symbols of the formulae in the sequence) is polynomial inn and each formula is constant depth (unlimited fan-in), polynomial size and contains only the variables ofPHPn.},
  language = {en},
  number = {4},
  journal = {Combinatorica},
  author = {Ajtai, M.},
  month = dec,
  year = {1994},
  pages = {417-433},
  file = {/Users/billlai/Folder/Papers/The complexity of the pigeonhole principle.pdf;/Users/billlai/Zotero/storage/2WKU4DKR/Ajtai - 1994 - The complexity of the Pigeonhole Principle.pdf;/Users/billlai/Zotero/storage/86SXXKF9/BF01302964.html}
}

@article{KrajicekExponentialLowerBound1995,
  title = {An {{Exponential Lower Bound}} to the {{Size}} of {{Bounded Depth Frege Proofs}} of the {{Pigeonhole Principle}}},
  volume = {7},
  issn = {1042-9832},
  doi = {10.1002/rsa.3240070103},
  number = {1},
  journal = {Random Struct. Algorithms},
  author = {Kraj{\'\i}{\v c}ek, Jan and Pudl{\'a}k, Pavel and Woods, Alan},
  month = aug,
  year = {1995},
  pages = {15--39},
  file = {/Users/billlai/Folder/Papers/An exponential lower bound to the size of bounded depth Frege proofs of the pigeonhole principle.pdf}
}

@article{PitassiExponentialLowerBounds1993,
  title = {Exponential {{Lower Bounds}} for the {{Pigeonhole Principle}}},
  volume = {3},
  issn = {1016-3328},
  doi = {10.1007/BF01200117},
  number = {2},
  journal = {Comput. Complex.},
  author = {Pitassi, Toniann and Beame, Paul and Impagliazzo, Russell},
  month = apr,
  year = {1993},
  keywords = {complexity of propositional proof systems,lower bounds},
  pages = {97--140},
  file = {/Users/billlai/Folder/Papers/Exponential lower bounds for the pigeonhole principle.pdf}
}

@book{KrajicekLowerBoundsSize1994,
  title = {Lower {{Bounds}} to the {{Size}} of {{Constant}}-{{Depth Propositional Proofs}}},
  abstract = {1  LK is a natural modification of Gentzen sequent calculus for propositional logic with connectives : and  V  ;  W  (both of unbounded arity). Then for every d  0 and n  2, there is a set T  d n of depth d sequents of total size O(n  3+d  ) which are refutable in LK by depth d + 1 proof of size exp(O(log  2  n)) but such that every depth d refutation must have the size at least exp(n$\backslash$Omega$\backslash$Gamma21 ). The sets T  d n express a weaker form of the pigeonhole principle. It is a fundamental problem of mathematical logic and complexity theory whether there exists a proof system for propositional logic in which every tautology has a short proof, where the length (equivalently the size) of a proof is measured essentially by the total number of symbols in it and short means polynomial in the length of the tautology. Equivalently one can ask whether for every theory T there is another theory S (both first order and reasonably axiomatized, e.g. by schemes) having the property that if a statement...},
  author = {Kraj{\'\i}cek, Jan},
  year = {1994},
  file = {/Users/billlai/Folder/Papers/Lower bounds to the size of constant-depth propositional proofs.pdf;/Users/billlai/Zotero/storage/E5KRY5TZ/Krajícek - 1994 - Lower Bounds to the Size of Constant-Depth Proposi.pdf;/Users/billlai/Zotero/storage/39863NRN/summary.html}
}

@book{KrajicekBoundedArithmeticPropositional1995,
  address = {New York, NY, USA},
  title = {Bounded {{Arithmetic}}, {{Propositional Logic}}, and {{Complexity Theory}}},
  isbn = {978-0-521-45205-2},
  publisher = {{Cambridge University Press}},
  author = {Kraj{\'\i}{\v c}ek, Jan},
  year = {1995},
  file = {/Users/billlai/Folder/Books/SAT/Bounded Arithmetic, Propositional Logic and Complexity Theory.pdf}
}

@misc{NineteenthCenturyClouds,
  title = {Nineteenth {{Century Clouds}} over the {{Dynamical Theory}}... - {{Google}} 学术搜索},
  howpublished = {https://www.ndtsg.com/scholar?hl=zh-CN\&as\_sdt=0\%2C5\&q=+Nineteenth+Century+Clouds+over+the+Dynamical+Theory+of+Heat+and+Light\&btnG=},
  file = {/Users/billlai/Zotero/storage/HCT9NXRQ/scholar.html}
}

@misc{Nineteenthcenturyclouds,
  title = {I. {{Nineteenth}} Century Clouds over the Dynamical Theory of Heat and Light: {{The London}}, {{Edinburgh}}, and {{Dublin Philosophical Magazine}} and {{Journal}} of {{Science}}: {{Vol}} 2, {{No}} 7},
  howpublished = {http://www.tandfonline.com/doi/pdf/10.1080/14786440109462664}
}

@article{Kelvin1901,
  title = {I.},
  volume = {2},
  doi = {10.1080/14786440109462664},
  journal = {Philosophical Magazine Series 6},
  author = {Kelvin, Lord},
  month = jul,
  year = {1901},
  pages = {1-40}
}

@article{zotero-76,
  type = {Article}
}

@article{EibenSmallResolutionProofs2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.02120},
  primaryClass = {cs},
  title = {Small {{Resolution Proofs}} for {{QBF}} Using {{Dependency Treewidth}}},
  abstract = {In spite of the close connection between the evaluation of quantified Boolean formulas (QBF) and propositional satisfiability (SAT), tools and techniques which exploit structural properties of SAT instances are known to fail for QBF. This is especially true for the structural parameter treewidth, which has allowed the design of successful algorithms for SAT but cannot be straightforwardly applied to QBF since it does not take into account the interdependencies between quantified variables. In this work we introduce and develop dependency treewidth, a new structural parameter based on treewidth which allows the efficient solution of QBF instances. Dependency treewidth pushes the frontiers of tractability for QBF by overcoming the limitations of previously introduced variants of treewidth for QBF. We augment our results by developing algorithms for computing the decompositions that are required to use the parameter.},
  journal = {arXiv:1711.02120 [cs]},
  author = {Eiben, Eduard and Ganian, Robert and Ordyniak, Sebastian},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Logic in Computer Science},
  file = {/Users/billlai/Folder/Papers/Small Resolution Proofs for QBF using Dependency Treewidth.pdf;/Users/billlai/Zotero/storage/UY5I3U59/Eiben et al. - 2017 - Small Resolution Proofs for QBF using Dependency T.pdf;/Users/billlai/Zotero/storage/TI8SYK3P/1711.html}
}

@article{Cookrelativeefficiencypropositional1979,
  title = {The Relative Efficiency of Propositional Proof Systems},
  volume = {44},
  issn = {0022-4812, 1943-5886},
  doi = {10.2307/2273702},
  abstract = {We are interested in studying the length of the shortest proof of a propositional tautology in various proof systems as a function of the length of the tautology. The smallest upper bound known for this function is exponential, no matter what the proof system. A question we would like to answer (but have not been able to) is whether this function has a polynomial bound for some proof system. (This question is motivated below.) Our results here are relative results.In \textsection\textsection{}2 and 3 we indicate that all standard Hilbert type systems (or Frege systems, as we call them) and natural deduction systems are equivalent, up to application of a polynomial, as far as minimum proof length goes. In \textsection{}4 we introduce extended Frege systems, which allow introduction of abbreviations for formulas. Since these abbreviations can be iterated, they eliminate the need for a possible exponential growth in formula length in a proof, as is illustrated by an example (the pigeonhole principle). In fact, Theorem 4.6 (which is a variation of a theorem of Statman) states that with a penalty of at most a linear increase in the number of lines of a proof in an extended Frege system, no line in the proof need be more than a constant times the length of the formula proved.},
  number = {1},
  journal = {The Journal of Symbolic Logic},
  author = {Cook, Stephen A. and Reckhow, Robert A.},
  month = mar,
  year = {1979},
  pages = {36-50},
  file = {/Users/billlai/Folder/Papers/The Relative Efficiency of Propositional Proof Systems.pdf;/Users/billlai/Zotero/storage/JCMRNR2R/218048250981F835B4B2A4080205A0BA.html}
}

@techreport{OliveiraAverageCaseLowerBound2017,
  title = {An {{Average}}-{{Case Lower Bound}} against {{ACC}}\^0},
  number = {173},
  author = {Oliveira, Igor Carboni and Chen, Ruiwen and Santhanam, Rahul},
  year = {2017},
  keywords = {circuit lower bounds,hardness amplification,non-trivial learning,satisfiability algorithms},
  file = {/Users/billlai/Folder/Papers/An Average-Case Lower Bound against ACC^0.pdf;/Users/billlai/Zotero/storage/QR2RH35B/Oliveira et al. - 2017 - An Average-Case Lower Bound against ACC^0.pdf;/Users/billlai/Zotero/storage/ZRX2DDRG/173.html}
}

@article{WilliamsNonuniformACCCircuit2014,
  title = {Nonuniform {{ACC Circuit Lower Bounds}}},
  volume = {61},
  issn = {0004-5411},
  doi = {10.1145/2559903},
  abstract = {The class ACC consists of circuit families with constant depth over unbounded fan-in AND, OR, NOT, and MODm gates, where m $>$ 1 is an arbitrary constant. We prove the following. ---NEXP, the class of languages accepted in nondeterministic exponential time, does not have nonuniform ACC circuits of polynomial size. The size lower bound can be slightly strengthened to quasipolynomials and other less natural functions. ---ENP, the class of languages recognized in 2O(n) time with an NP oracle, doesn't have nonuniform ACC circuits of 2no(1) size. The lower bound gives an exponential size-depth tradeoff: for every d, m there is a $\delta$ $>$ 0 such that ENP doesn't have depth-d ACC circuits of size 2n$\delta$ with MODm gates. Previously, it was not known whether EXPNP had depth-3 polynomial-size circuits made out of only MOD6 gates. The high-level strategy is to design faster algorithms for the circuit satisfiability problem over ACC circuits, then prove that such algorithms entail these lower bounds. The algorithms combine known properties of ACC with fast rectangular matrix multiplication and dynamic programming, while the second step requires a strengthening of the author's prior work.},
  number = {1},
  journal = {J. ACM},
  author = {Williams, Ryan},
  month = jan,
  year = {2014},
  keywords = {lower bounds,ACC,Circuit complexity,NEXP,satisfiability},
  pages = {2:1--2:32},
  file = {/Users/billlai/Folder/Papers/Non-Uniform ACC Circuit Lower Bounds.pdf}
}

@misc{PolynomialSizeProofs,
  title = {Polynomial {{Size Proofs}} of the {{Propositional Pigeonhole Principle}} on {{JSTOR}}},
  howpublished = {https://www.jstor.org/stable/2273826?seq=1\#page\_scan\_tab\_contents},
  file = {/Users/billlai/Zotero/storage/MRFLJFT8/2273826.html}
}

@article{BussQuasipolynomialSizeProofs2015,
  title = {Quasipolynomial {{Size Proofs}} of the {{Propositional Pigeonhole Principle}}},
  volume = {576},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2015.02.005},
  abstract = {Cook and Reckhow proved in 1979 that the propositional pigeonhole principle has polynomial size extended Frege proofs. Buss proved in 1987 that it also has polynomial size Frege proofs; these Frege proofs used a completely different proof method based on counting. This paper shows that the original Cook and Reckhow extended Frege proofs can be formulated as quasipolynomial size Frege proofs. The key point is that st-connectivity can be used to define the Cook-Reckhow construction.},
  number = {C},
  journal = {Theor. Comput. Sci.},
  author = {Buss, Sam},
  month = apr,
  year = {2015},
  keywords = {Extended Frege proofs,Frege proofs,Pigeonhole principle,Proof complexity,Propositional proofs},
  pages = {77--84},
  file = {/Users/billlai/Folder/Papers/Quasipolynomial size proofs of the propositional pigeonhole principle.pdf}
}

@book{zotero-119,
  type = {Book}
}

@book{DeMarcoControllingSoftwareProjects1986,
  address = {Upper Saddle River, NJ, USA},
  title = {Controlling {{Software Projects}}: {{Management}}, {{Measurement}}, and {{Estimates}}},
  isbn = {978-0-13-171711-4},
  shorttitle = {Controlling {{Software Projects}}},
  publisher = {{Prentice Hall PTR}},
  author = {DeMarco, T.},
  year = {1986}
}

@article{FerrerEstimatingSoftwareTesting2013,
  title = {Estimating {{Software Testing Complexity}}},
  volume = {55},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2013.07.007},
  number = {12},
  journal = {Inf. Softw. Technol.},
  author = {Ferrer, Javier and Chicano, Francisco and Alba, Enrique},
  month = dec,
  year = {2013},
  keywords = {Branch coverage,Complexity,Evolutionary algorithms,Evolutionary testing,Search based software engineering,Testability},
  pages = {2125--2139}
}

@misc{overviewcomputationalcomplexity,
  title = {An Overview of Computational Complexity},
  howpublished = {https://dl.acm.org/citation.cfm?id=358144},
  file = {/Users/billlai/Zotero/storage/PDT8BIHM/citation.html}
}

@inproceedings{RabinSpeedcomputationclassification1959,
  title = {Speed of Computation and Classification of Recursive Sets},
  booktitle = {Third {{Convention Sci}}. {{Soc}}., {{Israel}}},
  author = {Rabin, Michael Oser},
  year = {1959},
  pages = {1--2}
}

@article{RabinDegreedifficultycomputing1960,
  title = {Degree of Difficulty of Computing a Function and a Partial Ordering of Recursive Sets},
  journal = {Technical Report 2},
  author = {Rabin, Michael Oser},
  year = {1960}
}

@article{Blummachineindependenttheorycomplexity1967,
  title = {A Machine-Independent Theory of the Complexity of Recursive Functions},
  volume = {14},
  number = {2},
  journal = {Journal of the ACM (JACM)},
  author = {Blum, Manuel},
  year = {1967},
  pages = {322--336}
}

@article{Hartmaniscomputationalcomplexityalgorithms1965,
  title = {On the Computational Complexity of Algorithms},
  volume = {117},
  journal = {Transactions of the American Mathematical Society},
  author = {Hartmanis, Juris and Stearns, Richard E},
  year = {1965},
  pages = {285--306}
}

@article{Cobhamintrinsiccomputationaldifficulty1965,
  title = {The Intrinsic Computational Difficulty of Functions},
  author = {Cobham, Alan},
  year = {1965}
}

@book{CormenIntroductionalgorithms2009,
  title = {Introduction to Algorithms},
  publisher = {{MIT press}},
  author = {Cormen, Thomas H},
  year = {2009}
}

@article{Shannonsynthesistwoterminalswitching1949,
  title = {The Synthesis of Two-Terminal Switching Circuits},
  volume = {28},
  number = {1},
  journal = {Bell Labs Technical Journal},
  author = {Shannon, Claude and {others}},
  year = {1949},
  pages = {59--98}
}

@book{Savagecomplexitycomputing1976,
  title = {The Complexity of Computing},
  publisher = {{Wiley New York}},
  author = {Savage, John E},
  year = {1976}
}

@book{AroraComputationalcomplexitymodern2009,
  title = {Computational Complexity: A Modern Approach},
  publisher = {{Cambridge University Press}},
  author = {Arora, Sanjeev and Barak, Boaz},
  year = {2009}
}

@book{SipserIntroductionTheoryComputation2006,
  title = {Introduction to the {{Theory}} of {{Computation}}},
  volume = {2},
  publisher = {{Thomson Course Technology Boston}},
  author = {Sipser, Michael},
  year = {2006}
}

@article{VonNeumanncertainzerosumtwoperson1953,
  title = {A Certain Zero-Sum Two-Person Game Equivalent to the Optimal Assignment Problem},
  volume = {2},
  journal = {Contributions to the Theory of Games},
  author = {Von Neumann, John},
  year = {1953},
  pages = {5--12}
}

@article{EdmondsPathstreesflowers1965,
  title = {Paths, Trees, and Flowers},
  volume = {17},
  number = {3},
  journal = {Canadian Journal of mathematics},
  author = {Edmonds, Jack},
  year = {1965},
  pages = {449--467}
}

@incollection{KarpReducibilitycombinatorialproblems1972,
  title = {Reducibility among Combinatorial Problems},
  booktitle = {Complexity of Computer Computations},
  publisher = {{Springer}},
  author = {Karp, Richard M},
  year = {1972},
  pages = {85--103}
}

@article{KhachiyanPolynomialalgorithmslinear1980,
  title = {Polynomial Algorithms in Linear Programming},
  volume = {20},
  number = {1},
  journal = {USSR Computational Mathematics and Mathematical Physics},
  author = {Khachiyan, Leonid G},
  year = {1980},
  pages = {53--72}
}

@article{StrassenGaussianeliminationnot1969,
  title = {Gaussian Elimination Is Not Optimal},
  volume = {13},
  number = {4},
  journal = {Numerische mathematik},
  author = {Strassen, Volker},
  year = {1969},
  pages = {354--356}
}

@inproceedings{CookComplexityTheoremprovingProcedures1971,
  address = {New York, NY, USA},
  series = {STOC '71},
  title = {The {{Complexity}} of {{Theorem}}-Proving {{Procedures}}},
  doi = {10.1145/800157.805047},
  abstract = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be ``reduced'' to the problem of determining whether a given propositional formula is a tautology. Here ``reduced'' means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
  booktitle = {Proceedings of the {{Third Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Cook, Stephen A.},
  year = {1971},
  pages = {151--158},
  file = {/Users/billlai/Zotero/storage/GQUET2F8/Cook - 1971 - The Complexity of Theorem-proving Procedures.pdf}
}

@article{AgrawalPRIMES2004,
  title = {{{PRIMES}} Is in {{P}}},
  journal = {Annals of mathematics},
  author = {Agrawal, Manindra and Kayal, Neeraj and Saxena, Nitin},
  year = {2004},
  pages = {781--793}
}

@article{AroraProofverificationhardness1998,
  title = {Proof Verification and the Hardness of Approximation Problems},
  volume = {45},
  number = {3},
  journal = {Journal of the ACM (JACM)},
  author = {Arora, Sanjeev and Lund, Carsten and Motwani, Rajeev and Sudan, Madhu and Szegedy, Mario},
  year = {1998},
  pages = {501--555}
}

@misc{151203547Graph,
  title = {[1512.03547] {{Graph Isomorphism}} in {{Quasipolynomial Time}}},
  howpublished = {https://arxiv.org/abs/1512.03547},
  file = {/Users/billlai/Zotero/storage/6Z5A2YMG/1512.html}
}

@inproceedings{BabaiGraphisomorphismquasipolynomial2016,
  title = {Graph Isomorphism in Quasipolynomial Time},
  booktitle = {Proceedings of the 48th {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Babai, L{\'a}szl{\'o}},
  year = {2016},
  pages = {684--697}
}

@article{Planningeconomicimpactsinadequate2002,
  title = {The Economic Impacts of Inadequate Infrastructure for Software Testing},
  author = {Planning, Strategic},
  year = {2002}
}

@book{Myersartsoftwaretesting2011,
  title = {The Art of Software Testing},
  publisher = {{John Wiley \& Sons}},
  author = {Myers, Glenford J and Sandler, Corey and Badgett, Tom},
  year = {2011}
}

@article{YuExperiencepredictingfaultprone2012,
  title = {Experience in Predicting Fault-Prone Software Modules Using Complexity Metrics},
  volume = {9},
  number = {4},
  journal = {Quality Technology \& Quantitative Management},
  author = {Yu, Liguo and Mishra, Alok},
  year = {2012},
  pages = {421--434}
}

@article{Zhouabilitycomplexitymetrics2010,
  title = {On the Ability of Complexity Metrics to Predict Fault-Prone Classes in Object-Oriented Systems},
  volume = {83},
  number = {4},
  journal = {Journal of Systems and Software},
  author = {Zhou, Yuming and Xu, Baowen and Leung, Hareton},
  year = {2010},
  pages = {660--674}
}

@inproceedings{HassanPredictingfaultsusing2009,
  title = {Predicting Faults Using the Complexity of Code Changes},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Software Engineering}}},
  publisher = {{IEEE Computer Society}},
  author = {Hassan, Ahmed E},
  year = {2009},
  pages = {78--88}
}

@article{YooRegressiontestingminimization2012,
  title = {Regression Testing Minimization, Selection and Prioritization: A Survey},
  volume = {22},
  number = {2},
  journal = {Software Testing, Verification and Reliability},
  author = {Yoo, Shin and Harman, Mark},
  year = {2012},
  pages = {67--120}
}

@article{BertolinoHowmanypaths1996,
  title = {How Many Paths Are Needed for Branch Testing?},
  volume = {35},
  number = {2},
  journal = {Journal of Systems and Software},
  author = {Bertolino, Antonia and Marr{\'e}, Martina},
  year = {1996},
  pages = {95--106}
}

@article{Malevriscollateralcoveragedata2006,
  title = {The Collateral Coverage of Data Flow Criteria When Branch Testing},
  volume = {48},
  number = {8},
  journal = {Information and Software Technology},
  author = {Malevris, Nicos and Yates, Derek F},
  year = {2006},
  pages = {676--686}
}

@inproceedings{NogueiraPredictingsoftwarecomplexity2012,
  title = {Predicting Software Complexity by Means of Evolutionary Testing},
  booktitle = {Proceedings of the 27th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  publisher = {{ACM}},
  author = {Nogueira, Ana Filipa},
  year = {2012},
  pages = {402--405}
}

@article{McCabecomplexitymeasure1976,
  title = {A Complexity Measure},
  number = {4},
  journal = {IEEE Transactions on software Engineering},
  author = {McCabe, Thomas J},
  year = {1976},
  pages = {308--320}
}

@book{BoehmSoftwareitsimpact1973,
  title = {Software and Its Impact: {{A}} Quantitative Assessment},
  publisher = {{TRW Systems, Engineering and Integration Division}},
  author = {Boehm, Barry W},
  year = {1973}
}

@book{OviedoControlflowdata1984,
  title = {Control Flow, Data Flow and Program Complexity},
  publisher = {{State University of New York at Buffalo}},
  author = {Oviedo, Enrique Ivan},
  year = {1984}
}

@article{HalsteadElementsSoftwareScience1977,
  title = {Elements {{Of Software Science}}},
  journal = {Elsevierence},
  author = {Halstead, M. H},
  year = {1977}
}

@article{R.BasiliQualitativesoftwarecomplexity1980,
  title = {Qualitative Software Complexity Models: A Summary},
  journal = {IEEE Computer Society Press},
  author = {R. Basili, V},
  year = {1980}
}

@article{Shaonewmeasuresoftware2003,
  title = {A New Measure of Software Complexity Based on Cognitive Weights},
  volume = {28},
  number = {2},
  journal = {Canadian Journal of Electrical and Computer Engineering},
  author = {Shao, Jingqiu and Wang, Yingxu},
  year = {2003},
  pages = {69--74}
}

@inproceedings{CardosoControlflowcomplexitymeasurement2005,
  title = {Control-Flow Complexity Measurement of Processes and {{Weyuker}}'s Properties},
  volume = {8},
  booktitle = {6th {{International Enformatika Conference}}},
  author = {Cardoso, Jorge},
  year = {2005},
  pages = {213--218}
}

@article{KushwahaRobustnessanalysiscognitive2006,
  title = {Robustness Analysis of Cognitive Information Complexity Measure Using {{Weyuker}} Properties},
  volume = {31},
  number = {1},
  journal = {ACM SIGSOFT Software Engineering Notes},
  author = {Kushwaha, Dharmender Singh and Misra, Arun Kumar},
  year = {2006},
  pages = {1--6}
}

@article{WeyukerEvaluatingsoftwarecomplexity1988,
  title = {Evaluating Software Complexity Measures},
  volume = {14},
  number = {9},
  journal = {IEEE transactions on Software Engineering},
  author = {Weyuker, Elaine J.},
  year = {1988},
  pages = {1357--1365}
}

@article{KemererReliabilityfunctionpoints1993,
  title = {Reliability of Function Points Measurement: A Field Experiment},
  volume = {36},
  number = {2},
  journal = {Communications of the ACM},
  author = {Kemerer, Chris F},
  year = {1993},
  pages = {85--97}
}

@article{VesseyResearchstructuredprogramming1984,
  title = {Research on Structured Programming: {{An}} Empiricist's Evaluation},
  number = {4},
  journal = {IEEE Transactions on Software Engineering},
  author = {Vessey, Iris and Weber, Ron},
  year = {1984},
  pages = {397--407}
}

@inproceedings{Wandtheorydeepstructure1990,
  title = {Toward a Theory of the Deep Structure of Information Systems},
  booktitle = {{{ICIS}}},
  author = {Wand, Yair and Weber, Ron},
  year = {1990},
  pages = {3}
}

@techreport{EderCouplingcohesionobjectoriented1994,
  title = {Coupling and Cohesion in Object-Oriented Systems},
  institution = {{Technical Report, University of Klagenfurt}},
  author = {Eder, Johann and Kappel, Gerti and Schrefl, Michael},
  year = {1994}
}

@article{Kitchenhamframeworksoftwaremeasurement1995,
  title = {Towards a Framework for Software Measurement Validation},
  volume = {21},
  number = {12},
  journal = {IEEE Transactions on software Engineering},
  author = {Kitchenham, Barbara and Pfleeger, Shari Lawrence and Fenton, Norman},
  year = {1995},
  pages = {929--944}
}

@book{KellyComplexityAdvantage1999,
  title = {Complexity {{Advantage}}},
  publisher = {{McGraw-Hill Professional Publishing}},
  author = {Kelly, Susanne and Allison, Mary Ann},
  year = {1999}
}

@inproceedings{MarreReducingEstimatingCost1996,
  address = {Washington, DC, USA},
  series = {ICSE '96},
  title = {Reducing and {{Estimating}} the {{Cost}} of {{Test Coverage Criteria}}},
  isbn = {978-0-8186-7246-0},
  abstract = {Test coverage criteria define a set of entities of a program flowgraph and require that every entity is covered by some test. We first identify E/sub c/, the set of entities to be covered according to a criterion c, for a family of widely used test coverage criteria. We then present a method to derive a minimum set of entities, called a spanning set, such that a set of test paths covering the entities in this set covers every entity in E/sub c/. We provide a generalised algorithm, which is parametrized by the coverage criterion. We suggest several useful applications of spanning sets of entities to testing. In particular they help to reduce and to estimate the number of tests needed to satisfy test coverage criteria.},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Software Engineering}}},
  publisher = {{IEEE Computer Society}},
  author = {Marr{\'e}, Martina and Bertolino, Antonia},
  year = {1996},
  keywords = {flowcharting,generalised algorithm,minimum entity set,program flowgraph entities,program testing,software cost estimation,spanning set,test coverage criteria cost estimation,test coverage criteria cost reduction,test paths},
  pages = {486--494},
  file = {/Users/billlai/Zotero/storage/L3XQRS6I/Marré and Bertolino - 1996 - Reducing and Estimating the Cost of Test Coverage .pdf}
}

@article{ElberzhagerReducingtesteffort2012,
  title = {Reducing Test Effort: {{A}} Systematic Mapping Study on Existing Approaches},
  volume = {54},
  issn = {0950-5849},
  shorttitle = {Reducing Test Effort},
  doi = {10.1016/j.infsof.2012.04.007},
  abstract = {Quality assurance effort, especially testing effort, is often a major cost factor during software development, which sometimes consumes more than 50\% of the overall development effort. Consequently, one major goal is often to reduce testing effort. The main goal of the systematic mapping study is the identification of existing approaches that are able to reduce testing effort. Therefore, an overview should be presented both for researchers and practitioners in order to identify, on the one hand, future research directions and, on the other hand, potential for improvements in practical environments. Two researchers performed a systematic mapping study, focusing on four databases with an initial result set of 4020 articles. In total, we selected and categorized 144 articles. Five different areas were identified that exploit different ways to reduce testing effort: approaches that predict defect-prone parts or defect content, automation, test input reduction approaches, quality assurance techniques applied before testing, and test strategy approaches. The results reflect an increased interest in this topic in recent years. A lot of different approaches have been developed, refined, and evaluated in different environments. The highest attention was found with respect to automation and prediction approaches. In addition, some input reduction approaches were found. However, in terms of combining early quality assurance activities with testing to reduce test effort, only a small number of approaches were found. Due to the continuous challenge of reducing test effort, future research in this area is expected.},
  number = {10},
  journal = {Information and Software Technology},
  author = {Elberzhager, Frank and Rosbach, Alla and M{\"u}nch, J{\"u}rgen and Eschbach, Robert},
  month = oct,
  year = {2012},
  keywords = {Efficiency improvement,Mapping study,Quality assurance,Software testing,Test effort reduction},
  pages = {1092-1106},
  file = {/Users/billlai/Zotero/storage/FGITQI9F/Elberzhager et al. - 2012 - Reducing test effort A systematic mapping study o.pdf;/Users/billlai/Zotero/storage/DB6ID2EY/S0950584912000894.html}
}

@article{UrbanoOptimizationTechniquesAutomated2016,
  title = {{Optimization Techniques for Automated Software Test Data Generation}},
  copyright = {info:eu-repo/semantics/openAccess},
  abstract = {Esta tesis propone una variedad de contribuciones al campo de pruebas evolutivas. Hemos abarcados un amplio rango de aspectos relativos a las pruebas de programas: c{\'o}digo fuente procedimental y orientado a objetos, paradigmas estructural y funcional, problemas mono-objetivo y multi-objetivo, casos de prueba aislados y secuencias de pruebas, y trabajos te{\'o}ricos y experimentales. En relaci{\'o}n a los an{\'a}lisis llevados a cabo, hemos puesto {\'e}nfasis en el an{\'a}lisis estad{\'\i}stico de los resultados para evaluar la significancia pr{\'a}ctica de los resultados. En resumen, las principales contribuciones de la tesis son:
Definici{\'o}n de una nueva medida de distancia para el operador instanceof en programas orientados a objetos: En este trabajo nos hemos centrado en un aspecto relacionado con el software orientado a objetos, la herencia, para proponer algunos enfoques que pueden ayudar a guiar la b{\'u}squeda de datos de prueba en el contexto de las pruebas evolutivas. En particular, hemos propuesto una medida de distancia para computar la distancia de ramas en presencia del operador instanceof en programas Java. Tambi{\'e}n hemos propuesto dos operadores de mutaci{\'o}n que modifican las soluciones candidatas basadas en la medida de distancia definida.
Definici{\'o}n de una nueva medida de complejidad llamada ‘‘Branch Coverage Expectation'': En este trabajo nos enfrentamos a la complejidad de pruebas desde un punto de vista original: un programa es m{\'a}s complejo si es m{\'a}s dif{\'\i}cil de probar de forma autom{\'a}tica. Consecuentemente, definimos la ‘‘Branch Coverage Expectation'' para proporcionar conocimiento sobre la dificultad de probar programas. La fundaci{\'o}n de esta medida se basa en el modelo de Markov del programa. El modelo de Markov proporciona fundamentos te{\'o}ricos. El an{\'a}lisis de esta medida indica que est{\'a} m{\'a}s correlacionada con la cobertura de rama que las otras medidas de c{\'o}digo est{\'a}ticas. Esto significa que esto es un buen modo de estimar la dificultad de probar un programa.

Predicci{\'o}n te{\'o}rica del n{\'u}mero de casos de prueba necesarios para cubrir un porcentaje concreto de un programa: Nuestro modelo de Markov del programa puede ser usado para proporcionar una estimaci{\'o}n del n{\'u}mero de casos de prueba necesarios para cubrir un porcentaje concreto del programa. Hemos comparado nuestra predicci{\'o}n te{\'o}rica con la media de las ejecuciones reales de un generador de datos de prueba. Este modelo puede ayudar a predecir la evoluci{\'o}n de la fase de pruebas, la cual consecuentemente puede ahorrar tiempo y coste del proyecto completo. Esta predicci{\'o}n te{\'o}rica podr{\'\i}a ser tambi{\'e}n muy {\'u}til para determinar el porcentaje del programa cubierto dados un n{\'u}mero de casos de prueba.

Propuesta de enfoques para resolver el problema de generaci{\'o}n de datos de prueba multi-objetivo: En ese cap{\'\i}tulo estudiamos el problema de la generaci{\'o}n multi-objetivo con el fin de analizar el rendimiento de un enfoque directo multi-objetivo frente a la aplicaci{\'o}n de un algoritmo mono-objetivo seguido de una selecci{\'o}n de casos de prueba. Hemos evaluado cuatro algoritmos multi-objetivo (MOCell, NSGA-II, SPEA2, y PAES) y dos algoritmos mono-objetivo (GA y ES), y dos algoritmos aleatorios. En t{\'e}rminos de convergencia hac{\'\i}a el frente de Pareto {\'o}ptimo, GA y MOCell han sido los mejores resolutores en nuestra comparaci{\'o}n. Queremos destacar que el enfoque mono-objetivo, donde se ataca cada rama por separado, es m{\'a}s efectivo cuando el programa tiene un grado de anidamiento alto.

Comparativa de diferentes estrategias de priorizaci{\'o}n en l{\'\i}neas de productos y {\'a}rboles de clasificaci{\'o}n: En el contexto de pruebas funcionales hemos tratado el tema de la priorizaci{\'o}n de casos de prueba con dos representaciones diferentes, modelos de caracter{\'\i}sticas que representan l{\'\i}neas de productos software y {\'a}rboles de clasificaci{\'o}n. Hemos comparado cinco enfoques relativos al m{\'e}todo de clasificaci{\'o}n con {\'a}rboles y dos relativos a l{\'\i}neas de productos, cuatro de ellos propuestos por nosotros. Los resultados nos indican que las propuestas para ambas representaciones basadas en un algoritmo gen{\'e}tico son mejores que el resto en la mayor{\'\i}a de escenarios experimentales, es la mejor opci{\'o}n cuando tenemos restricciones de tiempo o coste.

Definici{\'o}n de la extensi{\'o}n del m{\'e}todo de clasificaci{\'o}n con {\'a}rbol para la generaci{\'o}n de secuencias de pruebas: Hemos definido formalmente esta extensi{\'o}n para la generaci{\'o}n de secuencias de pruebas que puede ser {\'u}til para la industria y para la comunidad investigadora. Sus beneficios son claros ya que indudablemente el coste de situar el artefacto bajo pruebas en el siguiente estado no es necesario, a la vez que reducimos significativamente el tama{\~n}o de la secuencia utilizando t{\'e}cnicas metaheur{\'\i}sticas. Particularmente nuestra propuesta basada en colonias de hormigas es el mejor algoritmo de la comparativa, siendo el {\'u}nico algoritmo que alcanza la cobertura m{\'a}xima para todos los modelos y tipos de cobertura.

Exploraci{\'o}n del efecto de diferentes estrategias de seeding en el c{\'a}lculo de frentes de Pareto {\'o}ptimos en l{\'\i}neas de productos: Estudiamos el comportamiento de algoritmos cl{\'a}sicos multi-objetivo evolutivos aplicados a las pruebas por pares de l{\'\i}neas de productos. El grupo de algoritmos fue seleccionado para cubrir una amplia y diversa gama de t{\'e}cnicas. Nuestra evaluaci{\'o}n indica claramente que las estrategias de seeding ayudan al proceso de b{\'u}squeda de forma determinante. Cuanta m{\'a}s informaci{\'o}n se disponga para crear esta poblaci{\'o}n inicial, mejores ser{\'a}n los resultados obtenidos. Adem{\'a}s, gracias al uso de t{\'e}cnicas multi-objetivo podemos proporcionar un conjunto de pruebas adecuado mayor o menor, en resumen, que mejor se adapte a sus restricciones econ{\'o}micas o tecnol{\'o}gicas.

Propuesta de t{\'e}cnica exacta para la computaci{\'o}n del frente de Pareto {\'o}ptimo en l{\'\i}neas de productos software: Hemos propuesto un enfoque exacto para este c{\'a}lculo en el caso multi-objetivo con cobertura paiwise. Definimos un programa lineal 0-1 y un algoritmo basado en resolutores SAT para obtener el frente de Pareto verdadero. La evaluaci{\'o}n de los resultados nos indica que, a pesar de ser un fant{\'a}stico m{\'e}todo para el c{\'a}lculo de soluciones {\'o}ptimas, tiene el inconveniente de la escalabilidad, ya que para modelos grandes el tiempo de ejecuci{\'o}n sube considerablemente. Tras realizar un estudio de correlaciones, confirmamos nuestras sospechas, existe una alta correlaci{\'o}n entre el tiempo de ejecuci{\'o}n y el n{\'u}mero de productos denotado por el modelo de caracter{\'\i}sticas del programa.},
  language = {spa},
  author = {Urbano, Ferrer and Javier, Francisco},
  year = {2016},
  file = {/Users/billlai/Zotero/storage/DH2PKB2V/Urbano and Javier - 2016 - Optimization Techniques for Automated Software Tes.pdf;/Users/billlai/Zotero/storage/Q5NIPY6H/13056.html}
}

@article{MohammadianUsingProgramSlicing2013,
  title = {Using {{Program Slicing Technique}} to {{Reduce}} the {{Cost}} of {{Software Testing}}},
  volume = {2},
  issn = {2345-4652},
  abstract = {Systems of computers and their application in the lives of modern human beings are vastly expanding. In any kind of computer application, failure in computer systems can lead to a range of financial and mortal losses. Indeed, the major origin of software failure can be located in designing or implementing software. With regard to these statistics, 30\% of the software projects have been prosperous and successful. The proposed method is intended to reduce the cost and time of testing and it focuses on enhancing the efficiency of software testing methods. In this paper, we investigated the effect of slicing techniques on the reduction rate of testing cost and time. The results of experiments show that we can cover a large number of program instructions, branches and paths by a small number of test cases in the sliced program},
  number = {7},
  journal = {Journal of Artificial Intelligence in Electrical Engineering},
  author = {Mohammadian, Asghar and Arasteh, Bahman},
  month = nov,
  year = {2013},
  pages = {24-33},
  file = {/Users/billlai/Zotero/storage/3X5MYKBB/Mohammadian and Arasteh - 2013 - Using Program Slicing Technique to Reduce the Cost.pdf}
}

@article{GearyExplorationrelationshiptacit2016,
  title = {Exploration of the Relationship between Tacit Knowledge and Software System Test Complexity},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/3.0/},
  abstract = {This research has explored the relationship between system test complexity and tacit knowledge. It is proposed as part of this thesis, that the process of system testing (comprising of test planning, test development, test execution, test fault analysis, test measurement, and case management), is directly affected by both complexity associated with the system under test, and also by other sources of complexity, independent of the system under test, but related to the wider process of system testing. While a certain amount of knowledge related to the system under test is inherent, tacit in nature, and therefore difficult to make explicit, it has been found that a significant amount of knowledge relating to these other sources of complexity, can indeed be made explicit. While the importance of explicit knowledge has been reinforced by this research, there has been a lack of evidence to suggest that the availability of tacit knowledge to a test team is of any less importance to the process of system testing, when operating in a traditional software development environment. The sentiment was commonly expressed by participants, that even though a considerable amount of explicit knowledge relating to the system is freely available, that a good deal of knowledge relating to the system under test, which is demanded for effective system testing, is actually tacit in nature (approximately 60\% of participants operating in a traditional development environment, and 60\% of participants operating in an agile development environment, expressed similar sentiments). To cater for the availability of tacit knowledge relating to the system under test, and indeed, both explicit and tacit knowledge required by system testing in general, an appropriate knowledge management structure needs to be in place. This would appear to be required, irrespective of the employed development methodology.},
  language = {en},
  author = {Geary, Niall},
  year = {2016},
  file = {/Users/billlai/Zotero/storage/4526QJ9S/3113.html}
}

@book{HolzmannDesignValidationComputer2007,
  address = {Englewood Cliffs, New Jersey 07632},
  title = {Design and {{Validation}} of {{Computer Protocols}}},
  publisher = {{Prentice-Hall}},
  author = {Holzmann, Gerard J},
  year = {2007}
}

@article{FerrerEstimatingsoftwaretesting2013,
  title = {Estimating Software Testing Complexity},
  volume = {55},
  number = {12},
  journal = {Information and Software Technology},
  author = {Ferrer, Javier and Chicano, Francisco and Alba, Enrique},
  year = {2013},
  pages = {2125--2139}
}

@article{Turingcomputablenumbersapplication1937,
  title = {On Computable Numbers, with an Application to the {{Entscheidungsproblem}}},
  volume = {2},
  number = {1},
  journal = {Proceedings of the London mathematical society},
  author = {Turing, Alan Mathison},
  year = {1937},
  pages = {230--265}
}

@incollection{NesetrilCoreAlgorithms2012,
  series = {Algorithms and Combinatorics},
  title = {Core {{Algorithms}}},
  isbn = {978-3-642-27874-7 978-3-642-27875-4},
  abstract = {An essential part of this book deals with estimates of complexity of algorithms. The aim of this chapter is to describe core algorithms, like the computation of a p-tree-depth decomposition. We shall describe this particular algorithm in a sufficiently precise way to allow an actual implementation of the described algorithms. In order to base our complexity results, we specify our computational model in this chapter.},
  language = {en},
  booktitle = {Sparsity},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Ne{\v s}et{\v r}il, Jaroslav and de Mendez, Patrice Ossona},
  year = {2012},
  pages = {381-396},
  file = {/Users/billlai/Folder/Papers/[Algorithms and Combinatorics 28] Jaroslav Nešetřil, Patrice Ossona de Mendez (auth.) - Sparsity_ Graphs, Structures, and Algorithms (2012, Springer-Verlag Berlin Heidelberg).pdf;/Users/billlai/Zotero/storage/VBLYUXBJ/Nešetřil and Mendez - 2012 - Core Algorithms.pdf;/Users/billlai/Zotero/storage/KPWX2G57/978-3-642-27875-4_17.html},
  doi = {10.1007/978-3-642-27875-4_17}
}

@inproceedings{AjtaiLogSortingNetwork1983,
  address = {New York, NY, USA},
  series = {STOC '83},
  title = {An 0({{N Log N}}) {{Sorting Network}}},
  isbn = {978-0-89791-099-6},
  doi = {10.1145/800061.808726},
  abstract = {The purpose of this paper is to describe a sorting network of size 0(n log n) and depth 0(log n). A natural way of sorting is through consecutive halvings: determine the upper and lower halves of the set, proceed similarly within the halves, and so on. Unfortunately, while one can halve a set using only 0(n) comparisons, this cannot be done in less than log n (parallel) time, and it is known that a halving network needs (\textonehalf{})n log n comparisons. It is possible, however, to construct a network of 0(n) comparisons which halves in constant time with high accuracy. This procedure (\&egr;-halving) and a derived procedure (\&egr;-nearsort) are described below, and our sorting network will be centered around these elementary steps.},
  booktitle = {Proceedings of the {{Fifteenth Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Ajtai, M. and Koml{\'o}s, J. and Szemer{\'e}di, E.},
  year = {1983},
  pages = {1--9},
  file = {/Users/billlai/Folder/Papers/p1-ajtai.pdf;/Users/billlai/Zotero/storage/UAHRDZP3/Ajtai et al. - 1983 - An 0(N Log N) Sorting Network.pdf}
}

@article{ParallelAlgorithmComputing1995,
  title = {A {{Parallel Algorithm}} for {{Computing Minimum Spanning Trees}}},
  volume = {19},
  issn = {0196-6774},
  doi = {10.1006/jagm.1995.1043},
  abstract = {present a simple and implementable algorithm that computes a minimum spanning tree of an undirected weighted graph G = (V, E) of n = |V| vertices and \ldots},
  language = {en},
  number = {3},
  journal = {Journal of Algorithms},
  month = nov,
  year = {1995},
  pages = {383-401},
  file = {/Users/billlai/Zotero/storage/CZD754ZL/S0196677485710437.html}
}

@inproceedings{GreinerComparisonParallelAlgorithms1994,
  address = {New York, NY, USA},
  series = {SPAA '94},
  title = {A {{Comparison}} of {{Parallel Algorithms}} for {{Connected Components}}},
  isbn = {978-0-89791-671-4},
  doi = {10.1145/181014.181021},
  abstract = {This paper presents a comparison of the pragmatic aspects of some parallel algorithms for finding connected components, together with optimizations on these algorithms. The algorithms being compared are two similar algorithms by Shiloach-Vishkin [22] and Awerbuch-Shiloach [2], a randomized contraction algorithm based on algorithms by Reif [21] and Phillips [20], and a hybrid algorithm [11]. Improvements are given for the first two to improve performance significantly, although without improving their asymptotic complexity. The hybrid combines features of the others and is generally the fastest of those tested. Timings were made using NESL [4] code as executed on a Connection Machine 2 and Cray Y-MP/C90.},
  booktitle = {Proceedings of the {{Sixth Annual ACM Symposium}} on {{Parallel Algorithms}} and {{Architectures}}},
  publisher = {{ACM}},
  author = {Greiner, John},
  year = {1994},
  pages = {16--25},
  file = {/Users/billlai/Zotero/storage/YKJ6WMC8/Greiner - 1994 - A Comparison of Parallel Algorithms for Connected .pdf}
}

@inproceedings{ChongImprovedparallelalgorithms1995,
  title = {Improved Parallel Algorithms for Finding Connected Components},
  volume = {1},
  doi = {10.1109/ICAPP.1995.472217},
  abstract = {Finding the connected components of a graph is a basic computational problem. In recent years, there were several exciting results in breaking the log2 n-time barrier to finding connected components on parallel machines using shared memory without concurrent-write capability. This paper further presents two new parallel algorithms both using less than log2 n time. The merit of the first algorithm is that it uses only a sublinear number of processors, yet retains the time complexity of the fastest existing algorithm. The second algorithm is slightly slower but its work (i.e., the time-processor product) is closer to optimal than all previous algorithms using less than log2 n time},
  booktitle = {Proceedings 1st {{International Conference}} on {{Algorithms}} and {{Architectures}} for {{Parallel Processing}}},
  author = {Chong, K. W. and Lam, T. W.},
  month = apr,
  year = {1995},
  keywords = {Algorithm design and analysis,Application software,basic computational problem,computational complexity,computational geometry,Computer science,Concurrent computing,connected components finding,graph,graph theory,Joining processes,parallel algorithms,Parallel algorithms,Parallel machines,Phase change random access memory,Random access memory,Search methods,shared memory,time complexity,time-processor product},
  pages = {452-459 vol.1}
}

@inproceedings{BusParallelAlgorithmConnected2001,
  series = {Lecture Notes in Computer Science},
  title = {A {{Parallel Algorithm}} for {{Connected Components}} on {{Distributed Memory Machines}}},
  isbn = {978-3-540-42609-7 978-3-540-45417-5},
  doi = {10.1007/3-540-45417-9_39},
  abstract = {Finding connected components (CC) of an undirected graph is a fundamental computational problem. Various CC algorithms exist for PRAM models. An implementation of a PRAM CC algorithm on a coarse-grain MIMD machine with distributed memory brings many problems, since the communication overhead is substantial compared to the local computation. Several implementations of CC algorithms on distributed memory machines have been described in the literature, all in Split-C. We have designed and implemented a CC algorithm in C++ and MPI, by combining the ideas of the previous PRAM and distributed memory algorithms. Our main optimization is based on replacing the conditional hooking by rules for reducing nontrivial cycles during the contraction of components. We have also implemented a method for reducing the number of exchanged messages which is based on buffering messages and on deferred processing of answers.},
  language = {en},
  booktitle = {Recent {{Advances}} in {{Parallel Virtual Machine}} and {{Message Passing Interface}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Bu{\v s}, Libor and Tvrd{\'\i}k, Pavel},
  month = sep,
  year = {2001},
  pages = {280-287},
  file = {/Users/billlai/Zotero/storage/9SDT3XCA/Buš and Tvrdík - 2001 - A Parallel Algorithm for Connected Components on D.pdf}
}

@inproceedings{BoulandTractableParameterizationsGraph2012,
  series = {Lecture Notes in Computer Science},
  title = {On {{Tractable Parameterizations}} of {{Graph Isomorphism}}},
  isbn = {978-3-642-33292-0 978-3-642-33293-7},
  doi = {10.1007/978-3-642-33293-7_21},
  abstract = {The fixed-parameter tractability of graph isomorphism is an open problem with respect to a number of natural parameters, such as tree-width, genus and maximum degree. We show that graph isomorphism is fixed-parameter tractable when parameterized by the tree-depth of the graph. We also extend this result to a parameter generalizing both tree-depth and max-leaf-number by deploying new variants of cops-and-robbers games.},
  language = {en},
  booktitle = {Parameterized and {{Exact Computation}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Bouland, Adam and Dawar, Anuj and Kopczy{\'n}ski, Eryk},
  month = sep,
  year = {2012},
  pages = {218-230},
  file = {/Users/billlai/Zotero/storage/I6PDYSJ4/Bouland et al. - 2012 - On Tractable Parameterizations of Graph Isomorphis.pdf;/Users/billlai/Zotero/storage/GLEY8DGX/978-3-642-33293-7_21.html}
}

@article{Forbiddengraphstreedepth2012,
  title = {Forbidden Graphs for Tree-Depth},
  volume = {33},
  issn = {0195-6698},
  doi = {10.1016/j.ejc.2011.09.014},
  abstract = {For every k$\geq$0, we define Gk as the class of graphs with tree-depth at most k, i.e. the class containing every graph G admitting a valid colouring $\rho$:V(\ldots},
  language = {en},
  number = {5},
  journal = {European Journal of Combinatorics},
  month = jul,
  year = {2012},
  pages = {969-979},
  file = {/Users/billlai/Folder/Papers/Forbidden graphs for tree-depth.pdf;/Users/billlai/Zotero/storage/EAM948U2/S0195669811001624.html}
}

@article{EdwardsHarmoniousChromaticNumber1997,
  title = {The {{Harmonious Chromatic Number}} of {{Bounded Degree Graphs}}},
  volume = {55},
  issn = {0024-6107},
  doi = {10.1112/S0024610797004857},
  abstract = {A harmonious colouring of a simple graph G is a proper vertex colouring such that each pair of colours appears together on at most one edge. The harmonious chromatic number h(G) is the least number of colours in such a colouring.Let d be a fixed positive integer, and $\epsilon$\&gt;0. We show that there is a natural number M such that if G is any graph with m$\geq$M edges and maximum degree at most d, then the harmonious chromatic number h(G) satisfies~(2m)1/2$\leq$h(G)$\leq$(1+$\epsilon$)(2m)1/2.},
  language = {en},
  number = {3},
  journal = {Journal of the London Mathematical Society},
  author = {Edwards, Keith},
  month = jun,
  year = {1997},
  pages = {435-447},
  file = {/Users/billlai/Folder/Papers/THE HARMONIOUS CHROMATIC NUMBER OF BOUNDED DEGREE GRAPHS.pdf;/Users/billlai/Zotero/storage/6IPEBE4Y/855827.html}
}

@inproceedings{ElberfeldAlgorithmicMetaTheorems2012,
  address = {Dagstuhl, Germany},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  title = {Algorithmic {{Meta Theorems}} for {{Circuit Classes}} of {{Constant}} and {{Logarithmic Depth}}},
  volume = {14},
  isbn = {978-3-939897-35-4},
  doi = {10.4230/LIPIcs.STACS.2012.66},
  booktitle = {29th {{International Symposium}} on {{Theoretical Aspects}} of {{Computer Science}} ({{STACS}} 2012)},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  author = {Elberfeld, Michael and Jakoby, Andreas and Tantau, Till},
  editor = {D{\"u}rr, Christoph and Wilke, Thomas},
  year = {2012},
  keywords = {algorithmic meta theorem,circuit complexity,monadic second-order logic,tree depth,tree width},
  pages = {66--77},
  file = {/Users/billlai/Folder/Papers/Algorithmic meta theorems for circuit classes of constant and logarithmic depth.pdf;/Users/billlai/Zotero/storage/ZXANQ4P7/Elberfeld et al. - 2012 - Algorithmic Meta Theorems for Circuit Classes of C.pdf;/Users/billlai/Zotero/storage/IKFKL7GS/3405.html}
}

@techreport{MurrayCircuitLowerBounds2017,
  title = {Circuit {{Lower Bounds}} for {{Nondeterministic Quasi}}-{{Polytime}}: {{An Easy Witness Lemma}} for {{NP}} and {{NQP}}},
  shorttitle = {Circuit {{Lower Bounds}} for {{Nondeterministic Quasi}}-{{Polytime}}},
  number = {188},
  author = {Murray, Cody and Williams, Ryan},
  year = {2017},
  keywords = {lower bounds,ACC,satisfiability,circuit complexity,derandomization,quasipolynomial time},
  file = {/Users/billlai/Folder/Papers/Circuit Lower Bounds for Nondeterministic Quasi-Polytime An Easy Witness Lemma for NP and NQP.pdf;/Users/billlai/Zotero/storage/VSPNMGJ5/Murray and Williams - 2017 - Circuit Lower Bounds for Nondeterministic Quasi-Po.pdf;/Users/billlai/Zotero/storage/LCRNAMJA/188.html}
}

@inproceedings{ReingoldEntropywaveszigzag2000,
  title = {Entropy Waves, the Zig-Zag Graph Product, and New Constant-Degree Expanders and Extractors},
  doi = {10.1109/SFCS.2000.892006},
  abstract = {The main contribution is a new type of graph product, which we call the zig-zag product. Taking a product of a large graph with a small graph, the resulting graph inherits (roughly) its size from the large one, its degree from the small one, and its expansion properties from both. Iteration yields simple explicit constructions of constant-degree expanders of every size, starting from one constant-size expander. Crucial to our intuition (and simple analysis) of the properties of this graph product is the view of expanders as functions which act as ``entropy wave'' propagators-they transform probability distributions in which entropy is concentrated in one area to distributions where that concentration is dissipated. In these terms, the graph product affords the constructive interference of two such waves. A variant of this product can be applied to extractors, giving the first explicit extractors whose seed length depends (poly)logarithmically on only the entropy deficiency of the source (rather than its length) and that extract almost all the entropy of high min-entropy sources. These high min-entropy extractors have several interesting applications, including the first constant-degree explicit expanders which beat the ``eigenvalue bound''},
  booktitle = {Proceedings 41st {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Reingold, O. and Vadhan, S. and Wigderson, A.},
  year = {2000},
  keywords = {Computer science,graph theory,Codes,Complexity theory,constant-degree expanders,constant-degree extractors,constructive interference,Cryptography,eigenvalue bound,eigenvalues and eigenfunctions,Eigenvalues and eigenfunctions,entropy,Entropy,entropy waves,explicit extractors,Graph theory,high min-entropy sources,Mathematics,Polynomials,probability,Probability distribution,probability distributions,zig-zag graph product},
  pages = {3-13},
  file = {/Users/billlai/Folder/Papers/Entropy waves, the zig-zag graph product, and new constant-degree expanders and extractors.pdf}
}

@article{ReingoldUndirectedConnectivityLogspace2008,
  title = {Undirected {{Connectivity}} in {{Log}}-Space},
  volume = {55},
  issn = {0004-5411},
  doi = {10.1145/1391289.1391291},
  abstract = {We present a deterministic, log-space algorithm that solves st-connectivity in undirected graphs. The previous bound on the space complexity of undirected st-connectivity was log4/3($\cdot$) obtained by Armoni, Ta-Shma, Wigderson and Zhou (JACM 2000). As undirected st-connectivity is complete for the class of problems solvable by symmetric, nondeterministic, log-space computations (the class SL), this algorithm implies that SL = L (where L is the class of problems solvable by deterministic log-space computations). Independent of our work (and using different techniques), Trifonov (STOC 2005) has presented an O(log n log log n)-space, deterministic algorithm for undirected st-connectivity. Our algorithm also implies a way to construct in log-space a fixed sequence of directions that guides a deterministic walk through all of the vertices of any connected graph. Specifically, we give log-space constructible universal-traversal sequences for graphs with restricted labeling and log-space constructible universal-exploration sequences for general graphs.},
  number = {4},
  journal = {J. ACM},
  author = {Reingold, Omer},
  month = sep,
  year = {2008},
  keywords = {bounded space algorithms,Derandomization,pseudorandom generator},
  pages = {17:1--17:24},
  file = {/Users/billlai/Folder/Papers/Undirected connectivity in log-space.pdf;/Users/billlai/Zotero/storage/HVRKWXQY/Reingold - 2008 - Undirected Connectivity in Log-space.pdf}
}

@inproceedings{SanthaQuantumRandomizedQuery2015,
  series = {Lecture Notes in Computer Science},
  title = {Quantum and {{Randomized Query Complexities}} ({{Extended Abstract}})},
  isbn = {978-3-319-17141-8 978-3-319-17142-5},
  doi = {10.1007/978-3-319-17142-5_3},
  abstract = {Deterministic query complexity is a simplified model of computation where the resource measured is only the number of questions to the input to get information about individual input bits, while all other operations are for free.},
  language = {en},
  booktitle = {Theory and {{Applications}} of {{Models}} of {{Computation}}},
  publisher = {{Springer, Cham}},
  author = {Santha, Miklos},
  month = may,
  year = {2015},
  pages = {18-19},
  file = {/Users/billlai/Zotero/storage/X4K26TEC/Santha - 2015 - Quantum and Randomized Query Complexities (Extende.pdf;/Users/billlai/Zotero/storage/SYY7C4HB/978-3-319-17142-5_3.html}
}

@misc{14017540Faster,
  title = {[1401.7540] {{A Faster Parameterized Algorithm}} for {{Treedepth}}},
  howpublished = {https://arxiv.org/abs/1401.7540},
  file = {/Users/billlai/Zotero/storage/Z5YBKB9M/1401.html}
}

@article{ReidlFasterParameterizedAlgorithm2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.7540},
  primaryClass = {cs},
  title = {A {{Faster Parameterized Algorithm}} for {{Treedepth}}},
  abstract = {The width measure $\backslash$emph\{treedepth\}, also known as vertex ranking, centered coloring and elimination tree height, is a well-established notion which has recently seen a resurgence of interest. We present an algorithm which---given as input an \$n\$-vertex graph, a tree decomposition of the graph of width \$w\$, and an integer \$t\$---decides Treedepth, i.e. whether the treedepth of the graph is at most \$t\$, in time \$2\^\{O(wt)\} $\backslash$cdot n\$. If necessary, a witness structure for the treedepth can be constructed in the same running time. In conjunction with previous results we provide a simple algorithm and a fast algorithm which decide treedepth in time \$2\^\{2\^\{O(t)\}\} $\backslash$cdot n\$ and \$2\^\{O(t\^2)\} $\backslash$cdot n\$, respectively, which do not require a tree decomposition as part of their input. The former answers an open question posed by Ossona de Mendez and Nesetril as to whether deciding Treedepth admits an algorithm with a linear running time (for every fixed \$t\$) that does not rely on Courcelle's Theorem or other heavy machinery. For chordal graphs we can prove a running time of \$2\^\{O(t $\backslash$log t)\}$\backslash$cdot n\$ for the same algorithm.},
  journal = {arXiv:1401.7540 [cs]},
  author = {Reidl, Felix and Rossmanith, Peter and Villaamil, Fernando Sanchez and Sikdar, Somnath},
  month = jan,
  year = {2014},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics},
  file = {/Users/billlai/Folder/Papers/A Faster Parameterized Algorithm for Treedepth.pdf;/Users/billlai/Zotero/storage/KNAY8VGU/Reidl et al. - 2014 - A Faster Parameterized Algorithm for Treedepth.pdf;/Users/billlai/Zotero/storage/CJXEN399/1401.html}
}

@article{Bodlaender5ApproximationAlgorithmTreewidth2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1304.6321},
  primaryClass = {cs},
  title = {A {{O}}(C\^k n) 5-{{Approximation Algorithm}} for {{Treewidth}}},
  abstract = {We give an algorithm that for an input n-vertex graph G and integer k$>$0, in time 2\^[O(k)]n either outputs that the treewidth of G is larger than k, or gives a tree decomposition of G of width at most 5k+4. This is the first algorithm providing a constant factor approximation for treewidth which runs in time single-exponential in k and linear in n. Treewidth based computations are subroutines of numerous algorithms. Our algorithm can be used to speed up many such algorithms to work in time which is single-exponential in the treewidth and linear in the input size.},
  journal = {arXiv:1304.6321 [cs]},
  author = {Bodlaender, Hans and Drange, P\aa{}l G. and Dregi, Markus S. and Fomin, Fedor V. and Lokshtanov, Daniel and Pilipczuk, Micha\l},
  month = apr,
  year = {2013},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics},
  file = {/Users/billlai/Zotero/storage/CALDLQQX/Bodlaender et al. - 2013 - A O(c^k n) 5-Approximation Algorithm for Treewidth.pdf;/Users/billlai/Zotero/storage/W77S5GR6/1304.html}
}

@inproceedings{BodlaenderLinearTimeAlgorithm1993,
  address = {New York, NY, USA},
  series = {STOC '93},
  title = {A {{Linear Time Algorithm}} for {{Finding Tree}}-Decompositions of {{Small Treewidth}}},
  isbn = {978-0-89791-591-5},
  doi = {10.1145/167088.167161},
  booktitle = {Proceedings of the {{Twenty}}-Fifth {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Bodlaender, Hans L.},
  year = {1993},
  keywords = {graph algorithms,graph minors,partial <italic>k</italic>-trees,pathwidth,treewidth},
  pages = {226--234},
  file = {/Users/billlai/Zotero/storage/HVVT7PPI/Bodlaender - 1993 - A Linear Time Algorithm for Finding Tree-decomposi.pdf}
}

@misc{SpaceCircuitComplexity,
  title = {On the {{Space}} and {{Circuit Complexity}} of {{Parameterized Problems}}: {{Classes}} and {{Completeness}} | {{SpringerLink}}},
  howpublished = {https://link.springer.com/article/10.1007/s00453-014-9944-y},
  file = {/Users/billlai/Zotero/storage/UWA6YFDE/s00453-014-9944-y.html}
}

@inproceedings{ElberfeldSpaceComplexityParameterized2012,
  series = {Lecture Notes in Computer Science},
  title = {On the {{Space Complexity}} of {{Parameterized Problems}}},
  isbn = {978-3-642-33292-0 978-3-642-33293-7},
  doi = {10.1007/978-3-642-33293-7_20},
  abstract = {Parameterized complexity theory measures the complexity of computational problems predominantly in terms of their parameterized time complexity. The purpose of the present paper is to demonstrate that the study of parameterized space complexity can give new insights into the complexity of well-studied parameterized problems like the feedback vertex set problem. We show that the undirected and the directed feedback vertex set problems have different parameterized space complexities, unless L = NL; which explains why the two problem variants seem to necessitate different algorithmic approaches even though their parameterized time complexity is the same. For a number of further natural parameterized problems, including the longest common subsequence problem and the acceptance problem for multi-head automata, we show that they lie in or are complete for different parameterized space classes; which explains why previous attempts at proving completeness of these problems for parameterized time classes have failed.},
  language = {en},
  booktitle = {Parameterized and {{Exact Computation}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Elberfeld, Michael and Stockhusen, Christoph and Tantau, Till},
  month = sep,
  year = {2012},
  pages = {206-217},
  file = {/Users/billlai/Zotero/storage/JYWLZRHE/Elberfeld et al. - 2012 - On the Space Complexity of Parameterized Problems.pdf;/Users/billlai/Zotero/storage/C4TKWGAT/978-3-642-33293-7_20.html}
}

@misc{SuccinctnessOrderInvariantLogics,
  title = {Succinctness of {{Order}}-{{Invariant Logics}} on {{Depth}}-{{Bounded Structures}}},
  howpublished = {https://dl.acm.org/citation.cfm?doid=3152770},
  file = {/Users/billlai/Zotero/storage/R9V3TPM3/citation.html}
}

@inproceedings{LokshtanovFixedParameterTractableCanonization2014,
  title = {Fixed-{{Parameter Tractable Canonization}} and {{Isomorphism Test}} for {{Graphs}} of {{Bounded Treewidth}}},
  doi = {10.1109/FOCS.2014.28},
  abstract = {We give a fixed-parameter tractable algorithm that, given a parameter k and two graphs G1, G2, either concludes that one of these graphs has treewidth at least k, or determines whether G1 and G2 are isomorphic. The running time of the algorithm on an n-vertex graph is 2O(k5 log k) $\cdot$ n5, and this is the first fixed-parameter algorithm for Graph Isomorphism parameterized by treewidth. Our algorithm in fact solves the more general canonization problem. We namely design a procedure working in 2OO(k5 log k) $\cdot$ n5 time that, for a given graph G on n vertices, either concludes that the treewidth of G is at least k, or finds an isomorphism-invariant construction term - an algebraic expression that encodes G together with a tree decomposition of G of width O(k4). Hence, a canonical graph isomorphic to G can be constructed by simply evaluating the obtained construction term, while the isomorphism test reduces to verifying whether the computed construction terms for G1 and G2 are equal.},
  booktitle = {2014 {{IEEE}} 55th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Lokshtanov, D. and Pilipczuk, M. and Pilipczuk, M. and Saurabh, S.},
  month = oct,
  year = {2014},
  keywords = {Algorithm design and analysis,computational complexity,graph theory,Complexity theory,Polynomials,treewidth,2OO(k5 log k) · n5 time,Adhesives,algebraic expression,canonical graph,canonization,fixed-parameter tractable algorithm,fixed-parameter tractable canonization graph,fixed-parameter tractable isomorphism test,graph isomorphism,Heuristic algorithms,isomorphism-invariant construction term,n-vertex graph,O(k4) width,parameterized algorithms,Particle separators,Standards},
  pages = {186-195},
  file = {/Users/billlai/Folder/Papers/Lokshtanov et al. - 2014 - Fixed-Parameter Tractable Canonization and Isomorp.pdf;/Users/billlai/Folder/Papers/Slides _ Fixed-Parameter Tractable Canonization and Isomorphism Test for Graphs of Bounded Treewidth.pdf;/Users/billlai/Zotero/storage/CM6KB5H9/6979003.html}
}

@article{maximumflowproblem1982,
  title = {The Maximum Flow Problem Is Log Space Complete for {{P}}},
  volume = {21},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(82)90092-5},
  abstract = {The space complexity of the maximum flow problem is investigated. It is shown that the problem is log space complete for deterministic polynomial time\ldots},
  language = {en},
  number = {1},
  journal = {Theoretical Computer Science},
  month = oct,
  year = {1982},
  pages = {105-111}
}

@misc{180200084NC,
  title = {[1802.00084] {{NC Algorithms}} for {{Perfect Matching}} and {{Maximum Flow}} in {{One}}-{{Crossing}}-{{Minor}}-{{Free Graphs}}},
  howpublished = {https://arxiv.org/abs/1802.00084},
  file = {/Users/billlai/Zotero/storage/UBUDLFXD/1802.html}
}

@article{IwataPowerTreeDepthFully2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.04376},
  primaryClass = {cs},
  title = {On the {{Power}} of {{Tree}}-{{Depth}} for {{Fully Polynomial FPT Algorithms}}},
  abstract = {There are many classical problems in P whose time complexities have not been improved over the past decades. Recent studies of "Hardness in P" have revealed that, for several of such problems, the current fastest algorithm is the best possible under some complexity assumptions. To bypass this difficulty, Fomin et al. (SODA 2017) introduced the concept of fully polynomial FPT algorithms. For a problem with the current best time complexity \$O(n\^c)\$, the goal is to design an algorithm running in \$k\^\{O(1)\}n\^\{c'\}\$ time for a parameter \$k\$ and a constant \$c'$<$c\$. In this paper, we investigate the complexity of graph problems in P parameterized by tree-depth, a graph parameter related to tree-width. We show that a simple divide-and-conquer method can solve many graph problems, including Weighted Matching, Negative Cycle Detection, Minimum Weight Cycle, Replacement Paths, and 2-hop Cover, in \$O($\backslash$mathrm\{td\}$\backslash$cdot m)\$ time or \$O($\backslash$mathrm\{td\}$\backslash$cdot (m+n$\backslash$log n))\$ time, where \$$\backslash$mathrm\{td\}\$ is the tree-depth of the input graph. Because any graph of tree-width \$$\backslash$mathrm\{tw\}\$ has tree-depth at most \$($\backslash$mathrm\{tw\}+1)$\backslash$log\_2 n\$, our algorithms also run in \$O($\backslash$mathrm\{tw\}$\backslash$cdot m$\backslash$log n)\$ time or \$O($\backslash$mathrm\{tw\}$\backslash$cdot (m+n$\backslash$log n)$\backslash$log n)\$ time. These results match or improve the previous best algorithms parameterized by tree-width. Especially, we solve an open problem of fully polynomial FPT algorithm for Weighted Matching parameterized by tree-width posed by Fomin et al.},
  journal = {arXiv:1710.04376 [cs]},
  author = {Iwata, Yoichi and Ogasawara, Tomoaki and Ohsaka, Naoto},
  month = oct,
  year = {2017},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics,Computer Science - Computational Complexity},
  file = {/Users/billlai/Zotero/storage/2WSUZ86E/Iwata et al. - 2017 - On the Power of Tree-Depth for Fully Polynomial FP.pdf}
}

@article{ElberfeldCanonizingGraphsBounded2017,
  title = {Canonizing {{Graphs}} of {{Bounded Tree Width}} in {{Logspace}}},
  volume = {9},
  issn = {1942-3454},
  doi = {10.1145/3132720},
  abstract = {Graph canonization is the problem of computing a unique representative, a canon, from the isomorphism class of a given graph. This implies that two graphs are isomorphic exactly if their canons are equal. We show that graphs of bounded tree width can be canonized by logarithmic-space (logspace) algorithms. This implies that the isomorphism problem for graphs of bounded tree width can be decided in logspace. In the light of isomorphism for trees being hard for the complexity class logspace, this makes the ubiquitous class of graphs of bounded tree width one of the few classes of graphs for which the complexity of the isomorphism problem has been exactly determined.},
  number = {3},
  journal = {ACM Trans. Comput. Theory},
  author = {Elberfeld, Michael and Schweitzer, Pascal},
  month = oct,
  year = {2017},
  keywords = {computational complexity,tree width,graph isomorphism,Algorithmic graph theory,graph canonization,logspace algorithms},
  pages = {12:1--12:29},
  file = {/Users/billlai/Zotero/storage/ZJYAFJUZ/Elberfeld and Schweitzer - 2017 - Canonizing Graphs of Bounded Tree Width in Logspac.pdf}
}

@inproceedings{deBeaudrapLinearTimeAlgorithm2016,
  address = {Germany},
  series = {CCC '16},
  title = {A {{Linear Time Algorithm}} for {{Quantum}} 2-{{SAT}}},
  isbn = {978-3-95977-008-8},
  doi = {10.4230/LIPIcs.CCC.2016.27},
  abstract = {The Boolean constraint satisfaction problem 3-SAT is arguably the canonical NP-complete problem. In contrast, 2-SAT can not only be decided in polynomial time, but in fact in deterministic linear time. In 2006, Bravyi proposed a physically motivated generalization of k-SAT to the quantum setting, defining the problem "quantum k-SAT". He showed that quantum 2-SAT is also solvable in polynomial time on a classical computer, in particular in deterministic time O(n4), assuming unit-cost arithmetic over a field extension of the rational numbers, where n is the number of variables. In this paper, we present an algorithm for quantum 2-SAT which runs in linear time, i.e. deterministic time O(n+m) for n and m the number of variables and clauses, respectively. Our approach exploits the transfer matrix techniques of Laumann et al. [QIC, 2010] used in the study of phase transitions for random quantum 2-SAT, and bears similarities with both the linear time 2-SAT algorithms of Even, Itai, and Shamir (based on backtracking) [SICOMP, 1976] and Aspvall, Plass, and Tarjan (based on strongly connected components) [IPL, 1979].},
  booktitle = {Proceedings of the 31st {{Conference}} on {{Computational Complexity}}},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  author = {{de Beaudrap}, Niel and Gharibian, Sevag},
  year = {2016},
  keywords = {limited backtracking,local hamiltonian,quantum 2-SAT,strongly connected components,transfer matrix},
  pages = {27:1--27:21}
}

@article{SundaramLineartimealgorithm,
  title = {Linear Time Algorithm(s) for {{Quantum 2SAT}}},
  journal = {. . .},
  author = {Sundaram, Aarthi},
  pages = {61},
  file = {/Users/billlai/Zotero/storage/H9VZATQH/Sundaram - Linear time algorithm(s) for Quantum 2SAT.pdf}
}

@misc{LouShiBengPanHouWoMenJiuMaiDeQiFangLiaoMaZhiHu,
  title = {楼市崩盘后我们就买得起房了吗？ - 知乎},
  howpublished = {https://www.zhihu.com/question/51833733},
  file = {/Users/billlai/Zotero/storage/6LI8HD7G/51833733.html}
}

@misc{12051183Complexity,
  title = {[1205.1183] {{On}} the {{Complexity}} of {{Trial}} and {{Error}}},
  howpublished = {https://arxiv.org/abs/1205.1183},
  file = {/Users/billlai/Zotero/storage/2RJPS6RF/1205.html}
}

@misc{12051183Complexitya,
  title = {[1205.1183] {{On}} the {{Complexity}} of {{Trial}} and {{Error}}},
  howpublished = {https://arxiv.org/abs/1205.1183}
}

@inproceedings{Beicomplexitytrialerror2013,
  title = {On the Complexity of Trial and Error},
  isbn = {978-1-4503-2029-0},
  doi = {10.1145/2488608.2488613},
  abstract = {Motivated by certain applications from physics, biochemistry, economics, and computer science in which the objects under investigation are unknown or not directly accessible because of various limitations, we propose a trial-and-error model to examine search problems in which inputs are unknown. More specifically, we consider constraint satisfaction problems i Ci, where the constraints Ci are hidden, and the goal is to find a solution satisfying all constraints. We can adaptively propose a candidate solution (i.e., trial), and there is a verification oracle that either confirms that it is a valid solution, or returns the index i of a violated constraint (i.e., error), with the exact content of Ci still hidden.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Bei, Xiaohui and Chen, Ning and Zhang, Shengyu},
  year = {2013},
  pages = {31},
  file = {/Users/billlai/Zotero/storage/KY99M4YS/Bei et al. - 2013 - On the complexity of trial and error.pdf}
}

@book{FlumParameterizedComplexityTheory2006,
  address = {Berlin Heidelberg},
  series = {Texts in Theoretical Computer Science. An EATCS Series},
  title = {Parameterized {{Complexity Theory}}},
  isbn = {978-3-540-29952-3},
  abstract = {Parameterized complexity theory is a recent branch of computational complexity theory that provides a framework for a refined analysis of hard algorithmic problems. The central notion of the theory, fixed-parameter tractability, has led to the development of various new algorithmic techniques and a whole new theory of intractability. This book is a state-of-the-art introduction to both algorithmic techniques for fixed-parameter tractability and the structural theory of parameterized complexity classes, and it presents detailed proofs of recent advanced results that have not appeared in book form before. Several chapters are each devoted to intractability, algorithmic techniques for designing fixed-parameter tractable algorithms, and bounded fixed-parameter tractability and subexponential time complexity. The treatment is comprehensive, and the reader is supported with exercises, notes, a detailed index, and some background on complexity theory and logic. The book will be of interest to computer scientists, mathematicians and graduate students engaged with algorithms and problem complexity.},
  language = {en},
  publisher = {{Springer-Verlag}},
  author = {Flum, J. and Grohe, M.},
  year = {2006},
  file = {/Users/billlai/Folder/Papers/Flum and Grohe - 2006 - Parameterized Complexity Theory.pdf}
}

@article{AlonColorcoding1995,
  title = {Color-Coding},
  volume = {42},
  issn = {0004-5411},
  doi = {10.1145/210332.210337},
  number = {4},
  journal = {J. ACM},
  author = {Alon, Noga and Yuster, Raphael and Zwick, Uri},
  month = jul,
  year = {1995},
  keywords = {derandomization,perfect hashing,subgraph isomorphism,tree-width},
  pages = {844--856},
  file = {/Users/billlai/Zotero/storage/3MW4PB9D/Alon et al. - 1995 - Color-coding.pdf}
}

@inproceedings{ReingoldUndirectedSTconnectivityLogspace2005,
  address = {New York, NY, USA},
  series = {STOC '05},
  title = {Undirected {{ST}}-Connectivity in {{Log}}-Space},
  isbn = {978-1-58113-960-0},
  doi = {10.1145/1060590.1060647},
  abstract = {We present a deterministic, log-space algorithm that solves st-connectivity in undirected graphs. The previous bound on the space complexity of undirected st-connectivity was log4/3 obtained by Armoni, Ta-Shma, Wigderson and Zhou [9]. As undirected st-connectivity is complete for the class of problems solvable by symmetric, non-deterministic, log-space computations (the class SL), this algorithm implies that SL = L (where L is the class of problems solvable by deterministic log-space computations). Independent of our work (and using different techniques), Trifonov [45] has presented an O(log n log log n)-space, deterministic algorithm for undirected st-connectivity.Our algorithm also implies a way to construct in log-space a fixed sequence of directions that guides a deterministic walk through all of the vertices of any connected graph. Specifically, we give log-space constructible universal-traversal sequences for graphs with restricted labelling and log-space constructible universal-exploration sequences for general graphs.},
  booktitle = {Proceedings of the {{Thirty}}-Seventh {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Reingold, Omer},
  year = {2005},
  pages = {376--385},
  file = {/Users/billlai/Zotero/storage/QM7GEWGE/Reingold - 2005 - Undirected ST-connectivity in Log-space.pdf}
}

@article{FederComputationalStructureMonotone1998,
  title = {The {{Computational Structure}} of {{Monotone Monadic SNP}} and {{Constraint Satisfaction}}: {{A Study}} through {{Datalog}} and {{Group Theory}}},
  volume = {28},
  issn = {0097-5397},
  shorttitle = {The {{Computational Structure}} of {{Monotone Monadic SNP}} and {{Constraint Satisfaction}}},
  doi = {10.1137/S0097539794266766},
  abstract = {This paper starts with the project of finding a large subclass of NP which exhibits a dichotomy. The approach is to find this subclass via syntactic prescriptions. While the paper does not achieve this goal, it does isolate a class (of problems specified by) "monotone monadic SNP without inequality" which may exhibit this dichotomy. We justify the placing of all these restrictions by showing, essentially using Ladner's theorem, that classes obtained by using only two of the above three restrictions do not show this dichotomy. We then explore the structure of this class. We show that all problems in this class reduce to the seemingly simpler class CSP. We divide CSP into subclasses and try to unify the collection of all known polytime algorithms for CSP problems and extract properties that make CSP problems NP-hard. This is where the second part of the title, "a study through Datalog and group theory," comes in. We present conjectures about this class which would end in showing the dichotomy.},
  number = {1},
  journal = {SIAM Journal on Computing},
  author = {Feder, T. and Vardi, M.},
  month = jan,
  year = {1998},
  pages = {57-104},
  file = {/Users/billlai/Zotero/storage/WAWP7GX3/Feder and Vardi - 1998 - The Computational Structure of Monotone Monadic SN.pdf}
}

@article{KempeComplexityLocalHamiltonian2004,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {quant-ph/0406180},
  title = {The {{Complexity}} of the {{Local Hamiltonian Problem}}},
  abstract = {The k-local Hamiltonian problem is a natural complete problem for the complexity class QMA, the quantum analog of NP. It is similar in spirit to MAX-k-SAT, which is NP-complete for k$<$=2. It was known that the problem is QMA-complete for any k $<$= 3. On the other hand 1-local Hamiltonian is in P, and hence not believed to be QMA-complete. The complexity of the 2-local Hamiltonian problem has long been outstanding. Here we settle the question and show that it is QMA-complete. We provide two independent proofs; our first proof uses only elementary linear algebra. Our second proof uses a powerful technique for analyzing the sum of two Hamiltonians; this technique is based on perturbation theory and we believe that it might prove useful elsewhere. Using our techniques we also show that adiabatic computation with two-local interactions on qubits is equivalent to standard quantum computation.},
  journal = {arXiv:quant-ph/0406180},
  author = {Kempe, Julia and Kitaev, Alexei and Regev, Oded},
  month = jun,
  year = {2004},
  keywords = {Computer Science - Computational Complexity,Quantum Physics},
  file = {/Users/billlai/Zotero/storage/CYMGY3LY/Kempe et al. - 2004 - The Complexity of the Local Hamiltonian Problem.pdf}
}

@article{BravyiEfficientalgorithmquantum2006,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {quant-ph/0602108},
  title = {Efficient Algorithm for a Quantum Analogue of 2-{{SAT}}},
  abstract = {Complexity of a quantum analogue of the satisfiability problem is studied. Quantum k-SAT is a problem of verifying whether there exists n-qubit pure state such that its k-qubit reduced density matrices have support on prescribed subspaces. We present a classical algorithm solving quantum 2-SAT in a polynomial time. It generalizes the well-known algorithm for the classical 2-SAT. Besides, we show that for any k$>$=4 quantum k-SAT is complete in the complexity class QMA with one-sided error.},
  journal = {arXiv:quant-ph/0602108},
  author = {Bravyi, Sergey},
  month = feb,
  year = {2006},
  keywords = {Quantum Physics},
  file = {/Users/billlai/Zotero/storage/46YQAVF4/Bravyi - 2006 - Efficient algorithm for a quantum analogue of 2-SA.pdf;/Users/billlai/Zotero/storage/RQEQXYU8/0602108.html}
}

@article{GossetQuantum3SATQMA1complete2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1302.0290},
  primaryClass = {quant-ph},
  title = {Quantum 3-{{SAT}} Is {{QMA1}}-Complete},
  doi = {10.1109/FOCS.2013.86},
  abstract = {Quantum satisfiability is a constraint satisfaction problem that generalizes classical boolean satisfiability. In the quantum k-SAT problem, each constraint is specified by a k-local projector and is satisfied by any state in its nullspace. Bravyi showed that quantum 2-SAT can be solved efficiently on a classical computer and that quantum k-SAT with k greater than or equal to 4 is QMA1-complete. Quantum 3-SAT was known to be contained in QMA1, but its computational hardness was unknown until now. We prove that quantum 3-SAT is QMA1-hard, and therefore complete for this complexity class.},
  journal = {arXiv:1302.0290 [quant-ph]},
  author = {Gosset, David and Nagaj, Daniel},
  month = oct,
  year = {2013},
  keywords = {Computer Science - Computational Complexity,Quantum Physics},
  pages = {756-765},
  file = {/Users/billlai/Zotero/storage/2ZDFDTWE/Gosset and Nagaj - 2013 - Quantum 3-SAT is QMA1-complete.pdf;/Users/billlai/Zotero/storage/MZ3FHZ4C/1302.html}
}

@article{BravyiEfficientalgorithmquantum2006a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {quant-ph/0602108},
  title = {Efficient Algorithm for a Quantum Analogue of 2-{{SAT}}},
  abstract = {Complexity of a quantum analogue of the satisfiability problem is studied. Quantum k-SAT is a problem of verifying whether there exists n-qubit pure state such that its k-qubit reduced density matrices have support on prescribed subspaces. We present a classical algorithm solving quantum 2-SAT in a polynomial time. It generalizes the well-known algorithm for the classical 2-SAT. Besides, we show that for any k$>$=4 quantum k-SAT is complete in the complexity class QMA with one-sided error.},
  journal = {arXiv:quant-ph/0602108},
  author = {Bravyi, Sergey},
  month = feb,
  year = {2006},
  keywords = {Quantum Physics},
  file = {/Users/billlai/Zotero/storage/NVWRB6CC/Bravyi - 2006 - Efficient algorithm for a quantum analogue of 2-SA.pdf;/Users/billlai/Zotero/storage/69TLWYX6/0602108.html}
}

@article{HagerupCharacterizingMultiterminalFlow1998,
  title = {Characterizing {{Multiterminal Flow Networks}} and {{Computing Flows}} in {{Networks}} of {{Small Treewidth}}},
  volume = {57},
  issn = {0022-0000},
  doi = {10.1006/jcss.1998.1592},
  abstract = {We show that if a flow network haskinput/output terminals (for the traditional maximum-flow problem,k=2), its external flow pattern (the possible values of flow into and out of the terminals) has two characterizations of size independent of the total number of vertices: a set of 2k+1 inequalities inkvariables representing flow values at the terminals, and a mimicking network with at most 22kvertices and the same external flow pattern as the original network. For the case in which the underlying graph has bounded treewidth, we present sequential and parallel algorithms that can compute these characterizations as well as a flow consistent with any desired feasible external flow (including a maximum flow between two given terminals). For constantk, the sequential algorithm runs inO(n) time onn-vertex networks, and the parallel algorithm runs inO(logn) time on an EREW PRAM withO(n/logn) processors if an explicit tree decomposition of the network of sizeO(n) is given; if not, known algorithms can compute such a tree decomposition inO((logn)2) time usingO(n/(logn)2) processors.},
  number = {3},
  journal = {J. Comput. Syst. Sci.},
  author = {Hagerup, Torben and Katajainen, Jyrki and Nishimura, Naomi and Ragde, Prabhakar},
  month = dec,
  year = {1998},
  pages = {366--375},
  file = {/Users/billlai/Folder/Papers/Hagerup et al. - 1998 - Characterizing Multiterminal Flow Networks and Com.pdf}
}

@article{probabilityonerandom1989,
  title = {With Probability One, a Random Oracle Separates {{PSPACE}} from the Polynomial-Time Hierarchy},
  volume = {38},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(89)90033-0},
  abstract = {We consider how much error a fixed depth Boolean circuit must make in computing the parity function. We show that with an exponential bound of the for\ldots},
  language = {en},
  number = {1},
  journal = {Journal of Computer and System Sciences},
  month = feb,
  year = {1989},
  pages = {68-85},
  file = {/Users/billlai/Zotero/storage/28NZHHPB/0022000089900330.html}
}

@article{probabilityonerandom1989a,
  title = {With Probability One, a Random Oracle Separates {{PSPACE}} from the Polynomial-Time Hierarchy},
  volume = {38},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(89)90033-0},
  abstract = {We consider how much error a fixed depth Boolean circuit must make in computing the parity function. We show that with an exponential bound of the for\ldots},
  language = {en},
  number = {1},
  journal = {Journal of Computer and System Sciences},
  month = feb,
  year = {1989},
  pages = {68-85},
  file = {/Users/billlai/Zotero/storage/8A28GXZF/0022000089900330.html}
}

@article{probabilityonerandom1989b,
  title = {With Probability One, a Random Oracle Separates {{PSPACE}} from the Polynomial-Time Hierarchy},
  volume = {38},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(89)90033-0},
  abstract = {We consider how much error a fixed depth Boolean circuit must make in computing the parity function. We show that with an exponential bound of the for\ldots},
  language = {en},
  number = {1},
  journal = {Journal of Computer and System Sciences},
  month = feb,
  year = {1989},
  pages = {68-85},
  file = {/Users/billlai/Zotero/storage/J68CSIWG/0022000089900330.html}
}

@article{probabilityonerandom1989c,
  title = {With Probability One, a Random Oracle Separates {{PSPACE}} from the Polynomial-Time Hierarchy},
  volume = {38},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(89)90033-0},
  abstract = {We consider how much error a fixed depth Boolean circuit must make in computing the parity function. We show that with an exponential bound of the for\ldots},
  language = {en},
  number = {1},
  journal = {Journal of Computer and System Sciences},
  month = feb,
  year = {1989},
  pages = {68-85},
  file = {/Users/billlai/Folder/Papers/1989 - With probability one, a random oracle separates PS.pdf;/Users/billlai/Zotero/storage/68R8HQD5/0022000089900330.html}
}

@article{Graphisomorphismlow1988,
  title = {Graph Isomorphism Is in the Low Hierarchy},
  volume = {37},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(88)90010-4},
  abstract = {It is shown that the graph isomorphism problem is located in level L2p of the low hierarchy in NP. This implies that this problem is not NP-complete (\ldots},
  language = {en},
  number = {3},
  journal = {Journal of Computer and System Sciences},
  month = dec,
  year = {1988},
  pages = {312-323},
  file = {/Users/billlai/Folder/Papers/1988 - Graph isomorphism is in the low hierarchy.pdf;/Users/billlai/Zotero/storage/JSV59LN6/0022000088900104.html}
}

@article{FlumDescribingParameterizedComplexity2003,
  title = {Describing {{Parameterized Complexity Classes}}},
  volume = {187},
  issn = {0890-5401},
  doi = {10.1016/S0890-5401(03)00161-5},
  abstract = {We describe parameterized complexity classes by means of classical complexity theory and descriptive complexity theory. For every classical complexity class we introduce a parameterized analogue in a natural way. In particular, the analogue of polynomial time is the class of all fixed-parameter tractable problems. We develop a basic complexity theory for the parameterized analogues of classical complexity classes and give, among other things, complete problems and logical descriptions. We then show that most of the well-known intractable parameterized complexity classes are not analogues of classical classes. Nevertheless, for all these classes we can provide natural logical descriptions.},
  number = {2},
  journal = {Inf. Comput.},
  author = {Flum, J{\"o}rg and Grohe, Martin},
  month = dec,
  year = {2003},
  pages = {291--319},
  file = {/Users/billlai/Folder/Papers/Flum and Grohe - 2003 - Describing Parameterized Complexity Classes.pdf}
}

@inproceedings{RossmanConstantdepthComplexityKclique2008,
  address = {New York, NY, USA},
  series = {STOC '08},
  title = {On the {{Constant}}-Depth {{Complexity}} of {{K}}-Clique},
  isbn = {978-1-60558-047-0},
  doi = {10.1145/1374376.1374480},
  abstract = {We prove a lower bound of $\omega$(nk/4) on the size of constant-depth circuits solving the k-clique problem on n-vertex graphs (for every constant k). This improves a lower bound of $\omega$(nk/89d2) due to Beame where d is the circuit depth. Our lower bound has the advantage that it does not depend on the constant d in the exponent of n, thus breaking the mold of the traditional size-depth tradeoff. Our k-clique lower bound derives from a stronger result of independent interest. Suppose fn :0,1n/2 $\rightarrow$ \{0,1\} is a sequence of functions computed by constant-depth circuits of size O(nt). Let G be an Erdos-Renyi random graph with vertex set \{1,...,n\} and independent edge probabilities n-$\alpha$ where $\alpha$ $\leq$ 1/2t-1. Let A be a uniform random k-element subset of \{1,...,n\} (where k is any constant independent of n) and let KA denote the clique supported on A. We prove that fn(G) = fn(G $\cup$ KA) asymptotically almost surely. These results resolve a long-standing open question in finite model theory (going back at least to Immerman in 1982). The m-variable fragment of first-order logic, denoted by FOm, consists of the first-order sentences which involve at most m variables. Our results imply that the bounded variable hierarchy FO1 $\subset$ FO2 $\subset$ ... $\subset$ FOm $\subset$ ... is strict in terms of expressive power on finite ordered graphs. It was previously unknown that FO3 is less expressive than full first-order logic on finite ordered graphs.},
  booktitle = {Proceedings of the {{Fortieth Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Rossman, Benjamin},
  year = {2008},
  keywords = {circuit complexity,ac0,bounded variable hierarchy,constant-depth circuits,first-order logic,k-clique},
  pages = {721--730},
  file = {/Users/billlai/Zotero/storage/BRHHX8XD/Rossman - 2008 - On the Constant-depth Complexity of K-clique.pdf}
}

@article{Forbiddengraphstreedepth2012a,
  title = {Forbidden Graphs for Tree-Depth},
  volume = {33},
  issn = {0195-6698},
  doi = {10.1016/j.ejc.2011.09.014},
  abstract = {For every k$\geq$0, we define Gk as the class of graphs with tree-depth at most k, i.e. the class containing every graph G admitting a valid colouring $\rho$:V(\ldots},
  language = {en},
  number = {5},
  journal = {European Journal of Combinatorics},
  month = jul,
  year = {2012},
  pages = {969-979},
  file = {/Users/billlai/Zotero/storage/2RMPYLW6/S0195669811001624.html}
}

@article{Forbiddengraphstreedepth2012b,
  title = {Forbidden Graphs for Tree-Depth},
  volume = {33},
  issn = {0195-6698},
  doi = {10.1016/j.ejc.2011.09.014},
  abstract = {For every k$\geq$0, we define Gk as the class of graphs with tree-depth at most k, i.e. the class containing every graph G admitting a valid colouring $\rho$:V(\ldots},
  language = {en},
  number = {5},
  journal = {European Journal of Combinatorics},
  month = jul,
  year = {2012},
  pages = {969-979},
  file = {/Users/billlai/Zotero/storage/99WG54EU/S0195669811001624.html}
}

@article{FominComputingTreedepthFaster2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1306.3857},
  primaryClass = {cs, math},
  title = {Computing {{Tree}}-Depth {{Faster Than}} \$2\^\{n\}\$},
  abstract = {A connected graph has tree-depth at most \$k\$ if it is a subgraph of the closure of a rooted tree whose height is at most \$k\$. We give an algorithm which for a given \$n\$-vertex graph \$G\$, in time \$$\backslash$mathcal\{O\}(1.9602\^n)\$ computes the tree-depth of \$G\$. Our algorithm is based on combinatorial results revealing the structure of minimal rooted trees whose closures contain \$G\$.},
  journal = {arXiv:1306.3857 [cs, math]},
  author = {Fomin, Fedor V. and Giannopoulou, Archontia C. and Pilipczuk, Micha\l},
  month = jun,
  year = {2013},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Combinatorics},
  file = {/Users/billlai/Zotero/storage/4TV4VL9Y/Fomin et al. - 2013 - Computing Tree-depth Faster Than $2^ n $.pdf;/Users/billlai/Zotero/storage/XU777V67/1306.html}
}

@article{Forbiddengraphstreedepth2012c,
  title = {Forbidden Graphs for Tree-Depth},
  volume = {33},
  issn = {0195-6698},
  doi = {10.1016/j.ejc.2011.09.014},
  abstract = {For every k$\geq$0, we define Gk as the class of graphs with tree-depth at most k, i.e. the class containing every graph G admitting a valid colouring $\rho$:V(\ldots},
  language = {en},
  number = {5},
  journal = {European Journal of Combinatorics},
  month = jul,
  year = {2012},
  pages = {969-979},
  file = {/Users/billlai/Folder/Papers/2012 - Forbidden graphs for tree-depth.pdf;/Users/billlai/Zotero/storage/QDU8LVQB/S0195669811001624.html}
}

@article{NesetrilLowTreeDepthDecompositions2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.1581},
  title = {On {{Low Tree}}-{{Depth Decompositions}}},
  volume = {31},
  issn = {0911-0119, 1435-5914},
  doi = {10.1007/s00373-015-1569-7},
  abstract = {The theory of sparse structures usually uses tree like structures as building blocks. In the context of sparse/dense dichotomy this role is played by graphs with bounded tree depth. In this paper we survey results related to this concept and particularly explain how these graphs are used to decompose and construct more complex graphs and structures. In more technical terms we survey some of the properties and applications of low tree depth decomposition of graphs.},
  number = {6},
  journal = {Graphs and Combinatorics},
  author = {Nesetril, Jaroslav and De Mendez, Patrice Ossona},
  month = nov,
  year = {2015},
  keywords = {Mathematics - Combinatorics},
  pages = {1941-1963},
  file = {/Users/billlai/Zotero/storage/7DJLZUDW/Nesetril and De Mendez - 2015 - On Low Tree-Depth Decompositions.pdf;/Users/billlai/Zotero/storage/86CVHER3/1412.html}
}

@article{Adviceclassesparameterized1997,
  title = {Advice Classes of Parameterized Tractability},
  volume = {84},
  issn = {0168-0072},
  doi = {10.1016/S0168-0072(95)00020-8},
  abstract = {Many natural computational problems have input consisting of two or more parts, one of which may be considered a parameter. For example, there are man\ldots},
  language = {en},
  number = {1},
  journal = {Annals of Pure and Applied Logic},
  month = mar,
  year = {1997},
  pages = {119-138},
  file = {/Users/billlai/Zotero/storage/IUHNZ8I9/S0168007295000208.html}
}

@article{BannachFastParallelFixedParameter2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.06984},
  primaryClass = {cs},
  title = {Fast {{Parallel Fixed}}-{{Parameter Algorithms}} via {{Color Coding}}},
  abstract = {Fixed-parameter algorithms have been successfully applied to solve numerous difficult problems within acceptable time bounds on large inputs. However, most fixed-parameter algorithms are inherently $\backslash$emph\{sequential\} and, thus, make no use of the parallel hardware present in modern computers. We show that parallel fixed-parameter algorithms do not only exist for numerous parameterized problems from the literature -- including vertex cover, packing problems, cluster editing, cutting vertices, finding embeddings, or finding matchings -- but that there are parallel algorithms working in $\backslash$emph\{constant\} time or at least in time $\backslash$emph\{depending only on the parameter\} (and not on the size of the input) for these problems. Phrased in terms of complexity classes, we place numerous natural parameterized problems in parameterized versions of AC\$\^0\$. On a more technical level, we show how the $\backslash$emph\{color coding\} method can be implemented in constant time and apply it to embedding problems for graphs of bounded tree-width or tree-depth and to model checking first-order formulas in graphs of bounded degree.},
  journal = {arXiv:1509.06984 [cs]},
  author = {Bannach, Max and Stockhusen, Christoph and Tantau, Till},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Computational Complexity,F.1.3},
  file = {/Users/billlai/Zotero/storage/RFVPGWYR/Bannach et al. - 2015 - Fast Parallel Fixed-Parameter Algorithms via Color.pdf;/Users/billlai/Zotero/storage/7RYBLRI8/1509.html}
}

@article{BannachFastParallelFixedParameter2015a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.06984},
  primaryClass = {cs},
  title = {Fast {{Parallel Fixed}}-{{Parameter Algorithms}} via {{Color Coding}}},
  abstract = {Fixed-parameter algorithms have been successfully applied to solve numerous difficult problems within acceptable time bounds on large inputs. However, most fixed-parameter algorithms are inherently $\backslash$emph\{sequential\} and, thus, make no use of the parallel hardware present in modern computers. We show that parallel fixed-parameter algorithms do not only exist for numerous parameterized problems from the literature -- including vertex cover, packing problems, cluster editing, cutting vertices, finding embeddings, or finding matchings -- but that there are parallel algorithms working in $\backslash$emph\{constant\} time or at least in time $\backslash$emph\{depending only on the parameter\} (and not on the size of the input) for these problems. Phrased in terms of complexity classes, we place numerous natural parameterized problems in parameterized versions of AC\$\^0\$. On a more technical level, we show how the $\backslash$emph\{color coding\} method can be implemented in constant time and apply it to embedding problems for graphs of bounded tree-width or tree-depth and to model checking first-order formulas in graphs of bounded degree.},
  journal = {arXiv:1509.06984 [cs]},
  author = {Bannach, Max and Stockhusen, Christoph and Tantau, Till},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Computational Complexity,F.1.3},
  file = {/Users/billlai/Zotero/storage/6P9EI2NN/Bannach et al. - 2015 - Fast Parallel Fixed-Parameter Algorithms via Color.pdf;/Users/billlai/Zotero/storage/VG9D4JFM/1509.html}
}

@article{Cliquesenumerationtreelike2018,
  title = {Cliques Enumeration and Tree-like Resolution Proofs},
  volume = {135},
  issn = {0020-0190},
  doi = {10.1016/j.ipl.2018.03.001},
  abstract = {We show the close connection between the enumeration of cliques in a k-clique free graph G, the running time of DPLL-style algorithms for k-clique pro\ldots},
  language = {en},
  journal = {Information Processing Letters},
  month = jul,
  year = {2018},
  pages = {62-67},
  file = {/Users/billlai/Zotero/storage/EE4YA23J/S0020019018300486.html}
}

@article{Chenlowerboundsparameterized2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.08014},
  primaryClass = {cs},
  title = {Some Lower Bounds in Parameterized \$\{$\backslash$rm \vphantom\}{{AC}}\vphantom\{\}\^0\$},
  abstract = {We demonstrate some lower bounds for parameterized problems via parameterized classes corresponding to the classical \$\{$\backslash$rm AC\}\^0\$. Among others, we derive such a lower bound for all fpt-approximations of the parameterized clique problem and for a parameterized halting problem, which recently turned out to link problems of computational complexity, descriptive complexity, and proof theory. To show the first lower bound, we prove a strong \$\{$\backslash$rm AC\}\^0\$ version of the planted clique conjecture: \$\{$\backslash$rm AC\}\^0\$-circuits asymptotically almost surely can not distinguish between a random graph and this graph with a randomly planted clique of any size \$$\backslash$le n\^$\backslash$xi\$ (where \$0 $\backslash$le $\backslash$xi $<$ 1\$).},
  journal = {arXiv:1606.08014 [cs]},
  author = {Chen, Yijia and Flum, Joerg},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Computational Complexity},
  file = {/Users/billlai/Zotero/storage/82EKCRQD/Chen and Flum - 2016 - Some lower bounds in parameterized $ rm AC ^0$.pdf;/Users/billlai/Zotero/storage/4SPVHR5H/1606.html}
}

@article{LiACComplexitySubgraph2017,
  title = {On the \${{AC}}\^0\$ {{Complexity}} of {{Subgraph Isomorphism}}},
  volume = {46},
  issn = {0097-5397},
  doi = {10.1137/14099721X},
  abstract = {Let \$P\$ be a fixed graph (hereafter called a ``pattern''), and let \$\{$\backslash$sc Subgraph\}(P)\$ denote the problem of deciding whether a given graph \$G\$ contains a subgraph isomorphic to \$P\$. We are interested in \$AC\^0\$-complexity of this problem, determined by the smallest possible exponent \$C(P)\$ for which \$\{$\backslash$sc Subgraph\}(P)\$ possesses bounded-depth circuits of size \$n\^\{C(P)+o(1)\}\$. Motivated by the previous research in the area, we also consider its ``colorful'' version \$\{$\backslash$sc Subgraph\}\_$\backslash$mathsf\{col\}(P)\$ in which the target graph \$G\$ is \$V(P)\$-colored, and the average-case version \$\{$\backslash$sc Subgraph\}\_$\backslash$mathsf\{ave\}(P)\$ under the distribution \$G(n,n\^\{-$\backslash$theta(P)\})\$, where \$$\backslash$theta(P)\$ is the threshold exponent of \$P\$. Defining \$C\_$\backslash$mathsf\{col\}(P)\$ and \$C\_$\backslash$mathsf\{ave\}(P)\$ analogously to \$C(P)\$, our main contributions can be summarized as follows: (1) \$C\_$\backslash$mathsf\{col\}(P)\$ coincides with the treewidth of the pattern \$P\$ up to a logarithmic factor. This shows that the previously known upper bound by Alon, Yuster, and Zwick [J. ACM, 42 (1995), pp. 844--856] is almost tight. (2) We give a characterization of \$C\_$\backslash$mathsf\{ave\}(P)\$ in purely combinatorial terms up to a multiplicative factor of 2. This shows that the lower bound technique of Rossman [Proceedings of the  40th ACM Symposium on Theory of Computing, 2008, pp. 721--730] is essentially tight for any pattern \$P\$ whatsoever. (3) We prove that if \$Q\$ is a minor of \$P\$, then \$\{$\backslash$sc Subgraph\}\_$\backslash$mathsf\{col\}(Q)\$ is reducible to \$\{$\backslash$sc Subgraph\}\_$\backslash$mathsf\{col\}(P)\$ via a linear-size monotone projection. At the same time, we show that there is no monotone projection whatsoever that reduces \$\{$\backslash$sc Subgraph\}(M\_3)\$ to \$\{$\backslash$sc Subgraph\}(P\_3 + M\_2)\$ (\$P\_3\$ is a path on three vertices,  \$M\_k\$ is a matching with \$k\$ edges, and ``+'' stands for the disjoint union). This result strongly suggests that the colorful version of the subgraph isomorphism problem is much better structured and well-behaved than the standard (worst-case, uncolored) one.},
  number = {3},
  journal = {SIAM Journal on Computing},
  author = {Li, Y. and Razborov, A. and Rossman, B.},
  month = jan,
  year = {2017},
  pages = {936-971},
  file = {/Users/billlai/Zotero/storage/3GHGSWRN/Li et al. - 2017 - On the $AC^0$ Complexity of Subgraph Isomorphism.pdf;/Users/billlai/Zotero/storage/LE6NNT28/14099721X.html}
}

@article{BeyersdorffParameterizedComplexityDPLL2013,
  title = {Parameterized {{Complexity}} of {{DPLL Search Procedures}}},
  volume = {14},
  issn = {1529-3785},
  doi = {10.1145/2499937.2499941},
  abstract = {We study the performance of DPLL algorithms on parameterized problems. In particular, we investigate how difficult it is to decide whether small solutions exist for satisfiability and other combinatorial problems. For this purpose we develop a Prover-Delayer game that models the running time of DPLL procedures and we establish an information-theoretic method to obtain lower bounds to the running time of parameterized DPLL procedures. We illustrate this technique by showing lower bounds to the parameterized pigeonhole principle and to the ordering principle. As our main application we study the DPLL procedure for the problem of deciding whether a graph has a small clique. We show that proving the absence of a k-clique requires n$\Omega$(k) steps for a nontrivial distribution of graphs close to the critical threshold. For the restricted case of tree-like Parameterized Resolution, this result answers a question asked by Beyersdorff et al. [2012] of understanding the Resolution complexity of this family of formulas.},
  number = {3},
  journal = {ACM Trans. Comput. Logic},
  author = {Beyersdorff, Olaf and Galesi, Nicola and Lauria, Massimo},
  month = aug,
  year = {2013},
  keywords = {Proof complexity,parameterized complexity,prover-delayer games,resolution},
  pages = {20:1--20:21},
  file = {/Users/billlai/Zotero/storage/4RGLPVMC/Beyersdorff et al. - 2013 - Parameterized Complexity of DPLL Search Procedures.pdf}
}

@article{AllenderAmplifyingLowerBounds2010,
  title = {Amplifying {{Lower Bounds}} by {{Means}} of {{Self}}-Reducibility},
  volume = {57},
  issn = {0004-5411},
  doi = {10.1145/1706591.1706594},
  abstract = {We observe that many important computational problems in NC1 share a simple self-reducibility property. We then show that, for any problem A having this self-reducibility property, A has polynomial-size TC0 circuits if and only if it has TC0 circuits of size n1+\&epsis; for every \&epsis;$>$ 0 (counting the number of wires in a circuit as the size of the circuit). As an example of what this observation yields, consider the Boolean Formula Evaluation problem (BFE), which is complete for NC1 and has the self-reducibility property. It follows from a lower bound of Impagliazzo, Paturi, and Saks, that BFE requires depth d TC0 circuits of size n1+\&epsis;d. If one were able to improve this lower bound to show that there is some constant \&epsis;$>$ 0 (independent of the depth d) such that every TC0 circuit family recognizing BFE has size at least n1+\&epsis;, then it would follow that TC0 $\not =$ NC1. We show that proving lower bounds of the form n1+\&epsis; is not ruled out by the Natural Proof framework of Razborov and Rudich and hence there is currently no known barrier for separating classes such as ACC0, TC0 and NC1 via existing ``natural'' approaches to proving circuit lower bounds. We also show that problems with small uniform constant-depth circuits have algorithms that simultaneously have small space and time bounds. We then make use of known time-space tradeoff lower bounds to show that SAT requires uniform depth d TC0 and AC0[6] circuits of size n1+c for some constant c depending on d.},
  number = {3},
  journal = {J. ACM},
  author = {Allender, Eric and Kouck{\'y}, Michal},
  month = mar,
  year = {2010},
  keywords = {lower bounds,Circuit complexity,natural proofs,self-reducibility,time-space tradeoffs},
  pages = {14:1--14:36},
  file = {/Users/billlai/Zotero/storage/9N8YZQVU/Allender and Koucký - 2010 - Amplifying Lower Bounds by Means of Self-reducibil.pdf}
}

@misc{searcheasywitness,
  title = {In Search of an Easy Witness: Exponential Time vs. Probabilistic Polynomial Time - {{ScienceDirect}}},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S0022000002000247}
}

@article{ImpagliazzoSearchEasyWitness2002,
  title = {In {{Search}} of an {{Easy Witness}}: {{Exponential Time}} vs. {{Probabilistic Polynomial Time}}},
  volume = {65},
  issn = {0022-0000},
  shorttitle = {In {{Search}} of an {{Easy Witness}}},
  doi = {10.1016/S0022-0000(02)00024-7},
  abstract = {Restricting the search space \{0,1\}n to the set of truth tables of "easy" Boolean functions on log n variables, as well as using some known hardness-randomness tradeoffs, we establish a number of results relating the complexity of exponential-time and probabilistic polynomial-time complexity classes. In particular, we show that NEXP$\subset$P/poly $\leftarrow$ NEXP = MA; this can be interpreted as saying that no derandomization of MA (and, hence, of promise-BPP) is possible unless NEXP contains a hard Boolean function. We also prove several downward closure results for ZPP, RP, BPP, and MA; e.g., we show EXP = BPP $\leftarrow$ EE = BPE, where EE is the double-exponential time class and BPE is the exponential-time analogue of BPP.},
  number = {4},
  journal = {J. Comput. Syst. Sci.},
  author = {Impagliazzo, Russell and Kabanets, Valentine and Wigderson, Avi},
  month = dec,
  year = {2002},
  pages = {672--694},
  file = {/Users/billlai/Folder/Papers/Impagliazzo et al. - 2002 - In Search of an Easy Witness Exponential Time vs..pdf}
}

@article{NisanHardnessvsRandomness1994,
  title = {Hardness vs {{Randomness}}},
  volume = {49},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(05)80043-1},
  abstract = {We present a simple new construction of a pseudorandom bit generator. It stretches a short string of truly random bits into a long string that looks random to any algorithm from a complexity class C (e.g., P, NC, PSPACE, ...) using an arbitrary function that is hard for C. This construction reveals an equivalence between the problem of proving lower bounds and the problem of generating good pseudorandom sequences. Our construction has many consequences. The most direct one is that efficient deterministic simulation of randomized algorithms is possible under much weaker assumptions than previously known. The efficiency of the simulations depends on the strength of the assumptions, and may achieve P = BPP. We believe that our results are very strong evidence that the gap between randomized and deterministic complexity is not large. Using the known lower bounds for constant depth circuits, our construction yields an unconditionally proven pseudorandom generator for constant depth circuits. As an application of this generator we characterize the power of NP with a random oracle.},
  number = {2},
  journal = {J. Comput. Syst. Sci.},
  author = {Nisan, Noam and Wigderson, Avi},
  month = oct,
  year = {1994},
  pages = {149--167}
}

@article{GroheFasterIsomorphismTest2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.04659},
  primaryClass = {cs, math},
  title = {A {{Faster Isomorphism Test}} for {{Graphs}} of {{Small Degree}}},
  abstract = {Luks's algorithm (JCSS 1982) to test isomorphism of bounded degree graphs in polynomial time is one of the most important results in the context of the Graph Isomorphism Problem and has been repeatedly used as a basic building block for many other algorithms. In particular, for graphs of logarithmic degree, Babai's quasipolynomial isomorphism test (STOC 2016) essentially boils down to Luks's algorithm, and any improvement of Babai's algorithm requires an improved isomorphism test for graphs of (poly)logarithmic degree. In this work, we obtain such an improvement: we give an algorithm that solves the Graph Isomorphism Problem in time \$n\^\{$\backslash$mathcal\{O\}(($\backslash$log d)\^\{c\})\}\$, where \$n\$ is the number of vertices of the input graphs, \$d\$ is the maximum degree of the input graphs, and \$c\$ is an absolute constant. The best previous isomorphism test for graphs of maximum degree \$d\$ due to Babai, Kantor and Luks (FOCS 1983) runs in time \$n\^\{$\backslash$mathcal\{O\}(d/ $\backslash$log d)\}\$. Our result generalizes the quasipolynomial-time algorithm for the general isomorphism problem due to Babai.},
  journal = {arXiv:1802.04659 [cs, math]},
  author = {Grohe, Martin and Neuen, Daniel and Schweitzer, Pascal},
  month = feb,
  year = {2018},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics,Mathematics - Combinatorics,Mathematics - Group Theory},
  file = {/Users/billlai/Zotero/storage/Z9ZIPSLU/Grohe et al. - 2018 - A Faster Isomorphism Test for Graphs of Small Degr.pdf;/Users/billlai/Zotero/storage/ZV5RQH79/1802.html}
}

@inproceedings{Ben-SassonShortPCPsProjection2014,
  series = {Lecture Notes in Computer Science},
  title = {Short {{PCPs}} with {{Projection Queries}}},
  isbn = {978-3-662-43947-0 978-3-662-43948-7},
  doi = {10.1007/978-3-662-43948-7_14},
  abstract = {We construct a PCP for NTIME(2 n ) with constant soundness, 2 n poly(n) proof length, and poly(n) queries where the verifier's computation is simple: the queries are a projection of the input randomness, and the computation on the prover's answers is a 3CNF. The previous upper bound for these two computations was polynomial-size circuits. Composing this verifier with a proof oracle increases the circuit-depth of the latter by 2. Our PCP is a simple variant of the PCP by Ben-Sasson, Goldreich, Harsha, Sudan, and Vadhan (CCC 2005). We also give a more modular exposition of the latter, separating the combinatorial from the algebraic arguments.If our PCP is taken as a black box, we obtain a more direct proof of the result by Williams, later with Santhanam (CCC 2013) that derandomizing circuits on n bits from a class C in time 2 n /n $\omega$(1) yields that NEXP is not in a related circuit class C${'}$. Our proof yields a tighter connection: C is an And-Or of circuits from C${'}$. Along the way we show that the same lower bound follows if the satisfiability of the And of any 3 circuits from C${'}$ can be solved in time 2 n /n $\omega$(1).},
  language = {en},
  booktitle = {Automata, {{Languages}}, and {{Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {{Ben-Sasson}, Eli and Viola, Emanuele},
  month = jul,
  year = {2014},
  pages = {163-173},
  file = {/Users/billlai/Zotero/storage/GHFND28J/Ben-Sasson and Viola - 2014 - Short PCPs with Projection Queries.pdf;/Users/billlai/Zotero/storage/DYCHL2UX/10.html}
}

@article{BengtssonQuantumComputationComputer2005,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {quant-ph/0511274},
  title = {Quantum {{Computation}}: {{A Computer Science Perspective}}},
  shorttitle = {Quantum {{Computation}}},
  abstract = {The theory of quantum computation is presented in a self contained way from a computer science perspective. The basics of classical computation and quantum mechanics is reviewed. The circuit model of quantum computation is presented in detail. Throughout there is an emphasis on the physical as well as the abstract aspects of computation and the interplay between them. This report is presented as a Master's thesis at the department of Computer Science and Engineering at G\{$\backslash$"o\}teborg University, G\{$\backslash$"o\}teborg, Sweden. The text is part of a larger work that is planned to include chapters on quantum algorithms, the quantum Turing machine model and abstract approaches to quantum computation.},
  journal = {arXiv:quant-ph/0511274},
  author = {Bengtsson, Anders K. H.},
  month = nov,
  year = {2005},
  keywords = {Quantum Physics},
  file = {/Users/billlai/Zotero/storage/B6DSYV76/Bengtsson - 2005 - Quantum Computation A Computer Science Perspectiv.pdf;/Users/billlai/Zotero/storage/9IDJ7DKH/0511274.html}
}

@article{VaziraniQuantumComputationCS,
  title = {Quantum {{Computation}}: {{A CS Perspective}}},
  language = {en},
  author = {Vazirani, Umesh V and Berkeley, U C},
  pages = {21},
  file = {/Users/billlai/Zotero/storage/BKMEN85J/Vazirani and Berkeley - Quantum Computation A CS Perspective.pdf}
}

@article{MerminQuantumComputerScience,
  title = {Quantum {{Computer Science}}: {{An Introduction}}},
  language = {en},
  author = {Mermin, N David},
  pages = {237},
  file = {/Users/billlai/Zotero/storage/WBRT7AQJ/Mermin - Quantum Computer Science An Introduction.pdf}
}

@inproceedings{DasLogspaceFPTAlgorithms2015,
  series = {Lecture Notes in Computer Science},
  title = {Logspace and {{FPT Algorithms}} for {{Graph Isomorphism}} for {{Subclasses}} of {{Bounded Tree}}-{{Width Graphs}}},
  isbn = {978-3-319-15611-8 978-3-319-15612-5},
  doi = {10.1007/978-3-319-15612-5_30},
  abstract = {We give a deterministic logspace algorithm for the graph isomorphism problem for graphs with bounded tree-depth. We also show that the graph isomorphism problem is fixed parameter tractable for a related parameterized graph class where the graph parameter is the length of the longest cycle.},
  language = {en},
  booktitle = {{{WALCOM}}: {{Algorithms}} and {{Computation}}},
  publisher = {{Springer, Cham}},
  author = {Das, Bireswar and Enduri, Murali Krishna and Reddy, I. Vinod},
  month = feb,
  year = {2015},
  pages = {329-334},
  file = {/Users/billlai/Folder/Papers/Das et al. - 2015 - Logspace and FPT Algorithms for Graph Isomorphism .pdf;/Users/billlai/Zotero/storage/CADLS7QI/978-3-319-15612-5_30.html}
}

@incollection{ChenBoundedVariableLogic2014,
  address = {Berlin, Heidelberg},
  title = {Bounded {{Variable Logic}}, {{Parameterized Logarithmic Space}}, and {{Savitch}}'s {{Theorem}}},
  volume = {8634},
  isbn = {978-3-662-44521-1 978-3-662-44522-8},
  abstract = {We study the parameterized space complexity of model-checking first-order logic with a bounded number of variables. By restricting the number of the quantifier alternations we obtain problems complete for a natural hierarchy between parameterized logarithmic space and FPT. We call this hierarchy the tree hierarchy, provide a machine characterization, and link it to the recently introduced classes PATH and TREE. We show that the lowest class PATH collapses to parameterized logarithmic space only if Savitch's theorem can be improved. Finally, we settle the complexity with respect to the tree-hierarchy of finding short undirected paths and small undirected trees.},
  language = {en},
  booktitle = {Mathematical {{Foundations}} of {{Computer Science}} 2014},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Chen, Yijia and M{\"u}ller, Moritz},
  editor = {{Csuhaj-Varj{\'u}}, Erzs{\'e}bet and Dietzfelbinger, Martin and {\'E}sik, Zolt{\'a}n},
  year = {2014},
  pages = {183-195},
  file = {/Users/billlai/Zotero/storage/829VIM46/Chen and Müller - 2014 - Bounded Variable Logic, Parameterized Logarithmic .pdf},
  doi = {10.1007/978-3-662-44522-8_16}
}

@misc{08104812constructive,
  title = {[0810.4812] {{A}} Constructive Proof of the {{Lovasz Local Lemma}}},
  howpublished = {https://arxiv.org/abs/0810.4812},
  file = {/Users/billlai/Zotero/storage/VXHRX4R6/0810.html}
}

@misc{08103647Exact,
  title = {[0810.3647] {{Exact}} Quantum Lower Bound for {{Grover}}'s Problem},
  howpublished = {https://arxiv.org/abs/0810.3647},
  file = {/Users/billlai/Zotero/storage/FA8DP8NX/0810.html}
}

@misc{08103647Exacta,
  title = {[0810.3647] {{Exact}} Quantum Lower Bound for {{Grover}}'s Problem},
  howpublished = {https://arxiv.org/abs/0810.3647},
  file = {/Users/billlai/Zotero/storage/LY6T5MHH/0810.html}
}

@article{LokshtanovFixedparametertractablecanonization,
  title = {Fixed-Parameter Tractable Canonization and Isomorphism Test for Graphs of Bounded Treewidth${_\ast}$},
  abstract = {We give a fixed-parameter tractable algorithm that, given a parameter k and two graphs G1, G2, either concludes that one of these graphs has treewidth at least k, or determines whether G1 and G2 are isomorphic. The running time of the algorithm on an n-vertex graph is 2O(k5 log k) $\cdot$ n5, and this is the first fixed-parameter algorithm for Graph Isomorphism parameterized by treewidth.},
  language = {en},
  author = {Lokshtanov, Daniel and Pilipczuk, Marcin and Pilipczuk, Michal and Saurabh, Saket},
  pages = {30},
  file = {/Users/billlai/Zotero/storage/G3EUMDAE/Lokshtanov et al. - Fixed-parameter tractable canonization and isomorp.pdf}
}

@article{Groheimprovedisomorphismtest2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.06858},
  primaryClass = {cs, math},
  title = {An Improved Isomorphism Test for Bounded-Tree-Width Graphs},
  abstract = {We give a new fpt algorithm testing isomorphism of \$n\$-vertex graphs of tree width \$k\$ in time \$2\^\{k$\backslash$operatorname\{polylog\} (k)\}$\backslash$operatorname\{poly\} (n)\$, improving the fpt algorithm due to Lokshtanov, Pilipczuk, Pilipczuk, and Saurabh (FOCS 2014), which runs in time \$2\^\{$\backslash$mathcal\{O\}(k\^5$\backslash$log k)\}$\backslash$operatorname\{poly\} (n)\$. Based on an improved version of the isomorphism-invariant graph decomposition technique introduced by Lokshtanov et al., we prove restrictions on the structure of the automorphism groups of graphs of tree width \$k\$. Our algorithm then makes heavy use of the group theoretic techniques introduced by Luks (JCSS 1982) in his isomorphism test for bounded degree graphs and Babai (STOC 2016) in his quasipolynomial isomorphism test. In fact, we even use Babai's algorithm as a black box in one place. We also give a second algorithm which, at the price of a slightly worse running time \$2\^\{$\backslash$mathcal\{O\}(k\^2 $\backslash$log k)\}$\backslash$operatorname\{poly\} (n)\$, avoids the use of Babai's algorithm and, more importantly, has the additional benefit that it can also used as a canonization algorithm.},
  journal = {arXiv:1803.06858 [cs, math]},
  author = {Grohe, Martin and Neuen, Daniel and Schweitzer, Pascal and Wiebking, Daniel},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics,Mathematics - Combinatorics},
  file = {/Users/billlai/Zotero/storage/63R5UPYY/Grohe et al. - 2018 - An improved isomorphism test for bounded-tree-widt.pdf;/Users/billlai/Zotero/storage/65UCMT69/1803.html}
}

@article{Cliqueseparatordecomposition2012,
  title = {Clique Separator Decomposition of Hole-Free and Diamond-Free Graphs and Algorithmic Consequences},
  volume = {160},
  issn = {0166-218X},
  doi = {10.1016/j.dam.2011.10.031},
  abstract = {Clique separator decomposition, introduced by Whitesides and Tarjan, is one of the most important graph decompositions. A hole is a chordless cycle wi\ldots},
  language = {en},
  number = {4-5},
  journal = {Discrete Applied Mathematics},
  month = mar,
  year = {2012},
  pages = {471-478},
  file = {/Users/billlai/Zotero/storage/D5UP44HN/S0166218X11004136.html}
}

@article{BerryIntroductionCliqueMinimal2010,
  title = {An {{Introduction}} to {{Clique Minimal Separator Decomposition}}},
  volume = {3},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  doi = {10.3390/a3020197},
  abstract = {This paper is a review which presents and explains the decomposition of graphs by clique minimal separators. The pace is leisurely, we give many examples and figures. Easy algorithms are provided to implement this decomposition. The historical and theoretical background is given, as well as sketches of proofs of the structural results involved.},
  language = {en},
  number = {2},
  journal = {Algorithms},
  author = {Berry, Anne and Pogorelcnik, Romain and Simonet, Genevi{\`e}ve},
  month = may,
  year = {2010},
  keywords = {clique minimal separator,graph decomposition,minimal triangulation},
  pages = {197-215},
  file = {/Users/billlai/Zotero/storage/6TJCPGWK/Berry et al. - 2010 - An Introduction to Clique Minimal Separator Decomp.pdf;/Users/billlai/Zotero/storage/43PZVHFF/197.html}
}

@article{WilliamsNewalgorithmslower2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.2444},
  primaryClass = {cs},
  title = {New Algorithms and Lower Bounds for Circuits with Linear Threshold Gates},
  abstract = {Let ACC$\smwhtcircle$ THR be the class of constant-depth circuits comprised of AND, OR, and MODm gates (for some constant m $>$ 1), with a bottom layer of gates computing arbitrary linear threshold functions. This class of circuits can be seen as a ``midpoint'' between ACC (where we know nontrivial lower bounds) and depth-two linear threshold circuits (where nontrivial lower bounds remain open).},
  language = {en},
  journal = {arXiv:1401.2444 [cs]},
  author = {Williams, Ryan},
  month = jan,
  year = {2014},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Computational Complexity},
  file = {/Users/billlai/Zotero/storage/T3K8X6PX/Williams - 2014 - New algorithms and lower bounds for circuits with .pdf}
}

@article{LokshtanovFixedParameterTractableCanonization2017,
  title = {Fixed-{{Parameter Tractable Canonization}} and {{Isomorphism Test}} for {{Graphs}} of {{Bounded Treewidth}}},
  volume = {46},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/140999980},
  abstract = {We give a fixed-parameter tractable algorithm that, given a parameter k and two graphs G1, G2, either concludes that one of these graphs has treewidth at least k or determines whether G1 and G2 are isomorphic. The running time of the algorithm on an n-vertex graph is 2O(k5 log k) $\cdot$n5, and this is the first fixed-parameter algorithm for Graph Isomorphism parameterized by treewidth. Our algorithm in fact solves the more general canonization problem. We namely design a procedure working in 2O(k5 log k) $\cdot$ n5 time that, for a given graph G on n vertices, either concludes that the treewidth of G is at least k or (i) finds in an isomorphic-invariant way a graph c(G) that is isomorphic to G; (ii) finds an isomorphism-invariant construction term\textemdash{}an algebraic expression that encodes G together with a tree decomposition of G of width less than k. Hence, the isomorphism test reduces to verifying whether the computed isomorphic copies or the construction terms for G1 and G2 are equal.},
  language = {en},
  number = {1},
  journal = {SIAM Journal on Computing},
  author = {Lokshtanov, Daniel and Pilipczuk, Marcin and Pilipczuk, Micha\l{} and Saurabh, Saket},
  month = jan,
  year = {2017},
  pages = {161-189},
  file = {/Users/billlai/Zotero/storage/87X4TRVR/Lokshtanov et al. - 2017 - Fixed-Parameter Tractable Canonization and Isomorp.pdf}
}

@article{HaastadAverageCaseDepthHierarchy2017,
  title = {An {{Average}}-{{Case Depth Hierarchy Theorem}} for {{Boolean Circuits}}},
  volume = {64},
  issn = {0004-5411},
  doi = {10.1145/3095799},
  abstract = {We prove an average-case depth hierarchy theorem for Boolean circuits over the standard basis of AND, OR, and NOT gates. Our hierarchy theorem says that for every d $\geq$ 2, there is an explicit n-variable Boolean function f, computed by a linear-size depth-d formula, which is such that any depth-(d-1) circuit that agrees with f on (1/2 + on(1)) fraction of all inputs must have size exp(n$\Omega$ (1/d)). This answers an open question posed by H\aa{}stad in his Ph.D. thesis (H\aa{}stad 1986b). Our average-case depth hierarchy theorem implies that the polynomial hierarchy is infinite relative to a random oracle with probability 1, confirming a conjecture of H\aa{}stad (1986a), Cai (1986), and Babai (1987). We also use our result to show that there is no ``approximate converse'' to the results of Linial, Mansour, Nisan (Linial et al. 1993) and (Boppana 1997) on the total influence of bounded-depth circuits. A key ingredient in our proof is a notion of random projections which generalize random restrictions.},
  number = {5},
  journal = {J. ACM},
  author = {{H$\backslash$a astad}, Johan and Rossman, Benjamin and Servedio, Rocco A. and Tan, Li-Yang},
  month = aug,
  year = {2017},
  keywords = {Boolean circuit complexity,polynomial hierarchy,random oracles,random projections},
  pages = {35:1--35:27},
  file = {/Users/billlai/Zotero/storage/IV9RB4IC/Ha astad et al. - 2017 - An Average-Case Depth Hierarchy Theorem for Boolea.pdf}
}

@inproceedings{LibertySimpleDeterministicMatrix2013,
  address = {New York, NY, USA},
  series = {KDD '13},
  title = {Simple and {{Deterministic Matrix Sketching}}},
  isbn = {978-1-4503-2174-7},
  doi = {10.1145/2487575.2487623},
  abstract = {A sketch of a matrix A is another matrix B which is significantly smaller than A but still approximates it well. Finding such sketches efficiently is an important building block in modern algorithms for approximating, for example, the PCA of massive matrices. This task is made more challenging in the streaming model, where each row of the input matrix can only be processed once and storage is severely limited. In this paper we adapt a well known streaming algorithm for approximating item frequencies to the matrix sketching setting. The algorithm receives n rows of a large matrix A $\epsilon$ $\mathfrak{R}$ n x m one after the other in a streaming fashion. It maintains a sketch B $\mathfrak{R}$ l x m containing only l $<$$<$ n rows but still guarantees that ATA BTB. More accurately, $\forall$x || x,||=1 0$\leq$||Ax||2 - ||Bx||2 $\leq$ 2||A||\_f 2 l Or BTB prec ATA and ||ATA - BTB|| $\leq$ 2 ||A||f2 l. This gives a streaming algorithm whose error decays proportional to 1/l using O(ml) space. For comparison, random-projection, hashing or sampling based algorithms produce convergence bounds proportional to 1/$\surd$l. Sketch updates per row in A require amortized O(ml) operations and the algorithm is perfectly parallelizable. Our experiments corroborate the algorithm's scalability and improved convergence rate. The presented algorithm also stands out in that it is deterministic, simple to implement and elementary to prove.},
  booktitle = {Proceedings of the 19th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  author = {Liberty, Edo},
  year = {2013},
  keywords = {sketching,streaming},
  pages = {581--588},
  file = {/Users/billlai/Zotero/storage/EU97Q4N2/Liberty - 2013 - Simple and Deterministic Matrix Sketching.pdf}
}

@inproceedings{ShahafConnectingDotsNews2010,
  address = {New York, NY, USA},
  series = {KDD '10},
  title = {Connecting the {{Dots Between News Articles}}},
  isbn = {978-1-4503-0055-1},
  doi = {10.1145/1835804.1835884},
  abstract = {The process of extracting useful knowledge from large datasets has become one of the most pressing problems in today's society. The problem spans entire sectors, from scientists to intelligence analysts and web users, all of whom are constantly struggling to keep up with the larger and larger amounts of content published every day. With this much data, it is often easy to miss the big picture. In this paper, we investigate methods for automatically connecting the dots -- providing a structured, easy way to navigate within a new topic and discover hidden connections. We focus on the news domain: given two news articles, our system automatically finds a coherent chain linking them together. For example, it can recover the chain of events starting with the decline of home prices (January 2007), and ending with the ongoing health-care debate. We formalize the characteristics of a good chain and provide an efficient algorithm (with theoretical guarantees) to connect two fixed endpoints. We incorporate user feedback into our framework, allowing the stories to be refined and personalized. Finally, we evaluate our algorithm over real news data. Our user studies demonstrate the algorithm's effectiveness in helping users understanding the news.},
  booktitle = {Proceedings of the 16th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  author = {Shahaf, Dafna and Guestrin, Carlos},
  year = {2010},
  keywords = {coherence,news},
  pages = {623--632},
  file = {/Users/billlai/Zotero/storage/BPZPRWTX/Shahaf and Guestrin - 2010 - Connecting the Dots Between News Articles.pdf}
}

@article{Bar-YehudaDistributedepsilonApproximation2017,
  title = {A {{Distributed}} (2 + \$$\backslash$epsilon\$)-{{Approximation}} for {{Vertex Cover}} in {{O}}({{Log}} \$$\backslash${{Delta}}\$ / \$$\backslash$epsilon\$ {{Log Log}} \$$\backslash${{Delta}}\$) {{Rounds}}},
  volume = {64},
  issn = {0004-5411},
  doi = {10.1145/3060294},
  abstract = {We present a simple deterministic distributed (2 + $\epsilon$)-approximation algorithm for minimum-weight vertex cover, which completes in O(log $\Delta$/$\epsilon$log log $\Delta$) rounds, where $\Delta$ is the maximum degree in the graph, for any $\epsilon$ $>$ 0 that is at most O(1). For a constant $\epsilon$, this implies a constant approximation in O(log $\Delta$/log log $\Delta$) rounds, which contradicts the lower bound of [KMW10].},
  number = {3},
  journal = {J. ACM},
  author = {{Bar-Yehuda}, Reuven and {Censor-Hillel}, Keren and Schwartzman, Gregory},
  month = jun,
  year = {2017},
  keywords = {graph algorithms,approximation algorithms,Distributed computing,local-ratio,vertex cover},
  pages = {23:1--23:11},
  file = {/Users/billlai/Zotero/storage/WDWZ5PLR/Bar-Yehuda et al. - 2017 - A Distributed (2 + $epsilon$)-Approximation for V.pdf}
}

@article{GalWhichFairestRent2017,
  title = {Which {{Is}} the {{Fairest}} ({{Rent Division}}) of {{Them All}}?},
  volume = {64},
  issn = {0004-5411},
  doi = {10.1145/3131361},
  abstract = {``Mirror, mirror, on the wall, who is the fairest of them all?'' The Evil Queen What is a fair way to assign rooms to several housemates and divide the rent between them? This is not just a theoretical question: many people have used the Spliddit website to obtain envy-free solutions to rent division instances. But envy freeness, in and of itself, is insufficient to guarantee outcomes that people view as intuitive and acceptable. We therefore focus on solutions that optimize a criterion of social justice, subject to the envy-freeness constraint, in order to pinpoint the ``fairest'' solutions. We develop a general algorithmic framework that enables the computation of such solutions in polynomial time. We then study the relations between natural optimization objectives and identify the maximin solution, which maximizes the minimum utility subject to envy freeness, as the most attractive. We demonstrate, in theory and using experiments on real data from Spliddit, that the maximin solution gives rise to significant gains in terms of our optimization objectives. Finally, a user study with Spliddit users as subjects demonstrates that people find the maximin solution to be significantly fairer than arbitrary envy-free solutions; this user study is unprecedented in that it asks people about their real-world rent division instances. Based on these results, the maximin solution has been deployed on Spliddit since April 2015.},
  number = {6},
  journal = {J. ACM},
  author = {Gal, Ya'akov (Kobi) and Mash, Moshe and Procaccia, Ariel D. and Zick, Yair},
  month = nov,
  year = {2017},
  keywords = {Computational,division,fair},
  pages = {39:1--39:22},
  file = {/Users/billlai/Zotero/storage/5SHR3WWK/Gal et al. - 2017 - Which Is the Fairest (Rent Division) of Them All.pdf}
}

@article{AwasthiPowerLocalizationEfficiently2017,
  title = {The {{Power}} of {{Localization}} for {{Efficiently Learning Linear Separators}} with {{Noise}}},
  volume = {63},
  issn = {0004-5411},
  doi = {10.1145/3006384},
  abstract = {We introduce a new approach for designing computationally efficient learning algorithms that are tolerant to noise, and we demonstrate its effectiveness by designing algorithms with improved noise tolerance guarantees for learning linear separators. We consider both the malicious noise model of Valiant [1985] and Kearns and Li [1988] and the adversarial label noise model of Kearns, Schapire, and Sellie [1994]. For malicious noise, where the adversary can corrupt both the label and the features, we provide a polynomial-time algorithm for learning linear separators in $\mathfrak{R}$d under isotropic log-concave distributions that can tolerate a nearly information-theoretically optimal noise rate of $\eta$ = $\Omega$($\epsilon$), improving on the $\Omega$ (\&frac;$\epsilon$3/log2(d/$\epsilon$)) noise-tolerance of Klivans et al. [2009a]. In the case that the distribution is uniform over the unit ball, this improves on the $\Omega$ (\&frac;$\epsilon$/d1/4) noise-tolerance of Kalai et al. [2005] and the $\Omega$ (\&frac;$\epsilon$2/log(d/$\epsilon$)) of Klivans et al. [2009a]. For the adversarial label noise model, where the distribution over the feature vectors is unchanged and the overall probability of a noisy label is constrained to be at most $\eta$, we also give a polynomial-time algorithm for learning linear separators in $\mathfrak{R}$d under isotropic log-concave distributions that can handle a noise rate of $\eta$ = $\Omega$($\epsilon$). In the case of uniform distribution, this improves over the results of Kalai et al. [2005], which either required runtime super-exponential in 1/$\epsilon$ (ours is polynomial in 1/$\epsilon$) or tolerated less noise.1 Our algorithms are also efficient in the active learning setting, where learning algorithms only receive the classifications of examples when they ask for them. We show that, in this model, our algorithms achieve a label complexity whose dependence on the error parameter $\epsilon$ is polylogarithmic (and thus exponentially better than that of any passive algorithm). This provides the first polynomial-time active learning algorithm for learning linear separators in the presence of malicious noise or adversarial label noise. Our algorithms and analysis combine several ingredients including aggressive localization, minimization of a progressively rescaled hinge loss, and a novel localized and soft outlier removal procedure. We use localization techniques (previously used for obtaining better sample complexity results) to obtain better noise-tolerant polynomial-time algorithms.},
  number = {6},
  journal = {J. ACM},
  author = {Awasthi, Pranjal and Balcan, Maria Florina and Long, Philip M.},
  month = jan,
  year = {2017},
  keywords = {active learning,agnostic learning,Learning theory,linear classification,localization,malicious noise,noise-tolerant learning},
  pages = {50:1--50:27},
  file = {/Users/billlai/Zotero/storage/XYANLGSB/Awasthi et al. - 2017 - The Power of Localization for Efficiently Learning.pdf}
}

@article{RazborovNewKindTradeoffs2016,
  title = {A {{New Kind}} of {{Tradeoffs}} in {{Propositional Proof Complexity}}},
  volume = {63},
  issn = {0004-5411},
  doi = {10.1145/2858790},
  abstract = {We exhibit an unusually strong tradeoff in propositional proof complexity that significantly deviates from the established pattern of almost all results of this kind. Namely, restrictions on one resource (width, in our case) imply an increase in another resource (tree-like size) that is exponential not only with respect to the complexity of the original problem, but also to the whole class of all problems of the same bit size. More specifically, we show that for any parameter k = k(n), there are unsatisfiable k-CNFs that possess refutations of width O(k), but such that any tree-like refutation of width n1 - $\epsilon$/k must necessarily have doubly exponential size exp\,(n$\Omega$(k)). This means that there exist contradictions that allow narrow refutations, but in order to keep the size of such a refutation even within a single exponent, it must necessarily use a high degree of parallelism. Our construction and proof methods combine, in a non-trivial way, two previously known techniques: the hardness escalation method based on substitution formulas and expansion. This combination results in a hardness compression approach that strives to preserve hardness of a contradiction while significantly decreasing the number of its variables.},
  number = {2},
  journal = {J. ACM},
  author = {Razborov, Alexander},
  month = apr,
  year = {2016},
  keywords = {resolution,Hardness compression,tradeoff},
  pages = {16:1--16:14},
  file = {/Users/billlai/Zotero/storage/67EDQ2LB/Razborov - 2016 - A New Kind of Tradeoffs in Propositional Proof Com.pdf}
}

@article{HaeuplerSimpleFastDeterministic2015,
  title = {Simple, {{Fast}} and {{Deterministic Gossip}} and {{Rumor Spreading}}},
  volume = {62},
  issn = {0004-5411},
  doi = {10.1145/2767126},
  abstract = {We study gossip algorithms for the rumor spreading problem, which asks each node to deliver a rumor to all nodes in an unknown network. Gossip algorithms allow nodes only to call one neighbor per round and have recently attracted attention as message efficient, simple, and robust solutions to the rumor spreading problem. A long series of papers analyzed the performance of uniform random gossip in which nodes repeatedly call a random neighbor to exchange all rumors with. A main result of this investigation was that uniform gossip completes in O(log n/$\Phi$) rounds where $\Phi$ is the conductance of the network. Nonuniform random gossip schemes were devised to allow efficient rumor spreading in networks with bottlenecks. In particular, [Censor-Hillel et al., STOC\&12] gave an O(log3 n) algorithm to solve the 1-local broadcast problem in which each node wants to exchange rumors locally with its 1-neighborhood. By repeatedly applying this protocol, one can solve the global rumor spreading quickly for all networks with small diameter, independently of the conductance. All these algorithms are inherently randomized in their design and analysis. A parallel research direction has been to reduce and determine the amount of randomness needed for efficient rumor spreading. This has been done via lower bounds for restricted models and by designing gossip algorithms with a reduced need for randomness, for instance, by using pseudorandom generators with short random seeds. The general intuition and consensus of these results has been that randomization plays a important role in effectively spreading rumors and that at least a polylogarithmic number of random bit are crucially needed. In this article improves over the state of the art in several ways by presenting a deterministic gossip algorithm that solves the the k-local broadcast problem in 2(k + log2 n) log2 n rounds. Besides being the first efficient deterministic solution to the rumor spreading problem this algorithm is interesting in many aspects: It is simpler, more natural, more robust, and faster than its randomized pendant and guarantees success with certainty instead of with high probability. Its analysis is furthermore simple, self-contained, and fundamentally different from prior works.},
  number = {6},
  journal = {J. ACM},
  author = {Haeupler, Bernhard},
  month = dec,
  year = {2015},
  keywords = {distributed computing,Gossip,information dissemination,LOCAL model,rumor spreading},
  pages = {47:1--47:18},
  file = {/Users/billlai/Zotero/storage/W495U6ZE/Haeupler - 2015 - Simple, Fast and Deterministic Gossip and Rumor Sp.pdf}
}

@article{Bodlaenderpartialkarboretumgraphs1998,
  title = {A Partial K-Arboretum of Graphs with Bounded Treewidth},
  volume = {209},
  issn = {03043975},
  doi = {10.1016/S0304-3975(97)00228-4},
  language = {en},
  number = {1-2},
  journal = {Theoretical Computer Science},
  author = {Bodlaender, Hans L.},
  month = dec,
  year = {1998},
  pages = {1-45},
  file = {/Users/billlai/Zotero/storage/S4B9L4S8/Bodlaender - 1998 - A partial k-arboretum of graphs with bounded treew.pdf}
}

@article{OtachiReductionTechniquesGraph2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1403.7238},
  primaryClass = {cs, math},
  title = {Reduction {{Techniques}} for {{Graph Isomorphism}} in the {{Context}} of {{Width Parameters}}},
  abstract = {We study the parameterized complexity of the graph isomorphism problem when parameterized by width parameters related to tree decompositions. We apply the following technique to obtain fixed-parameter tractability for such parameters. We first compute an isomorphism invariant set of potential bags for a decomposition and then apply a restricted version of the Weisfeiler-Lehman algorithm to solve isomorphism. With this we show fixed-parameter tractability for several parameters and provide a unified explanation for various isomorphism results concerned with parameters related to tree decompositions. As a possibly first step towards intractability results for parameterized graph isomorphism we develop an fpt Turing-reduction from strong tree width to the a priori unrelated parameter maximum degree.},
  journal = {arXiv:1403.7238 [cs, math]},
  author = {Otachi, Yota and Schweitzer, Pascal},
  month = mar,
  year = {2014},
  keywords = {Computer Science - Discrete Mathematics,Mathematics - Combinatorics,05C60; 05C85; 68R10;},
  file = {/Users/billlai/Zotero/storage/8HX3HQSI/Otachi and Schweitzer - 2014 - Reduction Techniques for Graph Isomorphism in the .pdf;/Users/billlai/Zotero/storage/YDLVZXX9/1403.html}
}

@article{BodlaenderPolynomialalgorithmsGraph,
  title = {Polynomial Algorithms for {{Graph Isomorphism}} and {{Chromatic Index}} on Partial K-Trees},
  abstract = {In this paper we show that GRAPH ISOMORPHISMand CHROMATICINDEXare solvable in polynomial time when restricted to the class of graphs with treewidth $<$ k (k a constant) (or equivalently, the class of partial k-trees). Also, we show that there exist algorithms that find tree-decompositionswith treewidth $<$ k of graphs with treewidth $<$ k, in O(n 3) time, (k constant).},
  language = {en},
  author = {Bodlaender, Hans L},
  pages = {10},
  file = {/Users/billlai/Zotero/storage/LV9D5MLW/Bodlaender - Polynomial algorithms for Graph Isomorphism and Ch.pdf}
}

@article{LokshtanovFixedparametertractablecanonization2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1404.0818},
  primaryClass = {cs},
  title = {Fixed-Parameter Tractable Canonization and Isomorphism Test for Graphs of Bounded Treewidth},
  abstract = {We give a fixed-parameter tractable algorithm that, given a parameter \$k\$ and two graphs \$G\_1,G\_2\$, either concludes that one of these graphs has treewidth at least \$k\$, or determines whether \$G\_1\$ and \$G\_2\$ are isomorphic. The running time of the algorithm on an \$n\$-vertex graph is \$2\^\{O(k\^5$\backslash$log k)\}$\backslash$cdot n\^5\$, and this is the first fixed-parameter algorithm for Graph Isomorphism parameterized by treewidth. Our algorithm in fact solves the more general canonization problem. We namely design a procedure working in \$2\^\{O(k\^5$\backslash$log k)\}$\backslash$cdot n\^5\$ time that, for a given graph \$G\$ on \$n\$ vertices, either concludes that the treewidth of \$G\$ is at least \$k\$, or: * finds in an isomorphic-invariant way a graph \$$\backslash$mathfrak\{c\}(G)\$ that is isomorphic to \$G\$; * finds an isomorphism-invariant construction term --- an algebraic expression that encodes \$G\$ together with a tree decomposition of \$G\$ of width \$O(k\^4)\$. Hence, the isomorphism test reduces to verifying whether the computed isomorphic copies or the construction terms for \$G\_1\$ and \$G\_2\$ are equal.},
  journal = {arXiv:1404.0818 [cs]},
  author = {Lokshtanov, Daniel and Pilipczuk, Marcin and Pilipczuk, Micha\l{} and Saurabh, Saket},
  month = apr,
  year = {2014},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Computational Complexity},
  file = {/Users/billlai/Zotero/storage/FSVB3I9L/Lokshtanov et al. - 2014 - Fixed-parameter tractable canonization and isomorp.pdf;/Users/billlai/Zotero/storage/N86P8UEC/1404.html}
}

@article{SanthanamCircuitLowerBounds2009,
  title = {Circuit {{Lower Bounds}} for {{Merlin}}\textendash{{Arthur Classes}}},
  volume = {39},
  issn = {0097-5397},
  doi = {10.1137/070702680},
  abstract = {We show that for each \$k$>$0\$, \$$\backslash$mathsf\{MA\}/1\$ (\$$\backslash$mathsf\{MA\}\$ with 1 bit of advice) does not have circuits of size \$n\^k\$. This implies the first superlinear circuit lower bounds for the promise versions of the classes \$$\backslash$mathsf\{MA\}\$, \$$\backslash$mathsf\{AM\}\$, and \$$\backslash$mathsf\{ZPP\}\_\{$\backslash$parallel\}\^\{$\backslash$mathsf\{NP\}\}\$. We extend our main result in several ways. For each k, we give an explicit language in \$($\backslash$mathsf\{MA\}$\backslash$cap$\backslash$mathsf\{coMA\})/1\$ which does not have circuits of size \$n\^k\$. We also adapt our lower bound to the average-case setting; i.e., we show that \$$\backslash$mathsf\{MA\}/1\$ cannot be solved on more than \$1/2+1/n\^k\$ fraction of inputs of length n by circuits of size \$n\^k\$. Furthermore, we prove that \$$\backslash$mathsf\{MA\}\$ does not have arithmetic circuits of size \$n\^k\$ for any k. As a corollary to our main result, we obtain that derandomization of \$$\backslash$mathsf\{MA\}/O(1)\$ implies the existence of pseudorandom generators computable using \$O(1)\$ bits of advice.},
  number = {3},
  journal = {SIAM Journal on Computing},
  author = {Santhanam, R.},
  month = jan,
  year = {2009},
  pages = {1038-1061},
  file = {/Users/billlai/Zotero/storage/T7EIJBQN/Santhanam - 2009 - Circuit Lower Bounds for Merlin–Arthur Classes.pdf;/Users/billlai/Zotero/storage/JYZF9CGE/070702680.html}
}

@book{Mas-ColellMicroeconomictheory1995,
  title = {Microeconomic Theory},
  volume = {1},
  publisher = {{Oxford university press New York}},
  author = {{Mas-Colell}, Andreu and Whinston, Michael Dennis and Green, Jerry R.},
  year = {1995},
  file = {/Users/billlai/Zotero/storage/VUBSURWU/Mas-Colell et al. - 1995 - Microeconomic theory.pdf;/Users/billlai/Zotero/storage/XYVW6HD2/Mas-Colell et al. - 1995 - Microeconomic theory.pdf}
}

@techreport{KlempererAuctionsTheoryPractice2004,
  title = {Auctions: {{Theory}} and {{Practice}}},
  shorttitle = {Auctions},
  abstract = {This book is a non-technical introduction to auction theory; its practical application in auction design (including many examples); and its uses in other parts of economics. It can be used for a graduate course on auction theory, or \textendash{} by picking selectively \textendash{} an advanced undergraduate or MBA course on auctions and auction design. Part A introduces the basic theory. Part B shows how modern auction-theoretic tools illuminate a range of mainstream economic questions that are superficially unconnected with auctions. Part C discusses practical auction design. Part D describes the one-hundred-billion dollar 3G mobile-phone license auctions. None of the writing is technical, except in the Appendices. The material was presented as the inaugural (2003) Toulouse Lectures in Economics and is forthcoming at Princeton University Press. This document contains the Contents, Preface and Introduction to the book. A draft of the FULL BOOK is available at http://www.paulklemperer.org.},
  language = {en},
  number = {2004-W09},
  institution = {{Economics Group, Nuffield College, University of Oxford}},
  author = {Klemperer, Paul},
  month = mar,
  year = {2004},
  keywords = {3G,Auction Theory,Auctions,Bidding,Mechanism Design,Spectrum Auctions,Telecommunications,UMTS},
  file = {/Users/billlai/Zotero/storage/SMNYLPGF/Klemperer - 2004 - Auctions Theory and Practice.pdf;/Users/billlai/Zotero/storage/SQ4G9WBB/049.html}
}

@article{MilgromRationalexpectationsinformation1981,
  title = {Rational Expectations, Information Acquisition, and Competitive Bidding},
  journal = {Econometrica: Journal of the Econometric Society},
  author = {Milgrom, Paul R.},
  year = {1981},
  pages = {921--943},
  file = {/Users/billlai/Zotero/storage/2YRA4G87/Milgrom - 1981 - Rational expectations, information acquisition, an.pdf;/Users/billlai/Zotero/storage/HG8QPF5C/1912511.html}
}

@article{MaskintheoryimplementationNash1985,
  title = {The Theory of Implementation in {{Nash}} Equilibrium: {{A}} Survey},
  shorttitle = {The Theory of Implementation in {{Nash}} Equilibrium},
  journal = {Social goals and social organization},
  author = {Maskin, Eric},
  year = {1985},
  pages = {173--204},
  file = {/Users/billlai/Zotero/storage/HKHU8XE3/Maskin - 1985 - The theory of implementation in Nash equilibrium .pdf;/Users/billlai/Zotero/storage/9BPLRZS9/books.html}
}

@inproceedings{Lavicharacterizationtruthfulcombinatorial2003,
  title = {Towards a Characterization of Truthful Combinatorial Auctions},
  booktitle = {Foundations of {{Computer Science}}, 2003. {{Proceedings}}. 44th {{Annual IEEE Symposium}} On},
  publisher = {{IEEE}},
  author = {Lavi, Ron and Mu'Alem, Ahuva and Nisan, Noam},
  year = {2003},
  pages = {574--583},
  file = {/Users/billlai/Zotero/storage/XDQ4ZG98/Lavi et al. - 2003 - Towards a characterization of truthful combinatori.pdf;/Users/billlai/Zotero/storage/7MBQGIGC/1238230.html}
}

@article{HarsanyiGamesincompleteinformation1967,
  title = {Games with Incomplete Information Played by ``{{Bayesian}}'' Players, {{I}}\textendash{{III Part I}}. {{The}} Basic Model},
  volume = {14},
  number = {3},
  journal = {Management science},
  author = {Harsanyi, John C.},
  year = {1967},
  pages = {159--182},
  file = {/Users/billlai/Zotero/storage/37NIRENP/Harsanyi - 1967 - Games with incomplete information played by “Bayes.pdf;/Users/billlai/Zotero/storage/ZV5N55NX/mnsc.14.3.html}
}

@article{BikhchandaniWeakmonotonicitycharacterizes2006,
  title = {Weak Monotonicity Characterizes Deterministic Dominant-Strategy Implementation},
  volume = {74},
  number = {4},
  journal = {Econometrica},
  author = {Bikhchandani, Sushil and Chatterji, Shurojit and Lavi, Ron and Mu'alem, Ahuva and Nisan, Noam and Sen, Arunava},
  year = {2006},
  pages = {1109--1132},
  file = {/Users/billlai/Zotero/storage/CFHS5Q3V/Bikhchandani et al. - 2006 - Weak monotonicity characterizes deterministic domi.pdf}
}

@article{Robertscharacterizationimplementablechoice1979,
  title = {The Characterization of Implementable Choice Rules},
  volume = {12},
  number = {2},
  journal = {Aggregation and revelation of preferences},
  author = {Roberts, Kevin},
  year = {1979},
  pages = {321--348}
}

@article{MooreSubgameperfectimplementation1988,
  title = {Subgame Perfect Implementation},
  journal = {Econometrica: Journal of the Econometric Society},
  author = {Moore, John and Repullo, Rafael},
  year = {1988},
  pages = {1191--1220},
  file = {/Users/billlai/Zotero/storage/BY4GRRTU/Moore and Repullo - 1988 - Subgame perfect implementation.pdf;/Users/billlai/Zotero/storage/3DCQ8CKE/1911364.html}
}

@article{HoltJrCompetitivebiddingcontracts1980,
  title = {Competitive Bidding for Contracts under Alternative Auction Procedures},
  volume = {88},
  number = {3},
  journal = {Journal of political Economy},
  author = {Holt Jr, Charles A.},
  year = {1980},
  pages = {433--445},
  file = {/Users/billlai/Zotero/storage/VU89FQPJ/260878.html}
}

@article{Wilsonbiddingmodelperfect1977,
  title = {A Bidding Model of Perfect Competition},
  journal = {The Review of Economic Studies},
  author = {Wilson, Robert},
  year = {1977},
  pages = {511--518},
  file = {/Users/billlai/Zotero/storage/RRTXGE93/Wilson - 1977 - A bidding model of perfect competition.pdf;/Users/billlai/Zotero/storage/BKP6GB27/2296904.html}
}

@article{Milgromtheoryauctionscompetitive1982,
  title = {A Theory of Auctions and Competitive Bidding},
  journal = {Econometrica: Journal of the Econometric Society},
  author = {Milgrom, Paul R. and Weber, Robert J.},
  year = {1982},
  pages = {1089--1122},
  file = {/Users/billlai/Zotero/storage/VJCC7JTY/Milgrom and Weber - 1982 - A theory of auctions and competitive bidding.pdf;/Users/billlai/Zotero/storage/LTL85Q24/1911865.html}
}

@article{LehmannTruthrevelationapproximately2002,
  title = {Truth Revelation in Approximately Efficient Combinatorial Auctions},
  volume = {49},
  number = {5},
  journal = {Journal of the ACM (JACM)},
  author = {Lehmann, Daniel and O{\'c}allaghan, Liadan Ita and Shoham, Yoav},
  year = {2002},
  pages = {577--602},
  file = {/Users/billlai/Zotero/storage/VCKNFZXB/citation.html}
}

@article{GreenCharacterizationsatisfactorymechanisms1977,
  title = {Characterization of Satisfactory Mechanisms for the Revelation of Preferences for Public Goods},
  journal = {Econometrica: Journal of the Econometric Society},
  author = {Green, Jerry and Laffont, Jean-Jacques},
  year = {1977},
  pages = {427--438},
  file = {/Users/billlai/Zotero/storage/TPBTR423/Green and Laffont - 1977 - Characterization of satisfactory mechanisms for th.pdf;/Users/billlai/Zotero/storage/4IKZD8T6/1911219.html}
}

@inproceedings{SaksWeakmonotonicitysuffices2005,
  title = {Weak Monotonicity Suffices for Truthfulness on Convex Domains},
  booktitle = {Proceedings of the 6th {{ACM}} Conference on {{Electronic}} Commerce},
  publisher = {{ACM}},
  author = {Saks, Michael and Yu, Lan},
  year = {2005},
  pages = {286--293},
  file = {/Users/billlai/Zotero/storage/DV4KUDE3/Saks and Yu - 2005 - Weak monotonicity suffices for truthfulness on con.pdf;/Users/billlai/Zotero/storage/8Q7KH6XY/citation.html}
}

@article{MyersonOptimalauctiondesign1981,
  title = {Optimal Auction Design},
  volume = {6},
  number = {1},
  journal = {Mathematics of operations research},
  author = {Myerson, Roger B.},
  year = {1981},
  pages = {58--73},
  file = {/Users/billlai/Zotero/storage/GPB3UCP6/Myerson - 1981 - Optimal auction design.pdf;/Users/billlai/Zotero/storage/78JTDNWP/moor.6.1.html}
}

@article{Dasguptaimplementationsocialchoice1979,
  title = {The Implementation of Social Choice Rules: {{Some}} General Results on Incentive Compatibility},
  volume = {46},
  shorttitle = {The Implementation of Social Choice Rules},
  number = {2},
  journal = {The Review of Economic Studies},
  author = {Dasgupta, Partha and Hammond, Peter and Maskin, Eric},
  year = {1979},
  pages = {185--216},
  file = {/Users/billlai/Zotero/storage/HVFWKQCK/Dasgupta et al. - 1979 - The implementation of social choice rules Some ge.pdf;/Users/billlai/Zotero/storage/WYLPA7UW/2297045.html}
}

@article{MyersonEfficientmechanismsbilateral1983,
  title = {Efficient Mechanisms for Bilateral Trading},
  volume = {29},
  number = {2},
  journal = {Journal of economic theory},
  author = {Myerson, Roger B. and Satterthwaite, Mark A.},
  year = {1983},
  pages = {265--281},
  file = {/Users/billlai/Zotero/storage/ACLIRHBX/Myerson and Satterthwaite - 1983 - Efficient mechanisms for bilateral trading.pdf;/Users/billlai/Zotero/storage/U989VMI7/0022053183900480.html}
}

@article{Bartholdicomputationaldifficultymanipulating1989,
  title = {The Computational Difficulty of Manipulating an Election},
  volume = {6},
  number = {3},
  journal = {Social Choice and Welfare},
  author = {Bartholdi, John J. and Tovey, Craig A. and Trick, Michael A.},
  year = {1989},
  pages = {227--241},
  file = {/Users/billlai/Zotero/storage/STQDZRZ6/Bartholdi et al. - 1989 - The computational difficulty of manipulating an el.pdf;/Users/billlai/Zotero/storage/22Z9KZV2/BF00295861.html}
}

@article{GrovesIncentivesteams1973,
  title = {Incentives in Teams},
  journal = {Econometrica: Journal of the Econometric Society},
  author = {Groves, Theodore},
  year = {1973},
  pages = {617--631},
  file = {/Users/billlai/Zotero/storage/CBCER8CZ/Groves - 1973 - Incentives in teams.pdf;/Users/billlai/Zotero/storage/V6IPUNVX/1914085.html}
}

@article{VickreyCounterspeculationauctionscompetitive1961,
  title = {Counterspeculation, Auctions, and Competitive Sealed Tenders},
  volume = {16},
  number = {1},
  journal = {The Journal of finance},
  author = {Vickrey, William},
  year = {1961},
  pages = {8--37}
}

@article{ClarkeMultipartpricingpublic1971,
  title = {Multipart Pricing of Public Goods},
  volume = {11},
  number = {1},
  journal = {Public choice},
  author = {Clarke, Edward H.},
  year = {1971},
  pages = {17--33}
}

@article{SatterthwaiteStrategyproofnessArrowconditions1975,
  title = {Strategy-Proofness and {{Arrow}}'s Conditions: {{Existence}} and Correspondence Theorems for Voting Procedures and Social Welfare Functions},
  volume = {10},
  shorttitle = {Strategy-Proofness and {{Arrow}}'s Conditions},
  number = {2},
  journal = {Journal of economic theory},
  author = {Satterthwaite, Mark Allen},
  year = {1975},
  pages = {187--217},
  file = {/Users/billlai/Zotero/storage/CL47AQC3/Satterthwaite - 1975 - Strategy-proofness and Arrow's conditions Existen.pdf}
}

@book{ArrowSocialchoiceindividual1951,
  title = {Social Choice and Individual Values},
  publisher = {{Yale University Press}},
  author = {Arrow, Kenneth J.},
  year = {1951},
  file = {/Users/billlai/Zotero/storage/XAXRS4E8/books.html}
}

@article{GeanakoplosThreebriefproofs2005,
  title = {Three Brief Proofs of {{Arrow}}'s Impossibility Theorem},
  volume = {26},
  number = {1},
  journal = {Economic Theory},
  author = {Geanakoplos, John},
  year = {2005},
  pages = {211--215},
  file = {/Users/billlai/Zotero/storage/GPM4HP7D/Geanakoplos - 2005 - Three brief proofs of Arrow’s impossibility theore.pdf;/Users/billlai/Zotero/storage/QM36JJPS/s00199-004-0556-7.html}
}

@article{NisanAlgorithmicmechanismdesign2001,
  title = {Algorithmic Mechanism Design},
  volume = {35},
  number = {1-2},
  journal = {Games and Economic Behavior},
  author = {Nisan, Noam and Ronen, Amir},
  year = {2001},
  pages = {166--196},
  file = {/Users/billlai/Zotero/storage/2BIKHUFL/Nisan and Ronen - 2001 - Algorithmic mechanism design.pdf;/Users/billlai/Zotero/storage/WSZ5IUS9/S089982569990790X.html}
}

@article{GibbardManipulationvotingschemes1973,
  title = {Manipulation of Voting Schemes: A General Result},
  shorttitle = {Manipulation of Voting Schemes},
  journal = {Econometrica: journal of the Econometric Society},
  author = {Gibbard, Allan},
  year = {1973},
  pages = {587--601},
  file = {/Users/billlai/Zotero/storage/94DP98U6/Gibbard - 1973 - Manipulation of voting schemes a general result.pdf;/Users/billlai/Zotero/storage/I9U85KSG/1914083.html}
}

@book{KrishnaAuctiontheory2009,
  title = {Auction Theory},
  publisher = {{Academic press}},
  author = {Krishna, Vijay},
  year = {2009},
  file = {/Users/billlai/Zotero/storage/RM8QYECR/books.html}
}

@misc{KennethArrowSocial,
  title = {Kenneth {{Arrow}}, {{Social Choice}} and {{Individual Values}} - {{PhilPapers}}},
  howpublished = {https://philpapers.org/rec/ARRIVA},
  file = {/Users/billlai/Zotero/storage/V4YACYKV/ARRIVA.html}
}

@article{ArrowIndividualvaluessocial1951,
  title = {Individual Values and Social Choice},
  author = {Arrow, Kenneth},
  year = {1951}
}

@article{AgrawalFurtherOptimalRegret2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1209.3353},
  primaryClass = {cs, stat},
  title = {Further {{Optimal Regret Bounds}} for {{Thompson Sampling}}},
  abstract = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. In this paper, we provide a novel regret analysis for Thompson Sampling that simultaneously proves both the optimal problem-dependent bound of \$(1+$\backslash$epsilon)$\backslash$sum\_i $\backslash$frac\{$\backslash$ln T\}\{$\backslash$Delta\_i\}+O($\backslash$frac\{N\}\{$\backslash$epsilon\^2\})\$ and the first near-optimal problem-independent bound of \$O($\backslash$sqrt\{NT$\backslash$ln T\})\$ on the expected regret of this algorithm. Our near-optimal problem-independent bound solves a COLT 2012 open problem of Chapelle and Li. The optimal problem-dependent regret bound for this problem was first proven recently by Kaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques are conceptually simple, easily extend to distributions other than the Beta distribution, and also extend to the more general contextual bandits setting [Manuscript, Agrawal and Goyal, 2012].},
  journal = {arXiv:1209.3353 [cs, stat]},
  author = {Agrawal, Shipra and Goyal, Navin},
  month = sep,
  year = {2012},
  keywords = {Computer Science - Data Structures and Algorithms,68W40; 68Q25,Computer Science - Learning,F.2.0,Statistics - Machine Learning},
  file = {/Users/billlai/Zotero/storage/G2MGU7DF/Agrawal and Goyal - 2012 - Further Optimal Regret Bounds for Thompson Samplin.pdf;/Users/billlai/Zotero/storage/64HERMID/1209.html}
}

@article{AuerNonstochasticMultiarmedBandit2002,
  title = {The {{Nonstochastic Multiarmed Bandit Problem}}},
  volume = {32},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/S0097539701398375},
  abstract = {In the multiarmed bandit problem, a gambler must decide which arm of K nonidentical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines.},
  language = {en},
  number = {1},
  journal = {SIAM Journal on Computing},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicol{\`o} and Freund, Yoav and Schapire, Robert E.},
  month = jan,
  year = {2002},
  pages = {48-77},
  file = {/Users/billlai/Zotero/storage/MWMVYKN9/Auer et al. - 2002 - The Nonstochastic Multiarmed Bandit Problem.pdf}
}

@article{FlajoletHyperLogLoganalysisnearoptimal,
  title = {{{HyperLogLog}}: The Analysis of a near-Optimal Cardinality Estimation Algorithm},
  language = {en},
  author = {Flajolet, Philippe and Fusy, {\'E}ric and Gandouet, Olivier},
  pages = {20},
  file = {/Users/billlai/Zotero/storage/TFKJ37R5/Flajolet et al. - HyperLogLog the analysis of a near-optimal cardin.pdf}
}

@article{FlajoletUnderstandingHyperLogLogNearOptimal,
  title = {Understanding the {{HyperLogLog}}: A {{Near}}-{{Optimal Cardinality Estimation Algorithm}}},
  abstract = {The HyperLogLog algorithm (HLL) is a method to estimate the number of distinct elements in large datasets i.e. cardinality, in a single pass, and using a very small amount of memory. The HLL algorithm is an optimization of the method presented in 2003 by Durand and Flajolet in the paper LogLog Counting of Large Cardinalities. In this report we analyze the so called cornerstone of Big Data infrastructures, give a detailed description of the algorithm and explain the mathematical intuition behind it.},
  language = {en},
  author = {Flajolet, Philippe and Fusy, Eric and Gandouet, Olivier and Meunier, Frederic},
  pages = {14},
  file = {/Users/billlai/Zotero/storage/44BGHCNG/Flajolet et al. - Understanding the HyperLogLog a Near-Optimal Card.pdf}
}

@inproceedings{HeuleHyperLogLogpracticealgorithmic2013,
  title = {{{HyperLogLog}} in Practice: Algorithmic Engineering of a State of the Art Cardinality Estimation Algorithm},
  isbn = {978-1-4503-1597-5},
  shorttitle = {{{HyperLogLog}} in Practice},
  doi = {10.1145/2452376.2452456},
  abstract = {Cardinality estimation has a wide range of applications and is of particular importance in database systems. Various algorithms have been proposed in the past, and the HyperLogLog algorithm is one of them. In this paper, we present a series of improvements to this algorithm that reduce its memory requirements and significantly increase its accuracy for an important range of cardinalities. We have implemented our proposed algorithm for a system at Google and evaluated it empirically, comparing it to the original HyperLogLog algorithm. Like HyperLogLog, our improved algorithm parallelizes perfectly and computes the cardinality estimate in a single pass.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Heule, Stefan and Nunkesser, Marc and Hall, Alexander},
  year = {2013},
  pages = {683},
  file = {/Users/billlai/Zotero/storage/2RI9BY3J/Heule et al. - 2013 - HyperLogLog in practice algorithmic engineering o.pdf}
}

@misc{ShenQiDeHyperLogLogSuanFarainybowe,
  title = {{{神奇的HyperLogLog算法}} $\cdot$ Rainybowe},
  howpublished = {http://www.rainybowe.com/blog/2017/07/13/\%E7\%A5\%9E\%E5\%A5\%87\%E7\%9A\%84HyperLogLog\%E7\%AE\%97\%E6\%B3\%95/index.html},
  file = {/Users/billlai/Zotero/storage/P5RIMKIJ/index.html}
}

@article{KejariwalRealtimeanalytics2015,
  title = {Real Time Analytics: Algorithms and Systems},
  volume = {8},
  issn = {21508097},
  shorttitle = {Real Time Analytics},
  doi = {10.14778/2824032.2824132},
  abstract = {Velocity is one of the 4 Vs commonly used to characterize Big Data [5]. In this regard, Forrester remarked the following in Q3 2014 [8]: ``The high velocity, white-water flow of data from innumerable real-time data sources such as market data, Internet of Things, mobile, sensors, clickstream, and even transactions remain largely unnavigated by most firms. The opportunity to leverage streaming analytics has never been greater.'' Example use cases of streaming analytics include, but not limited to: (a) visualization of business metrics in real-time (b) facilitating highly personalized experiences (c) expediting response during emergencies. Streaming analytics is extensively used in a wide variety of domains such as healthcare, e-commerce, financial services, telecommunications, energy and utilities, manufacturing, government and transportation.},
  language = {en},
  number = {12},
  journal = {Proceedings of the VLDB Endowment},
  author = {Kejariwal, Arun and Kulkarni, Sanjeev and Ramasamy, Karthik},
  month = aug,
  year = {2015},
  pages = {2040-2041},
  file = {/Users/billlai/Zotero/storage/GVHX3S7U/Kejariwal et al. - 2015 - Real time analytics algorithms and systems.pdf}
}

@inproceedings{TingStreamedapproximatecounting2014,
  title = {Streamed Approximate Counting of Distinct Elements: Beating Optimal Batch Methods},
  isbn = {978-1-4503-2956-9},
  shorttitle = {Streamed Approximate Counting of Distinct Elements},
  doi = {10.1145/2623330.2623669},
  abstract = {Counting the number of distinct elements in a large dataset is a common task in web applications and databases. This problem is difficult in limited memory settings where storing a large hash table table is intractable. This paper advances the state of the art in probabilistic methods for estimating the number of distinct elements in a streaming setting New streaming algorithms are given that provably beat the ''optimal'' errors for Min-count and HyperLogLog while using the same sketch.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Ting, Daniel},
  year = {2014},
  pages = {442-451},
  file = {/Users/billlai/Zotero/storage/LVUIBJ9M/Ting - 2014 - Streamed approximate counting of distinct elements.pdf}
}

@unpublished{ChabchoubSlidingHyperLogLogEstimating2010,
  title = {Sliding {{HyperLogLog}}: {{Estimating}} Cardinality in a Data Stream},
  shorttitle = {Sliding {{HyperLogLog}}},
  abstract = {In this paper, a new algorithm estimating the number of active flows in a data stream is proposed. This algorithm adapts the HyperLogLog algorithm of Flajolet et al to the data stream processing by adding a sliding window mechanism. It has the advantage to estimate at any time the number of flows seen over any duration bounded by the length of the sliding window. The estimate is very accurate with a standard error of about 1.04/$\backslash$sqrt\{m\} (the same as in HyperLogLog algorithm). As the new algorithm answers more flexible queries, it needs an additional memory storage compared to HyerLogLog algorithm. It is proved that this additional memory is at most equal to 5mln(n/m) bytes, where n is the real number of flows in the sliding window. For instance, with an additional memory of only 35kB, a standard error of about 3\% can be achieved for a data stream of several million flows. Theoretical results are validated on both real and synthetic traffic.},
  author = {Chabchoub, Yousra and H{\'e}brail, Georges},
  month = mar,
  year = {2010},
  file = {/Users/billlai/Zotero/storage/YUTYQ3DX/Chabchoub and Hébrail - 2010 - Sliding HyperLogLog Estimating cardinality in a d.pdf}
}

@article{ErtlNewCardinalityEstimation2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.07290},
  primaryClass = {cs},
  title = {New {{Cardinality Estimation Methods}} for {{HyperLogLog Sketches}}},
  abstract = {This work presents new cardinality estimation methods for data sets recorded by HyperLogLog sketches. A simple derivation of the original estimator was found, that also gives insight how to correct its deficiencies. The result is an improved estimator that is unbiased over the full cardinality range, is easy computable, and does not rely on empirically determined data as previous approaches. Based on the maximum likelihood principle a second unbiased estimation method is presented which can also be extended to estimate cardinalities of union, intersection, or relative complements of two sets that are both represented as HyperLogLog sketches. Experimental results show that this approach is more precise than the conventional technique using the inclusion-exclusion principle.},
  journal = {arXiv:1706.07290 [cs]},
  author = {Ertl, Otmar},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/billlai/Zotero/storage/YTTIUCN7/Ertl - 2017 - New Cardinality Estimation Methods for HyperLogLog.pdf;/Users/billlai/Zotero/storage/2T72WBXR/1706.html}
}

@misc{DamnCoolAlgorithms,
  title = {Damn {{Cool Algorithms}}: {{Cardinality Estimation}} - {{Nick}}'s {{Blog}}},
  howpublished = {http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation},
  file = {/Users/billlai/Zotero/storage/3UPGG7VG/Dam-Cool-Algorithms-Cardinality-Estimation.html}
}

@inproceedings{KarpRandomizedrumorspreading2000,
  title = {Randomized Rumor Spreading},
  isbn = {978-0-7695-0850-4},
  doi = {10.1109/SFCS.2000.892324},
  language = {en},
  publisher = {{IEEE Comput. Soc}},
  author = {Karp, R. and Schindelhauer, C. and Shenker, S. and Vocking, B.},
  year = {2000},
  pages = {565-574},
  file = {/Users/billlai/Zotero/storage/4VSZ92SN/Karp et al. - 2000 - Randomized rumor spreading.pdf}
}

@article{DoerrRandomizedRumorSpreading2017,
  title = {Randomized {{Rumor Spreading Revisited}}},
  doi = {10.4230/lipics.icalp.2017.138},
  abstract = {We develop a simple and generic method to analyze randomized rumor spreading processes in fully connected networks. In contrast to all previous works, which heavily exploit the precise definition of the process under investigation, we only need to understand the probability and the covariance of the events that uninformed nodes become informed. This universality allows us to easily analyze the classic push, pull, and push-pull protocols both in their pure version and in several variations such as messages failing with constant probability or nodes calling a random number of others each round. Some dynamic models can be analyzed as well, e.g., when the network is a G(n, p) random graph sampled independently each round [Clementi et al. (ESA 2013)].},
  language = {en},
  author = {Doerr, Benjamin and Kostrygin, Anatolii},
  editor = {Herbstritt, Marc},
  year = {2017},
  file = {/Users/billlai/Zotero/storage/7ME5N948/Doerr and Kostrygin - 2017 - Randomized Rumor Spreading Revisited.pdf}
}

@incollection{AvinFasterRumorSpreading2013,
  series = {Lecture Notes in Computer Science},
  title = {Faster {{Rumor Spreading}}: {{Breaking}} the Log$<${{Emphasis Type}}="{{Italic}}"$>$n$<$/{{Emphasis}}$>$ {{Barrier}}},
  isbn = {978-3-642-41526-5 978-3-642-41527-2},
  shorttitle = {Faster {{Rumor Spreading}}},
  abstract = {O(logn) rounds has been a well known upper bound for rumor spreading using push\&pull in the random phone call model (i.e., uniform gossip in the complete graph). A matching lower bound of $\Omega$(logn) is also known for this special case. Under the assumptions of this model and with a natural addition that nodes can call a partner once they learn its address (e.g., its IP address) we present a new distributed, address-oblivious and robust algorithm that uses push\&pull with pointer jumping to spread a rumor to all nodes in only O(logn-----$\surd$)O(log⁡n)O($\backslash$sqrt\{$\backslash$log n\}) rounds, w.h.p. This algorithm can also cope with F=o(n/2logn$\surd$)F=o(n/2log⁡n)F= o(n/2\^\{$\backslash$sqrt\{$\backslash$log n\}\}) node failures, in which case all but O(F) nodes become informed within O(logn-----$\surd$)O(log⁡n)O($\backslash$sqrt\{$\backslash$log n\}) rounds, w.h.p.},
  language = {en},
  booktitle = {Distributed {{Computing}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Avin, Chen and Els{\"a}sser, Robert},
  month = oct,
  year = {2013},
  pages = {209-223},
  file = {/Users/billlai/Zotero/storage/VIBE5D6R/Avin and Elsässer - 2013 - Faster Rumor Spreading Breaking the logEmphasis .pdf;/Users/billlai/Zotero/storage/YH7ZDLXH/978-3-642-41527-2_15.html},
  doi = {10.1007/978-3-642-41527-2_15}
}

@inproceedings{HaeuplerOptimalGossipDirect2014,
  address = {New York, NY, USA},
  series = {PODC '14},
  title = {Optimal {{Gossip}} with {{Direct Addressing}}},
  isbn = {978-1-4503-2944-6},
  doi = {10.1145/2611462.2611489},
  abstract = {Gossip algorithms spread information in distributed networks by nodes repeatedly forwarding information to a few random contacts. By their very nature, gossip algorithms tend to be distributed and fault tolerant. If done right, they can also be fast and message-efficient. A common model for gossip communication is the random phone call model, in which in each synchronous round each node can PUSH or PULL information to or from a random other node. For example, Karp et al. [FOCS 2000] gave algorithms in this model that spread a message to all nodes in $\Theta$(log n) rounds while sending only O(log log n) messages per node on average. They also showed that at least $\Theta$(log n) rounds are necessary in this model and that algorithms achieving this round-complexity need to send $\omega$(1) messages per node on average. Recently, Avin and Elsasser [DISC 2013], studied the random phone call model with the natural and commonly used assumption of direct addressing. Direct addressing allows nodes to directly contact nodes whose ID (e.g., IP address) was learned before. They show that in this setting, one can "break the log n barrier" and achieve a gossip algorithm running in O($\surd$log n) rounds, albeit while using O($\surd$log n) messages per node. In this paper we study the same model and give a simple gossip algorithm which spreads a message in only O(log log n) rounds. We furthermore prove a matching $\Omega$(log log n) lower bound which shows that this running time is best possible. In particular we show that any gossip algorithm takes with high probability at least 0.99 log log n rounds to terminate. Lastly, our algorithm can be tweaked to send only O(1) messages per node on average with only O(log n) bits per message. Our algorithm therefore simultaneously achieves the optimal round-, message-, and bit-complexity for this setting. As all prior gossip algorithms, our algorithm is also robust against failures. In particular, if in the beginning an oblivious adversary fails any F nodes our algorithm still, with high probability, informs all but o(F) surviving nodes.},
  booktitle = {Proceedings of the 2014 {{ACM Symposium}} on {{Principles}} of {{Distributed Computing}}},
  publisher = {{ACM}},
  author = {Haeupler, Bernhard and Malkhi, Dahlia},
  year = {2014},
  keywords = {information dissemination,rumor spreading,direct addressing,gossip,peer-to-peer (P2P),pointer jumping},
  pages = {176--185},
  file = {/Users/billlai/Zotero/storage/5YIM3ZPN/Haeupler and Malkhi - 2014 - Optimal Gossip with Direct Addressing.pdf}
}

@article{HaeuplerAnalyzingNetworkCoding2016,
  title = {Analyzing {{Network Coding}} ({{Gossip}}) {{Made Easy}}},
  volume = {63},
  issn = {0004-5411},
  doi = {10.1145/2629696},
  abstract = {We introduce projection analysis\textemdash{}a new technique to analyze the stopping time of protocols that are based on random linear network coding (RLNC). Projection analysis drastically simplifies, extends, and strengthens previous results on RLNC gossip protocols. We analyze RLNC gossip in a general framework for network and communication models that encompasses and unifies the models used previously in this context. We show, in most settings for the first time, that the RLNC gossip converges with high probability in optimal time. Most stopping times are of the form O(k + T), where k is the number of messages to be distributed and T is the time it takes to disseminate one message. This means RLNC gossip achieves ``perfect pipelining.'' Our analysis directly extends to highly dynamic networks in which the topology can change completely at any time. This remains true, even if the network dynamics are controlled by a fully adaptive adversary that knows the complete network state. Virtually nothing besides simple O(kT) sequential flooding protocols was previously known for such a setting. While RLNC gossip works in this wide variety of networks our analysis remains the same and extremely simple. This contrasts with more complex proofs that were put forward to give less strong results for various special cases.},
  number = {3},
  journal = {J. ACM},
  author = {Haeupler, Bernhard},
  month = aug,
  year = {2016},
  keywords = {gossip,dynamic networks,multicast,Random linear network coding},
  pages = {26:1--26:22},
  file = {/Users/billlai/Zotero/storage/SRB6JGAT/Haeupler - 2016 - Analyzing Network Coding (Gossip) Made Easy.pdf}
}

@inproceedings{GuoGossipvsMarkov2015,
  address = {Philadelphia, PA, USA},
  series = {SODA '15},
  title = {Gossip vs. {{Markov Chains}}, and {{Randomness}}-Efficient {{Rumor Spreading}}},
  abstract = {We study gossip algorithms for the rumor spreading problem which asks one node to deliver a rumor to all nodes in an unknown network, and every node is only allowed to call one neighbor in each round. In this work we introduce two fundamentally new techniques in studying the rumor spreading problem: First, we establish a new connection between the rumor spreading process in an arbitrary graph and certain Markov chains. While most previous work analyzed the rumor spreading time in general graphs by studying the rate of the number of (un-)informed nodes after every round, we show that the mixing time of a certain Markov chain suffices to bound the rumor spreading time in an arbitrary graph. Second, we construct a reduction from rumor spreading processes to branching programs. This reduction gives us a general framework to derandomize the rumor spreading and other gossip processes. In particular, we show that, for any n-vertex expander graph, there is a protocol which informs every node in O(log n) rounds with high probability, and uses O(log n $\cdot$ log log n) random bits in total. The runtime of our protocol is tight, and the randomness requirement of O(log n $\cdot$ log log n) random bits almost matches the lower bound of $\Omega$(log n) random bits. We further show that, for many graph families (defined with respect to the expansion and the degree), O(poly log n) random bits in total suffice for fast rumor spreading. These results give us an almost complete understanding of the role of randomness in the rumor spreading process, which was extensively studied over the past years.},
  booktitle = {Proceedings of the {{Twenty}}-Sixth {{Annual ACM}}-{{SIAM Symposium}} on {{Discrete Algorithms}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Guo, Zeyu and Sun, He},
  year = {2015},
  pages = {411--430},
  file = {/Users/billlai/Zotero/storage/QV5WESHU/Guo and Sun - 2015 - Gossip vs. Markov Chains, and Randomness-efficient.pdf}
}

@inproceedings{ElsasserInfluenceGraphDensity2015,
  title = {On the {{Influence}} of {{Graph Density}} on {{Randomized Gossiping}}},
  doi = {10.1109/IPDPS.2015.32},
  abstract = {Information dissemination is a fundamental problem in parallel and distributed computing. In its simplest variant, known as the broadcasting problem, a single message has to be spread among all nodes of a graph. A prominent communication protocol for this problem is based on the socalled random phone call model (Karp et al., FOCS 2000). In each step, every node opens a communication channel to a randomly chosen neighbor, which can then be used for bidirectional communication. Motivated by replicated databases and peer-to-peer networks, Berenbrink et al., ICALP 2010, considered the so-called gossiping problem in the random phone call model. There, each node starts with its own message and all messages have to be disseminated to all nodes in the network. They showed that any O(log n)-time algorithm in complete graphs requires $\Omega$(log n) message transmissions per node to complete gossiping, with high probability, while it is known that in the case of broadcasting the average number of message transmissions per node is O(log log n). Furthermore, they explored different possibilities on how to reduce the communication overhead of randomized gossiping in complete graphs. It is known that the O(nloglogn) bound on the number of message transmissions produced by randomized broadcasting in complete graphs cannot be achieved in sparse graphs even if they have best expansion and connectivity properties. In this paper, we analyze whether a similar influence of the graph density also holds w.r.t. the performance of gossiping. We study analytically and empirically the communication overhead generated by gossiping algorithms w.r.t. the random phone call model in random graphs and also consider simple modifications of the random phone call model in these graphs. Our results indicate that, unlike in broadcasting, there seems to be no significant difference between the performance of randomized gossiping in complete graphs and sparse random graphs. Furthermore, our simulations - llustrate that by tuning the parameters of our algorithms, we can significantly reduce the communication overhead compared to the traditional push-pull approach in the graphs we consider.},
  booktitle = {2015 {{IEEE International Parallel}} and {{Distributed Processing Symposium}}},
  author = {Els{\"a}sser, R. and Kaaser, D.},
  month = may,
  year = {2015},
  keywords = {Algorithm design and analysis,graph theory,parallel algorithms,Complexity theory,probability,distributed computing,information dissemination,Analytical models,bidirectional communication,Broadcasting,broadcasting problem,communication channel,Communication channels,communication complexity,communication overhead,communication protocol,complete graph,connectivity property,graph density,graph nodes,message dissemination,message spreading,message transmission,network nodes,network theory (graphs),node message,parallel computing,parameter tuning,Peer-to-peer computing,Protocols,random phone call model,random processes,randomised algorithms,randomized gossiping,randomly chosen neighbor,sparse random graph},
  pages = {521-531},
  file = {/Users/billlai/Zotero/storage/95JKB5PQ/7161540.html}
}

@article{MiltersenSuperpolynomialhalfexponentialcircuit,
  title = {Super-Polynomial versus Half-Exponential Circuit Size in the Exponential Hierarchy},
  abstract = {Lower bounds on circuit size were previously established for functions in $\Sigma$p2, ZPPNP, $\Sigma$e2xp, ZPEXPNP and MAexp. We investigate the general question: Given a time bound f (n). What is the best circuit size lower bound that can be shown for the classes MA-TIME[f ], ZP-TIMENP[f ], . . . using the techniques currently known? For the classes MAexp, ZPEXPNP and $\Sigma$e2xp, the answer we get is ``halfexponential''. Informally, a function f is said to be half-exponential if f composed with itself is exponential.},
  language = {en},
  author = {Miltersen, Peter Bro and Vinodchandran, N V and Watanabe, Osamu},
  pages = {17},
  file = {/Users/billlai/Zotero/storage/8LD6BTD8/Miltersen et al. - Super-polynomial versus half-exponential circuit s.pdf}
}

@article{BabaiBPPHASSUBEXPONENTIAL,
  title = {{{BPP HAS SUBEXPONENTIAL TIME SIMULATIONS UNLESS EXPTIME HAS PUBLISHABLE PROOFS}}},
  language = {en},
  author = {Babai, Laszlo and Fortnow, Lance and Nisan, Noam and Wigderson, Avi},
  pages = {12},
  file = {/Users/billlai/Zotero/storage/VQSY4SA6/Babai et al. - BPP HAS SUBEXPONENTIAL TIME SIMULATIONS UNLESS EXP.pdf}
}

@inproceedings{BabaiCheckingcomputationspolylogarithmic1991,
  title = {Checking Computations in Polylogarithmic Time},
  isbn = {978-0-89791-397-3},
  doi = {10.1145/103418.103428},
  abstract = {Motivated by Manuel Blum's concept of instance checking, we consider new, very fast and generic mechanisms of checking computations. Our results exploit recent advances in interactive proof protocols LFKN92], Sha92], and especially the MIP = NEXP protocol from BFL91].},
  language = {en},
  publisher = {{ACM Press}},
  author = {Babai, L{\'a}szl{\'o} and Fortnow, Lance and Levin, Leonid A. and Szegedy, Mario},
  year = {1991},
  pages = {21-32},
  file = {/Users/billlai/Zotero/storage/E5E5W46U/Babai et al. - 1991 - Checking computations in polylogarithmic time.pdf}
}

@misc{zotero-576,
  howpublished = {https://cseweb.ucsd.edu/\textasciitilde{}russell/survey.ps},
  file = {/Users/billlai/Zotero/storage/TRU7XBP7/survey.html}
}

@article{CoudertRevisitingDecompositionClique2018,
  title = {Revisiting {{Decomposition}} by {{Clique Separators}}},
  volume = {32},
  issn = {0895-4801},
  doi = {10.1137/16M1059837},
  abstract = {We study the complexity of decomposing a graph by means of clique separators. This common algorithmic tool, first introduced by Tarjan, allows one to cut a graph into smaller pieces, and so it can be applied to preprocess the graph in the computation of optimization problems. However, the best-known algorithms for computing a decomposition have respective \$\{$\backslash$cal O\}(nm)\$-time and \$\{$\backslash$cal O\}(n\^\{(3+$\backslash$alpha)/2\}) = o(n\^\{2.69\})\$-time complexity with \$$\backslash$alpha $<$ 2.3729\$ being the exponent for matrix multiplication. Such running times are prohibitive for large graphs. Here we prove that for every graph \$G\$, a decomposition can be computed in \$\{$\backslash$cal O\}(T(G) + $\backslash$min$\backslash$\{n\^\{$\backslash$alpha\},$\backslash$omega\^2 n$\backslash$\})\$-time with \$T(G)\$ and \$$\backslash$omega\$ being, respectively, the time needed to compute a minimal triangulation of \$G\$ and the clique-number of \$G\$. In particular, it implies that every graph can be decomposed by clique separators in \$\{$\backslash$cal O\}(n\^\{$\backslash$alpha\}$\backslash$log n)\$-time. Based on prior work from Kratsch et al., we prove in addition that decomposing a graph by clique-separators is as least as hard as triangle detection. Therefore, the existence of any \$o(n\^\{$\backslash$alpha\})\$-time algorithm for this problem would be a significant breakthrough in the algorithmic field. Finally, our main result implies that planar graphs, bounded-treewidth graphs, and bounded-degree graphs can be decomposed by clique separators in linear or quasi-linear time.},
  number = {1},
  journal = {SIAM Journal on Discrete Mathematics},
  author = {Coudert, D. and Ducoffe, G.},
  month = jan,
  year = {2018},
  pages = {682-694},
  file = {/Users/billlai/Zotero/storage/RN75KACJ/Coudert and Ducoffe - 2018 - Revisiting Decomposition by Clique Separators.pdf;/Users/billlai/Zotero/storage/Y74JVTT3/16M1059837.html}
}

@article{LeimerOptimaldecompositionclique1993,
  title = {Optimal Decomposition by Clique Separators},
  volume = {113},
  issn = {0012-365X},
  doi = {10.1016/0012-365X(93)90510-Z},
  abstract = {Decompositions of a graph by clique separators are investigated which have the additional property that they do not generate new maximal prime subgraphs. Using such decompositions is preferable in many applications, since they lead to a minimal system of derived subgraphs. The methods used in the proofs are familiar from the investigations of chordal graphs and acyclic hypergraphs and some well-known results for these (hyper-) graphs are shown to be simple special cases of results for maximal prime subgraphs. Tarjan has described an O(nm)-time algorithm to decompose a graph with n vertices and m edges by means of clique separators. This algorithm is modified, so that no new maximal prime subgraphs are generated, i.e. so that a graph is decomposed exactly into its maximal prime subgraphs which is the unique minimal derived system of prime subgraphs.},
  number = {1},
  journal = {Discrete Mathematics},
  author = {Leimer, Hanns-Georg},
  month = apr,
  year = {1993},
  pages = {99-123},
  file = {/Users/billlai/Zotero/storage/IZUYPCFR/Leimer - 1993 - Optimal decomposition by clique separators.pdf;/Users/billlai/Zotero/storage/HNXC26PY/0012365X9390510Z.html}
}

@article{HaeuplerSimpleFastDeterministic2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1210.1193},
  primaryClass = {cs, math},
  title = {Simple, {{Fast}} and {{Deterministic Gossip}} and {{Rumor Spreading}}},
  doi = {10.1137/1.9781611973105.51},
  abstract = {We study gossip algorithms for the rumor spreading problem which asks each node to deliver a rumor to all nodes in an unknown network. Gossip algorithms allow nodes only to call one neighbor per round and have recently attracted attention as message efficient, simple and robust solutions to the rumor spreading problem. Recently, non-uniform random gossip schemes were devised to allow efficient rumor spreading in networks with bottlenecks. In particular, [Censor-Hillel et al., STOC'12] gave an O(log\^3 n) algorithm to solve the 1-local broadcast problem in which each node wants to exchange rumors locally with its 1-neighborhood. By repeatedly applying this protocol one can solve the global rumor spreading quickly for all networks with small diameter, independently of the conductance. This and all prior gossip algorithms for the rumor spreading problem have been inherently randomized in their design and analysis. This resulted in a parallel research direction trying to reduce and determine the amount of randomness needed for efficient rumor spreading. This has been done via lower bounds for restricted models and by designing gossip algorithms with a reduced need for randomness. The general intuition and consensus of these results has been that randomization plays a important role in effectively spreading rumors. In this paper we improves over this state of the art in several ways by presenting a deterministic gossip algorithm that solves the the k-local broadcast problem in 2(k+log n)log n rounds. Besides being the first efficient deterministic solution to the rumor spreading problem this algorithm is interesting in many aspects: It is simpler, more natural, more robust and faster than its randomized pendant and guarantees success with certainty instead of with high probability. Its analysis is furthermore simple, self-contained and fundamentally different from prior works.},
  journal = {arXiv:1210.1193 [cs, math]},
  author = {Haeupler, Bernhard},
  month = jan,
  year = {2013},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Combinatorics,Computer Science - Distributed; Parallel; and Cluster Computing},
  pages = {705-716},
  file = {/Users/billlai/Zotero/storage/EMX636EU/Haeupler - 2013 - Simple, Fast and Deterministic Gossip and Rumor Sp.pdf;/Users/billlai/Zotero/storage/QKV9466L/1210.html}
}

@misc{12101193Simple,
  title = {[1210.1193] {{Simple}}, {{Fast}} and {{Deterministic Gossip}} and {{Rumor Spreading}}},
  howpublished = {https://arxiv.org/abs/1210.1193},
  file = {/Users/billlai/Zotero/storage/5AGQFJJ3/1210.html}
}

@article{HaeuplerSimpleFastDeterministic2013a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1210.1193},
  primaryClass = {cs, math},
  title = {Simple, {{Fast}} and {{Deterministic Gossip}} and {{Rumor Spreading}}},
  doi = {10.1137/1.9781611973105.51},
  abstract = {We study gossip algorithms for the rumor spreading problem which asks each node to deliver a rumor to all nodes in an unknown network. Gossip algorithms allow nodes only to call one neighbor per round and have recently attracted attention as message efficient, simple and robust solutions to the rumor spreading problem. Recently, non-uniform random gossip schemes were devised to allow efficient rumor spreading in networks with bottlenecks. In particular, [Censor-Hillel et al., STOC'12] gave an O(log\^3 n) algorithm to solve the 1-local broadcast problem in which each node wants to exchange rumors locally with its 1-neighborhood. By repeatedly applying this protocol one can solve the global rumor spreading quickly for all networks with small diameter, independently of the conductance. This and all prior gossip algorithms for the rumor spreading problem have been inherently randomized in their design and analysis. This resulted in a parallel research direction trying to reduce and determine the amount of randomness needed for efficient rumor spreading. This has been done via lower bounds for restricted models and by designing gossip algorithms with a reduced need for randomness. The general intuition and consensus of these results has been that randomization plays a important role in effectively spreading rumors. In this paper we improves over this state of the art in several ways by presenting a deterministic gossip algorithm that solves the the k-local broadcast problem in 2(k+log n)log n rounds. Besides being the first efficient deterministic solution to the rumor spreading problem this algorithm is interesting in many aspects: It is simpler, more natural, more robust and faster than its randomized pendant and guarantees success with certainty instead of with high probability. Its analysis is furthermore simple, self-contained and fundamentally different from prior works.},
  journal = {arXiv:1210.1193 [cs, math]},
  author = {Haeupler, Bernhard},
  month = jan,
  year = {2013},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Combinatorics,Computer Science - Distributed; Parallel; and Cluster Computing},
  pages = {705-716},
  file = {/Users/billlai/Zotero/storage/KSP7SXWM/Haeupler - 2013 - Simple, Fast and Deterministic Gossip and Rumor Sp.pdf}
}

@article{HaeuplerSimpleFastDeterministic2013b,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1210.1193},
  primaryClass = {cs, math},
  title = {Simple, {{Fast}} and {{Deterministic Gossip}} and {{Rumor Spreading}}},
  doi = {10.1137/1.9781611973105.51},
  abstract = {We study gossip algorithms for the rumor spreading problem, which asks each node to deliver a rumor to all nodes in an unknown network. Gossip algorithms allow nodes only to call one neighbor per round and have recently attracted attention as message efficient, simple, and robust solutions to the rumor spreading problem.},
  language = {en},
  journal = {arXiv:1210.1193 [cs, math]},
  author = {Haeupler, Bernhard},
  month = jan,
  year = {2013},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Combinatorics,Computer Science - Distributed; Parallel; and Cluster Computing},
  pages = {705-716},
  file = {/Users/billlai/Zotero/storage/KXMIUSN6/Haeupler - SIMPLE, FAST AND DETERMINISTIC GOSSIP AND RUMOR SP.pdf;/Users/billlai/Zotero/storage/W5TBQSJC/Haeupler - 2013 - Simple, Fast and Deterministic Gossip and Rumor Sp.pdf}
}

@article{HaeuplerSIMPLEFASTDETERMINISTIC,
  title = {{{SIMPLE}}, {{FAST AND DETERMINISTIC GOSSIP AND RUMOR SPREADING}}},
  language = {en},
  author = {Haeupler, Bernhard},
  pages = {79}
}

@article{ChierichettiRumorSpreadingConductance2018,
  title = {Rumor {{Spreading}} and {{Conductance}}},
  volume = {65},
  issn = {00045411},
  doi = {10.1145/3173043},
  language = {en},
  number = {4},
  journal = {Journal of the ACM},
  author = {Chierichetti, Flavio and Giakkoupis, George and Lattanzi, Silvio and Panconesi, Alessandro},
  month = apr,
  year = {2018},
  pages = {1-21},
  file = {/Users/billlai/Zotero/storage/VBB8IC24/Chierichetti et al. - 2018 - Rumor Spreading and Conductance.pdf}
}

@article{HaeuplerNewConstructiveAspects2011,
  title = {New {{Constructive Aspects}} of the {{Lov{\'a}sz Local Lemma}}},
  volume = {58},
  issn = {00045411},
  doi = {10.1145/2049697.2049702},
  language = {en},
  number = {6},
  journal = {Journal of the ACM},
  author = {Haeupler, Bernhard and Saha, Barna and Srinivasan, Aravind},
  month = dec,
  year = {2011},
  pages = {1-28},
  file = {/Users/billlai/Zotero/storage/C7CFEAKG/Haeupler et al. - 2011 - New Constructive Aspects of the Lovász Local Lemma.pdf}
}

@article{LindSpreadinggossipsocial2007,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0705.3224},
  title = {Spreading Gossip in Social Networks},
  volume = {76},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.76.036117},
  abstract = {We study a simple model of information propagation in social networks, where two quantities are introduced: the spread factor, which measures the average maximal fraction of neighbors of a given node that interchange information among each other, and the spreading time needed for the information to reach such fraction of nodes. When the information refers to a particular node at which both quantities are measured, the model can be taken as a model for gossip propagation. In this context, we apply the model to real empirical networks of social acquaintances and compare the underlying spreading dynamics with different types of scale-free and small-world networks. We find that the number of friendship connections strongly influences the probability of being gossiped. Finally, we discuss how the spread factor is able to be applied to other situations.},
  number = {3},
  journal = {Physical Review E},
  author = {Lind, Pedro G. and {da Silva}, Luciano R. and Andrade Jr., Jos{\'e} S. and Herrmann, Hans J.},
  month = sep,
  year = {2007},
  keywords = {Physics - Physics and Society},
  file = {/Users/billlai/Zotero/storage/3UBSNSAC/Lind et al. - 2007 - Spreading gossip in social networks.pdf;/Users/billlai/Zotero/storage/8CG5QJC5/0705.html}
}

@article{LindSpreadinggossipsocial2007a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0705.3224},
  title = {Spreading Gossip in Social Networks},
  volume = {76},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.76.036117},
  abstract = {We study a simple model of information propagation in social networks, where two quantities are introduced: the spread factor, which measures the average maximal fraction of neighbors of a given node that interchange information among each other, and the spreading time needed for the information to reach such fraction of nodes. When the information refers to a particular node at which both quantities are measured, the model can be taken as a model for gossip propagation. In this context, we apply the model to real empirical networks of social acquaintances and compare the underlying spreading dynamics with different types of scale-free and small-world networks. We find that the number of friendship connections strongly influences the probability of being gossiped. Finally, we discuss how the spread factor is able to be applied to other situations.},
  number = {3},
  journal = {Physical Review E},
  author = {Lind, Pedro G. and {da Silva}, Luciano R. and Andrade Jr., Jos{\'e} S. and Herrmann, Hans J.},
  month = sep,
  year = {2007},
  keywords = {Physics - Physics and Society},
  file = {/Users/billlai/Zotero/storage/BT6H6UD5/Lind et al. - 2007 - Spreading gossip in social networks.pdf;/Users/billlai/Zotero/storage/TAFAJ6CZ/0705.html}
}

@article{DunbarGossipevolutionaryperspective2004,
  title = {Gossip in Evolutionary Perspective},
  volume = {8},
  issn = {1939-1552(Electronic),1089-2680(Print)},
  doi = {10.1037/1089-2680.8.2.100},
  abstract = {Conversation is a uniquely human phenomenon. Analyses of freely forming conversations indicate that approximately two thirds of conversation time is devoted to social topics, most of which can be given the generic label gossip. This article first explores the origins of gossip as a mechanism for bonding social groups, tracing these origins back to social grooming among primates. It then asks why social gossip in this sense should form so important a component of human interaction and presents evidence to suggest that, aside from servicing social networks, a key function may be related explicitly to controlling free riders. Finally, the author reviews briefly the role of social cognition in facilitating conversations of this kind. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  number = {2},
  journal = {Review of General Psychology},
  author = {Dunbar, R. I. M.},
  year = {2004},
  keywords = {Gossip,Attachment Behavior,Conversation,Interpersonal Interaction,Social Groups,Social Networks},
  pages = {100-110},
  file = {/Users/billlai/Zotero/storage/7ZG2PCX6/2004-14314-004.html}
}

@article{Feinbergvirtuesgossipreputational2012,
  title = {The Virtues of Gossip: Reputational Information Sharing as Prosocial Behavior},
  volume = {102},
  issn = {1939-1315},
  shorttitle = {The Virtues of Gossip},
  doi = {10.1037/a0026650},
  abstract = {Reputation systems promote cooperation and deter antisocial behavior in groups. Little is known, however, about how and why people share reputational information. Here, we seek to establish the existence and dynamics of prosocial gossip, the sharing of negative evaluative information about a target in a way that protects others from antisocial or exploitative behavior. We present a model of prosocial gossip and the results of 4 studies testing the model's claims. Results of Studies 1 through 3 demonstrate that (a) individuals who observe an antisocial act experience negative affect and are compelled to share information about the antisocial actor with a potentially vulnerable person, (b) sharing such information reduces negative affect created by observing the antisocial behavior, and (c) individuals possessing more prosocial orientations are the most motivated to engage in such gossip, even at a personal cost, and exhibit the greatest reduction in negative affect as a result. Study 4 demonstrates that prosocial gossip can effectively deter selfishness and promote cooperation. Taken together these results highlight the roles of prosocial motivations and negative affective reactions to injustice in maintaining reputational information sharing in groups. We conclude by discussing implications for reputational theories of the maintenance of cooperation in human groups.},
  language = {eng},
  number = {5},
  journal = {Journal of Personality and Social Psychology},
  author = {Feinberg, Matthew and Willer, Robb and Stellar, Jennifer and Keltner, Dacher},
  month = may,
  year = {2012},
  keywords = {Communication,Cooperative Behavior,Emotions,Female,Group Processes,Humans,Information Dissemination,Logistic Models,Male,Motivation,Multivariate Analysis,Social Behavior,Social Control; Informal,Social Values,United States},
  pages = {1015-1030},
  pmid = {22229458}
}

@inproceedings{Censor-HillelGlobalComputationPoorly2012,
  address = {New York, NY, USA},
  series = {STOC '12},
  title = {Global {{Computation}} in a {{Poorly Connected World}}: {{Fast Rumor Spreading}} with {{No Dependence}} on {{Conductance}}},
  isbn = {978-1-4503-1245-5},
  shorttitle = {Global {{Computation}} in a {{Poorly Connected World}}},
  doi = {10.1145/2213977.2214064},
  abstract = {In this paper, we study the question of how efficiently a collection of interconnected nodes can perform a global computation in the GOSSIP model of communication. In this model, nodes do not know the global topology of the network, and they may only initiate contact with a single neighbor in each round. This model contrasts with the much less restrictive LOCAL model, where a node may simultaneously communicate with all of its neighbors in a single round. A basic question in this setting is how many rounds of communication are required for the information dissemination problem, in which each node has some piece of information and is required to collect all others. In the LOCAL model, this is quite simple: each node broadcasts all of its information in each round, and the number of rounds required will be equal to the diameter of the underlying communication graph. In the GOSSIP model, each node must independently choose a single neighbor to contact, and the lack of global information makes it difficult to make any sort of principled choice. As such, researchers have focused on the uniform gossip algorithm, in which each node independently selects a neighbor uniformly at random. When the graph is well-connected, this works quite well. In a string of beautiful papers, researchers proved a sequence of successively stronger bounds on the number of rounds required in terms of the conductance $\varphi$ and graph size n, culminating in a bound of O($\varphi$-1 log n). In this paper, we show that a fairly simple modification of the protocol gives an algorithm that solves the information dissemination problem in at most O(D + polylog (n)) rounds in a network of diameter D, with no dependence on the conductance. This is at most an additive polylogarithmic factor from the trivial lower bound of D, which applies even in the LOCAL model. In fact, we prove that something stronger is true: any algorithm that requires T rounds in the LOCAL model can be simulated in O(T + polylog(n)) rounds in the GOSSIP model. We thus prove that these two models of distributed computation are essentially equivalent.},
  booktitle = {Proceedings of the {{Forty}}-Fourth {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {{Censor-Hillel}, Keren and Haeupler, Bernhard and Kelner, Jonathan and Maymounkov, Petar},
  year = {2012},
  keywords = {gossip,broadcast,expander decomposition,information spreading,sparse spanners},
  pages = {961--970},
  file = {/Users/billlai/Zotero/storage/RMTHAUQJ/Censor-Hillel et al. - 2012 - Global Computation in a Poorly Connected World Fa.pdf}
}

@misc{GraphTheoryReinhard,
  title = {Graph {{Theory}} | {{Reinhard Diestel}} | {{Springer}}},
  howpublished = {https://www.springer.com/la/book/9783662536216}
}

@misc{GraphTheoryReinharda,
  title = {Graph {{Theory}} | {{Reinhard Diestel}} | {{Springer}}},
  howpublished = {https://www.springer.com/la/book/9783662536216}
}

@misc{GraphTheoryReinhardb,
  title = {Graph {{Theory}} - {{Reinhard Diestel}} - {{Google}} 图书},
  howpublished = {https://books.google.com/books/about/Graph\_Theory.html?id=aR2TMYQr2CMC},
  file = {/Users/billlai/Zotero/storage/W2V3MDQV/Graph_Theory.html}
}

@book{DiestelGraphTheory2017,
  address = {Berlin Heidelberg},
  edition = {5},
  series = {Graduate Texts in Mathematics},
  title = {Graph {{Theory}}},
  isbn = {978-3-662-53621-6},
  abstract = {This standard textbook of modern graph theory, now in its fifth edition, combines the authority of a classic with the engaging freshness of style that is the hallmark of active mathematics. It covers the core material of the subject with concise yet reliably complete proofs, while offering glimpses of more advanced methods in each field by one or two deeper results, again with proofs given in full detail. The book can be used as a reliable text for an introductory course, as a graduate text, and for self-study. From the reviews: ``This outstanding book cannot be substituted with any other book on the present textbook market. It has every chance of becoming the standard textbook for graph theory.'' Acta Scientiarum Mathematiciarum ``Deep, clear, wonderful. This is a serious book about the heart of graph theory. It has depth and integrity.'' Persi Diaconis \& Ron Graham, SIAM Review ``The book has received a very enthusiastic reception, which it amply deserves. A masterly elucidation of modern graph theory.'' Bulletin of the Institute of Combinatorics and its Applications ``Succeeds dramatically ... a hell of a good book.'' MAA Reviews ``A highlight of the book is what is by far the best account in print of the Seymour-Robertson theory of graph minors.'' Mathematika `` ... like listening to someone explain mathematics.'' Bulletin of the AMS},
  language = {en},
  publisher = {{Springer-Verlag}},
  author = {Diestel, Reinhard},
  year = {2017},
  file = {/Users/billlai/Zotero/storage/JMXCV2IG/9783662536216.html}
}

@article{JerrumApproximatingPermanent1989,
  title = {Approximating the {{Permanent}}},
  volume = {18},
  issn = {0097-5397},
  doi = {10.1137/0218077},
  abstract = {A randomised approximation scheme for the permanent of a 0\textendash{}1s presented. The task of estimating a permanent is reduced to that of almost uniformly generating perfect matchings in a graph; the latter is accomplished by simulating a Markov chain whose states are the matchings in the graph. For a wide class of 0\textendash{}1 matrices the approximation scheme is fully-polynomial, i.e., runs in time polynomial in the size of the matrix and a parameter that controls the accuracy of the output. This class includes all dense matrices (those that contain sufficiently many 1's) and almost all sparse matrices in some reasonable probabilistic model for 0\textendash{}1 matrices of given density.For the approach sketched above to be computationally efficient, the Markov chain must be rapidly mixing: informally, it must converge in a short time to its stationary distribution. A major portion of the paper is devoted to demonstrating that the matchings chain is rapidly mixing, apparently the first such result for a Markov chain with genuinely complex structure. The techniques used seem to have general applicability, and are applied again in the paper to validate a fully-polynomial randomised approximation scheme for the partition function of an arbitrary monomer-dimer system.},
  number = {6},
  journal = {SIAM Journal on Computing},
  author = {Jerrum, M. and Sinclair, A.},
  month = dec,
  year = {1989},
  pages = {1149-1178},
  file = {/Users/billlai/Zotero/storage/W9ZG3XV4/Jerrum and Sinclair - 1989 - Approximating the Permanent.pdf;/Users/billlai/Zotero/storage/XWI557KJ/0218077.html}
}

@inproceedings{DemersEpidemicAlgorithmsReplicated1987,
  address = {New York, NY, USA},
  series = {PODC '87},
  title = {Epidemic {{Algorithms}} for {{Replicated Database Maintenance}}},
  isbn = {978-0-89791-239-6},
  doi = {10.1145/41840.41841},
  booktitle = {Proceedings of the {{Sixth Annual ACM Symposium}} on {{Principles}} of {{Distributed Computing}}},
  publisher = {{ACM}},
  author = {Demers, Alan and Greene, Dan and Hauser, Carl and Irish, Wes and Larson, John and Shenker, Scott and Sturgis, Howard and Swinehart, Dan and Terry, Doug},
  year = {1987},
  pages = {1--12},
  file = {/Users/billlai/Zotero/storage/37SE8NIR/Demers et al. - 1987 - Epidemic Algorithms for Replicated Database Mainte.pdf}
}

@article{Erdosevolutionrandomgraphs1960,
  title = {On the Evolution of Random Graphs},
  volume = {5},
  journal = {Publ. Math. Inst. Hung. Acad. Sci},
  author = {Erd{\H o}s, Paul and R{\'e}nyi, Alfr{\'e}d},
  year = {1960},
  pages = {17--61},
  file = {/Users/billlai/Zotero/storage/H99JERGY/Erds and Rényi - 1960 - On the evolution of random graphs.pdf;/Users/billlai/Zotero/storage/XUUGP5KH/Erds and Rényi - 1960 - On the evolution of random graphs.pdf}
}

@article{JonassonLollipopgraphsare2002,
  title = {Lollipop Graphs Are Extremal for Commute Times},
  volume = {16},
  copyright = {Copyright \textcopyright{} 2000 John Wiley \& Sons, Inc.},
  issn = {1098-2418},
  doi = {10.1002/(SICI)1098-2418(200003)16:2<131::AID-RSA1>3.0.CO;2-3},
  language = {en},
  number = {2},
  journal = {Random Structures \& Algorithms},
  author = {Jonasson, Johan},
  year = {2002},
  pages = {131-142},
  file = {/Users/billlai/Zotero/storage/EFVUTGH5/(SICI)1098-2418(200003)162131AID-RSA13.0.html}
}

@article{AlonManyRandomWalks2007,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0705.0467},
  primaryClass = {math},
  title = {Many {{Random Walks Are Faster Than One}}},
  abstract = {We pose a new and intriguing question motivated by distributed computing regarding random walks on graphs: How long does it take for several independent random walks, starting from the same vertex, to cover an entire graph? We study the cover time - the expected time required to visit every node in a graph at least once - and we show that for a large collection of interesting graphs, running many random walks in parallel yields a speed-up in the cover time that is linear in the number of parallel walks. We demonstrate that an exponential speed-up is sometimes possible, but that some natural graphs allow only a logarithmic speed-up. A problem related to ours (in which the walks start from some probabilistic distribution on vertices) was previously studied in the context of space efficient algorithms for undirected s-t connectivity and our results yield, in certain cases, an improvement upon some of the earlier bounds.},
  journal = {arXiv:0705.0467 [math]},
  author = {Alon, Noga and Avin, Chen and Koucky, Michal and Kozma, Gady and Lotker, Zvi and Tuttle, Mark R.},
  month = may,
  year = {2007},
  keywords = {Mathematics - Probability},
  file = {/Users/billlai/Zotero/storage/VNS28NEA/Alon et al. - 2007 - Many Random Walks Are Faster Than One.pdf;/Users/billlai/Zotero/storage/FNIUEXBP/0705.html}
}


@article{Cheegerlowerboundsmallest1969,
  title = {{A lower bound for the smallest eigenvalue of the Laplacian}},
  language = {English (US)},
  journal = {Proceedings of the Princeton conference in honor of Professor S. Bochner},
  author = {Cheeger, Jeff},
  year = {1969},
  pages = {195-199},
  file = {/Users/billlai/Zotero/storage/D7TIMGMU/a-lower-bound-for-the-smallest-eigenvalue-of-the-laplacian.html}
}

@inproceedings{Censor-HillelFastInformationSpreading2011,
  address = {Philadelphia, PA, USA},
  series = {SODA '11},
  title = {Fast {{Information Spreading}} in {{Graphs}} with {{Large Weak Conductance}}},
  abstract = {Gathering data from nodes in a network is at the heart of many distributed applications, most notably, while performing a global task. We consider information spreading among n nodes of a network, where each node v has a message m(v) which must be received by all other nodes. The time required for information spreading has been previously upper-bounded with an inverse relationship to the conductance of the underlying communication graph. This implies high running times for graphs with small conductance. The main contribution of this paper is an information spreading algorithm which overcomes communication bottlenecks and thus achieves fast information spreading for a wide class of graphs, despite their small conductance. As a key tool in our study we use the recently defined concept of weak conductance, a generalization of classic graph conductance which measures how well-connected the components of a graph are. Our hybrid algorithm, which alternates between random and deterministic communication phases, exploits the connectivity within components by first applying partial information spreading, after which messages are sent across bottlenecks, thus spreading further throughout the network. This yields substantial improvements over the best known running times of algorithms for information spreading on any graph that has a large weak conductance, from polynomial to polylogarithmic number of rounds. We demonstrate the power of fast information spreading in accomplishing global tasks on the leader election problem, which lies at the core of distributed computing. Our results yield an algorithm for leader election that has a scalable running time on graphs with large weak conductance, improving significantly upon previous results.},
  booktitle = {Proceedings of the {{Twenty}}-Second {{Annual ACM}}-{{SIAM Symposium}} on {{Discrete Algorithms}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {{Censor-Hillel}, Keren and Shachnai, Hadas},
  year = {2011},
  keywords = {distributed computing,information spreading,leader election,randomized algorithms,weak conductance},
  pages = {440--448},
  file = {/Users/billlai/Zotero/storage/R8TJ8VC2/Censor-Hillel and Shachnai - 2011 - Fast Information Spreading in Graphs with Large We.pdf}
}
