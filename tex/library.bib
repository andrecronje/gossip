
@inproceedings{ImanishiUpperBoundResolution2017,
  series = {Lecture Notes in Computer Science},
  title = {An {{Upper Bound}} for {{Resolution Size}}: {{Characterization}} of {{Tractable SAT Instances}}},
  isbn = {978-3-319-53924-9 978-3-319-53925-6},
  shorttitle = {An {{Upper Bound}} for {{Resolution Size}}},
  doi = {10.1007/978-3-319-53925-6_28},
  abstract = {We show the first upper bound for resolution size of a SAT instance by pathwidth of its incidence graph. Namely, we prove that if an incidence graph of an unsatisfiable CNF formula has pathwidth pwpw\{$\backslash$mathrm \{pw\}\}, the formula can be refuted by a resolution proof with at most O${_\ast}$(3pw)O${_\ast}$(3pw)O\^*(3\^\{$\backslash$mathrm \{pw\}\}) clauses. It is known that modern practical SAT-solvers run efficiently for instances which have small and narrow resolution refutations. Resolution size is one of the parameters which make SAT tractable, whereas it is shown that even linearly approximating the resolution size is NP-hard. In contrast, computing graph based parameters such as treewidth or pathwidth is fixed-parameter tractable, and also efficient FPT algorithms for SAT of bounded such parameters are widely researched. However, few explicit connection between these parameters and resolutions or SAT-solvers are known. In this paper, we provide an FPT algorithm for SAT on path decomposition of its incidence graph. The algorithm can construct resolution refutations for unsatisfiable formulas, and analyzing the size of constructed proof gives the new bound.},
  language = {en},
  booktitle = {{{WALCOM}}: {{Algorithms}} and {{Computation}}},
  publisher = {{Springer, Cham}},
  author = {Imanishi, Kensuke},
  month = mar,
  year = {2017},
  pages = {359-369},
  file = {/Users/billlai/Folder/Papers/An Upper Bound for Resolution Size Characterization of Tractable SAT Instances.pdf;/Users/billlai/Zotero/storage/J3YU3WVP/Imanishi - 2017 - An Upper Bound for Resolution Size Characterizati.pdf;/Users/billlai/Zotero/storage/3V4T2VT2/978-3-319-53925-6_28.html}
}

@inproceedings{BeameUnderstandingPowerClause2003,
  address = {San Francisco, CA, USA},
  series = {IJCAI'03},
  title = {Understanding the {{Power}} of {{Clause Learning}}},
  booktitle = {Proceedings of the 18th {{International Joint Conference}} on {{Artificial Intelligence}}},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  author = {Beame, Paul and Kautz, Henry and Sabharwal, Ashish},
  year = {2003},
  pages = {1194--1201},
  file = {/Users/billlai/Folder/Papers/Understanding the power of clause learning.pdf;/Users/billlai/Folder/Papers/Understanding the power of clause learning.ppt}
}

@article{Ben-SassonShortProofsAre2001,
  title = {Short {{Proofs Are Narrow}}\textemdash{{Resolution Made Simple}}},
  volume = {48},
  issn = {0004-5411},
  doi = {10.1145/375827.375835},
  abstract = {The widthof a Resolution proof is defined to be the maximal number of literals in any clause of the proof. In this paper, we relate proof width to proof length (=size), in both general Resolution, and its tree-like variant. The following consequences of these relations reveal width as a crucial ``resource'' of Resolution proofs.
In one direction, the relations allow us to give simple, unified proofs for almost all known exponential lower bounds on size of resolution proofs, as well as several interesting new ones. They all follow from width lower bounds, and we show how these follow from natural expansion property of clauses of the input tautology.
In the other direction, the width-size relations naturally suggest a  simple dynamic programming procedure for automated theorem proving\textemdash{}one which simply searches for small width proofs. This relation guarantees that the runnuing time (and thus the size of the produced proof) is at most quasi-polynomial in the smallest tree-like proof. This algorithm is never much worse than any of the recursive automated provers (such as DLL) used in practice. In contrast, we present a family of tautologies on which it is exponentially faster.},
  number = {2},
  journal = {J. ACM},
  author = {{Ben-Sasson}, Eli and Wigderson, Avi},
  month = mar,
  year = {2001},
  pages = {149--169}
}

@article{Ben-Sasson*OptimalSeparationTreeLike2004,
  title = {Near {{Optimal Separation Of Tree}}-{{Like And General Resolution}}},
  volume = {24},
  issn = {0209-9683, 1439-6912},
  doi = {10.1007/s00493-004-0036-5},
  abstract = {We present the best known separation between tree-like and general resolution, improving on the recent exp(n$\in$) separation of [2]. This is done by constructing a natural family of contradictions, of size n, that have O(n)-size resolution refutations, but only exp($\Omega$(n/log n))- size tree-like refutations. This result implies that the most commonly used automated theorem procedures, which produce tree-like resolution refutations, will perform badly on some inputs, while other simple procedures, that produce general resolution refutations, will have polynomial run-time on these very same inputs. We show, furthermore that the gap we present is nearly optimal. Specifically, if S (ST) is the minimal size of a (tree-like) refutation, we prove that ST = exp(O(S log log S/log S)).},
  language = {en},
  number = {4},
  journal = {Combinatorica},
  author = {{Ben-Sasson*}, Eli and Impagliazzo\textdagger, Russell and Wigderson\textdaggerdbl, Avi},
  month = sep,
  year = {2004},
  pages = {585-603},
  file = {/Users/billlai/Folder/Papers/NEAR OPTIMAL SEPARATION OF TREE-LIKE AND GENERAL RESOLUTION.pdf;/Users/billlai/Zotero/storage/FJJJKEHM/Ben-Sasson 等。 - 2004 - Near Optimal Separation Of Tree-Like And General R.pdf}
}

@article{GoerdtRegularResolutionUnrestricted1993,
  title = {Regular {{Resolution Versus Unrestricted Resolution}}},
  volume = {22},
  issn = {0097-5397},
  doi = {10.1137/0222044},
  number = {4},
  journal = {SIAM J. Comput.},
  author = {Goerdt, Andreas},
  month = aug,
  year = {1993},
  keywords = {propositional logic,regular resolution,resolution theorem proving},
  pages = {661--683}
}

@inproceedings{PipatsrisawatPowerClauseLearningSAT2009,
  series = {Lecture Notes in Computer Science},
  title = {On the {{Power}} of {{Clause}}-{{Learning SAT Solvers}} with {{Restarts}}},
  isbn = {978-3-642-04243-0 978-3-642-04244-7},
  doi = {10.1007/978-3-642-04244-7_51},
  abstract = {In this work, we improve on existing work that studied the relationship between the proof system of modern SAT solvers and general resolution. Previous contributions such as those by Beame et al (2004), Hertel et al (2008), and Buss et al (2008) demonstrated that variations on modern clause-learning SAT solvers were as powerful as general resolution. However, the models used in these studies required either extra degrees of non-determinism or a preprocessing step that are not utilized by any state-of-the-art SAT solvers in practice. In this paper, we prove that modern SAT solvers that learn asserting clauses indeed p-simulate general resolution without the need for any additional techniques.},
  language = {en},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}} - {{CP}} 2009},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Pipatsrisawat, Knot and Darwiche, Adnan},
  month = sep,
  year = {2009},
  pages = {654-668},
  file = {/Users/billlai/Folder/Papers/On the Power of Clause-Learning SAT Solvers with Restarts.pdf;/Users/billlai/Zotero/storage/IUQ84BJI/10.html}
}

@inproceedings{AudemardRestrictionExtendedResolution2010,
  address = {Atlanta, Georgia},
  series = {AAAI'10},
  title = {A {{Restriction}} of {{Extended Resolution}} for {{Clause Learning Sat Solvers}}},
  abstract = {Modern complete SAT solvers almost uniformly implement variations of the clause learning framework introduced by Grasp and Chaff. The success of these solvers has been theoretically explained by showing that the clause learning framework is an implementation of a proof system which is as poweful as resolution. However, exponential lower bounds are known for resolution, which suggests that significant advances in SAT solving must come from implementations of more powerful proof systems. We present a clause learning SAT solver that uses extended resolution. It is based on a restriction of the application of the extension rule. This solver outperforms existing solvers on application instances from recent SAT competitions as well as on instances that are provably hard for resolution, such as XOR-SAT instances.},
  booktitle = {Proceedings of the {{Twenty}}-{{Fourth AAAI Conference}} on {{Artificial Intelligence}}},
  publisher = {{AAAI Press}},
  author = {Audemard, Gilles and Katsirelos, George and Simon, Laurent},
  year = {2010},
  pages = {15--20},
  file = {/Users/billlai/Folder/Papers/A Restriction of Extended Resolution for Clause Learning SAT Solvers.pdf}
}

@misc{KolokolovaComplexityExpanderBasedReasoning,
  title = {Complexity of {{Expander}}-{{Based Reasoning}} and the {{Power}} of {{Monotone Proofs}} | {{Simons Institute}} for the {{Theory}} of {{Computing}}},
  howpublished = {https://simons.berkeley.edu/talks/antonina-kolokolova-12-15-2016},
  author = {Kolokolova, Antonina},
  keywords = {RTR},
  file = {/Users/billlai/Folder/Papers/Power of reasoning over richer domains - Complexity of Expander-Based Reasoning and the Power of Monotone Proofs.pdf;/Users/billlai/Zotero/storage/A8EQPKTA/antonina-kolokolova-12-15-2016.html}
}

@misc{BussTalkProofComplexity,
  title = {Talk: {{Proof Complexity Mini}}-Tutorial},
  howpublished = {https://www.math.ucsd.edu/\textasciitilde{}sbuss/ResearchWeb/Banff2014/},
  author = {Buss, Sam},
  keywords = {RTR},
  file = {/Users/billlai/Folder/Papers/Mini-tutorial on proof complexity.pdf;/Users/billlai/Zotero/storage/9T2H9WV3/Banff2014.html}
}

@book{CookLogicalfoundationsproof,
  title = {Logical Foundations of Proof Complexity},
  abstract = {This book treats bounded arithmetic and propositional proof complexity from the point of view of computational complexity. The first seven chapters include the necessary logical background for the material and are suitable for a graduate course. Associated with each of many complexity classes are both a two-sorted predicate calculus theory, with induction restricted to concepts in the class, and a propositional proof system. The complexity classes range from AC0 for the weakest theory up to the polynomial hierarchy. Each bounded theorem in a theory translates into a family of (quantified) propositional tautologies with polynomial size proofs in the corresponding proof system. The theory proves the soundness of the associated proof system. The result is a uniform treatment of many systems in the literature, including Buss's theories for the polynomial hierarchy and many disparate systems for complexity classes such as AC0, AC0(m), TC0, NC1, L, NL, NC, and P.
Read more at http://www.cambridge.org/cn/academic/subjects/mathematics/logic-categories-and-sets/logical-foundations-proof-complexity\#pS8kF092DKSSYAK8.99},
  author = {Cook, Stephen and Phuong, Nguyen},
  file = {/Users/billlai/Folder/Books/SAT/Logical Foundations of Proof Complexity.pdf;/Users/billlai/Zotero/storage/GGFRVVBL/logical-foundations-proof-complexity.html}
}

@misc{BussPropositionalProofsFrege2015,
  title = {Propositional {{Proofs}} in {{Frege}} and {{Extended Frege Systems}} ({{Abstract}})},
  abstract = {We discuss recent results on the propositional proof complexity of Frege proof systems, including some recently discovered quasipolynomial size proofs for the pigeonhole principle and the Kneser-Lov$\backslash$'asz theorem. These are closely related to formalizability in bounded arithmetic.},
  howpublished = {http://link.springer.com/10.1007/978-3-319-20297-6\_1},
  journal = {Computer Science -- Theory and Applications},
  author = {Buss, Sam},
  collaborator = {Beklemishev, Lev D. and Musatov, Daniil V.},
  year = {2015},
  file = {/Users/billlai/Folder/Papers/Propositional Proofs in Frege and Extended Frege Systems (Abstract).pdf},
  doi = {10.1007/978-3-319-20297-6_1}
}

@misc{BussProofSystemsSAT,
  title = {Proof {{Systems SAT Solvers}}},
  howpublished = {https://www.math.ucsd.edu/\textasciitilde{}sbuss/ResearchWeb/Dagstuhl\_2015/},
  author = {Buss, Sam},
  file = {/Users/billlai/Folder/Papers/Proof Complexity & Complexity of SAT Solvers.pdf;/Users/billlai/Zotero/storage/7QKVN2KX/Dagstuhl_2015.html}
}

@misc{BeyersdorffProofComplexityOlaf2013,
  title = {Proof {{Complexity}} by {{Olaf Beyersdorff}}},
  abstract = {Tour of proof systems and the relation to other aspects},
  author = {Beyersdorff, Olaf},
  month = jun,
  year = {2013},
  file = {/Users/billlai/Folder/Papers/Proof Complexity - Beyersdorff.pdf}
}

@article{RazborovResolutionLowerBounds2003,
  title = {Resolution {{Lower Bounds}} for the {{Weak Functional Pigeonhole Principle}}},
  volume = {303},
  issn = {0304-3975},
  doi = {10.1016/S0304-3975(02)00453-X},
  abstract = {We show that every resolution proof of the functional version FPHPnm of the pigeonhole principle (in which one pigeon may not split between several holes) must have size exp($\Omega$(n/(log m)2)). This implies an exp($\Omega$(n1/3)) bound when the number of pigeons m is arbitrary.},
  number = {1},
  journal = {Theor. Comput. Sci.},
  author = {Razborov, Alexander A.},
  month = jun,
  year = {2003},
  pages = {233--243},
  file = {/Users/billlai/Folder/Papers/Resolution lower bounds for the weak functional pigeonhole principle.pdf}
}

@article{KrajicekPropositionalproofsystems1989,
  title = {Propositional Proof Systems, the Consistency of First Order Theories and the Complexity of Computations},
  volume = {54},
  issn = {0022-4812, 1943-5886},
  doi = {10.2307/2274765},
  abstract = {AbstractWe consider the problem about the length of proofs of the sentences  saying that there is no proof of contradiction in S whose length is $<$ n. We show the relation of this problem to some problems about propositional proof systems.},
  number = {3},
  journal = {The Journal of Symbolic Logic},
  author = {Kraj\'i{\v c}ek, Jan and Pudl\'ak, Pavel},
  month = sep,
  year = {1989},
  pages = {1063-1079},
  file = {/Users/billlai/Folder/Papers/Propositional proof systems, the consistency of first order theories and the complexity of computations.pdf;/Users/billlai/Zotero/storage/5XIRFEU5/2E0129971839631F3AE97FFCC21395E7.html}
}

@article{UrquhartComplexityPropositionalProofs1995,
  title = {The {{Complexity}} of {{Propositional Proofs}}},
  volume = {1},
  issn = {1079-8986},
  doi = {10.2307/421131},
  number = {4},
  journal = {The Bulletin of Symbolic Logic},
  author = {Urquhart, Alasdair},
  year = {1995},
  pages = {425-467},
  file = {/Users/billlai/Folder/Papers/The complexity of propositional proofs.pdf}
}

@article{AjtaicomplexityPigeonholePrinciple1994,
  title = {The Complexity of the {{Pigeonhole Principle}}},
  volume = {14},
  issn = {0209-9683, 1439-6912},
  doi = {10.1007/BF01302964},
  abstract = {The Pigeonhole Principle forn is the statement that there is no one-to-one function between a set of sizen and a set of sizen-1. This statement can be formulated as an unlimited fan-in constant depth polynomial size Boolean formulaPHPn inn(n-1) variables. We may think that the truth-value of the variablexi,j will be true iff the function maps thei-th element of the first set to thej-th element of the second (see Cook and Rechkow [5]).PHPn can be proved in the propositional calculus. That is, a sequence of Boolean formulae can be given so that each one is either an axiom of the propositional calculus or a consequence of some of the previous ones according to an inference rule of the propositional calculus, and the last one isPHPn. Our main result is that the Pigeonhole Principle cannot be proved this way, if the size of the proof (the total number or symbols of the formulae in the sequence) is polynomial inn and each formula is constant depth (unlimited fan-in), polynomial size and contains only the variables ofPHPn.},
  language = {en},
  number = {4},
  journal = {Combinatorica},
  author = {Ajtai, M.},
  month = dec,
  year = {1994},
  pages = {417-433},
  file = {/Users/billlai/Folder/Papers/The complexity of the pigeonhole principle.pdf;/Users/billlai/Zotero/storage/2WKU4DKR/Ajtai - 1994 - The complexity of the Pigeonhole Principle.pdf;/Users/billlai/Zotero/storage/86SXXKF9/BF01302964.html}
}

@article{KrajicekExponentialLowerBound1995,
  title = {An {{Exponential Lower Bound}} to the {{Size}} of {{Bounded Depth Frege Proofs}} of the {{Pigeonhole Principle}}},
  volume = {7},
  issn = {1042-9832},
  doi = {10.1002/rsa.3240070103},
  number = {1},
  journal = {Random Struct. Algorithms},
  author = {Kraj\'i{\v c}ek, Jan and Pudl\'ak, Pavel and Woods, Alan},
  month = aug,
  year = {1995},
  pages = {15--39},
  file = {/Users/billlai/Folder/Papers/An exponential lower bound to the size of bounded depth Frege proofs of the pigeonhole principle.pdf}
}

@article{PitassiExponentialLowerBounds1993,
  title = {Exponential {{Lower Bounds}} for the {{Pigeonhole Principle}}},
  volume = {3},
  issn = {1016-3328},
  doi = {10.1007/BF01200117},
  number = {2},
  journal = {Comput. Complex.},
  author = {Pitassi, Toniann and Beame, Paul and Impagliazzo, Russell},
  month = apr,
  year = {1993},
  keywords = {complexity of propositional proof systems,lower bounds},
  pages = {97--140},
  file = {/Users/billlai/Folder/Papers/Exponential lower bounds for the pigeonhole principle.pdf}
}

@book{KrajicekLowerBoundsSize1994,
  title = {Lower {{Bounds}} to the {{Size}} of {{Constant}}-{{Depth Propositional Proofs}}},
  abstract = {1  LK is a natural modification of Gentzen sequent calculus for propositional logic with connectives : and  V  ;  W  (both of unbounded arity). Then for every d  0 and n  2, there is a set T  d n of depth d sequents of total size O(n  3+d  ) which are refutable in LK by depth d + 1 proof of size exp(O(log  2  n)) but such that every depth d refutation must have the size at least exp(n$\backslash$Omega$\backslash$Gamma21 ). The sets T  d n express a weaker form of the pigeonhole principle. It is a fundamental problem of mathematical logic and complexity theory whether there exists a proof system for propositional logic in which every tautology has a short proof, where the length (equivalently the size) of a proof is measured essentially by the total number of symbols in it and short means polynomial in the length of the tautology. Equivalently one can ask whether for every theory T there is another theory S (both first order and reasonably axiomatized, e.g. by schemes) having the property that if a statement...},
  author = {Kraj\'icek, Jan},
  year = {1994},
  file = {/Users/billlai/Folder/Papers/Lower bounds to the size of constant-depth propositional proofs.pdf;/Users/billlai/Zotero/storage/E5KRY5TZ/Krajícek - 1994 - Lower Bounds to the Size of Constant-Depth Proposi.pdf;/Users/billlai/Zotero/storage/39863NRN/summary.html}
}

@book{KrajicekBoundedArithmeticPropositional1995,
  address = {New York, NY, USA},
  title = {Bounded {{Arithmetic}}, {{Propositional Logic}}, and {{Complexity Theory}}},
  isbn = {978-0-521-45205-2},
  publisher = {{Cambridge University Press}},
  author = {Kraj\'i{\v c}ek, Jan},
  year = {1995},
  file = {/Users/billlai/Folder/Books/SAT/Bounded Arithmetic, Propositional Logic and Complexity Theory.pdf}
}

@article{KelvinNineteenthcenturyclouds1901,
  title = {I. {{Nineteenth}} Century Clouds over the Dynamical Theory of Heat and Light},
  volume = {2},
  doi = {10.1080/14786440109462664},
  journal = {Philosophical Magazine Series 6},
  author = {Kelvin, Lord},
  month = jul,
  year = {1901},
  pages = {1-40}
}

@article{EibenSmallResolutionProofs2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.02120},
  primaryClass = {cs},
  title = {Small {{Resolution Proofs}} for {{QBF}} Using {{Dependency Treewidth}}},
  abstract = {In spite of the close connection between the evaluation of quantified Boolean formulas (QBF) and propositional satisfiability (SAT), tools and techniques which exploit structural properties of SAT instances are known to fail for QBF. This is especially true for the structural parameter treewidth, which has allowed the design of successful algorithms for SAT but cannot be straightforwardly applied to QBF since it does not take into account the interdependencies between quantified variables. In this work we introduce and develop dependency treewidth, a new structural parameter based on treewidth which allows the efficient solution of QBF instances. Dependency treewidth pushes the frontiers of tractability for QBF by overcoming the limitations of previously introduced variants of treewidth for QBF. We augment our results by developing algorithms for computing the decompositions that are required to use the parameter.},
  journal = {arXiv:1711.02120 [cs]},
  author = {Eiben, Eduard and Ganian, Robert and Ordyniak, Sebastian},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Logic in Computer Science},
  file = {/Users/billlai/Folder/Papers/Small Resolution Proofs for QBF using Dependency Treewidth.pdf;/Users/billlai/Zotero/storage/UY5I3U59/Eiben et al. - 2017 - Small Resolution Proofs for QBF using Dependency T.pdf;/Users/billlai/Zotero/storage/TI8SYK3P/1711.html}
}

@article{Cookrelativeefficiencypropositional1979,
  title = {The Relative Efficiency of Propositional Proof Systems},
  volume = {44},
  issn = {0022-4812, 1943-5886},
  doi = {10.2307/2273702},
  abstract = {We are interested in studying the length of the shortest proof of a propositional tautology in various proof systems as a function of the length of the tautology. The smallest upper bound known for this function is exponential, no matter what the proof system. A question we would like to answer (but have not been able to) is whether this function has a polynomial bound for some proof system. (This question is motivated below.) Our results here are relative results.In \textsection\textsection{}2 and 3 we indicate that all standard Hilbert type systems (or Frege systems, as we call them) and natural deduction systems are equivalent, up to application of a polynomial, as far as minimum proof length goes. In \textsection{}4 we introduce extended Frege systems, which allow introduction of abbreviations for formulas. Since these abbreviations can be iterated, they eliminate the need for a possible exponential growth in formula length in a proof, as is illustrated by an example (the pigeonhole principle). In fact, Theorem 4.6 (which is a variation of a theorem of Statman) states that with a penalty of at most a linear increase in the number of lines of a proof in an extended Frege system, no line in the proof need be more than a constant times the length of the formula proved.},
  number = {1},
  journal = {The Journal of Symbolic Logic},
  author = {Cook, Stephen A. and Reckhow, Robert A.},
  month = mar,
  year = {1979},
  pages = {36-50},
  file = {/Users/billlai/Folder/Papers/The Relative Efficiency of Propositional Proof Systems.pdf;/Users/billlai/Zotero/storage/JCMRNR2R/218048250981F835B4B2A4080205A0BA.html}
}

@techreport{OliveiraAverageCaseLowerBound2017,
  title = {An {{Average}}-{{Case Lower Bound}} against {{ACC}}\^0},
  number = {173},
  author = {Oliveira, Igor Carboni and Chen, Ruiwen and Santhanam, Rahul},
  year = {2017},
  keywords = {circuit lower bounds,hardness amplification,non-trivial learning,satisfiability algorithms},
  file = {/Users/billlai/Folder/Papers/An Average-Case Lower Bound against ACC^0.pdf;/Users/billlai/Zotero/storage/QR2RH35B/Oliveira et al. - 2017 - An Average-Case Lower Bound against ACC^0.pdf;/Users/billlai/Zotero/storage/ZRX2DDRG/173.html}
}

@article{WilliamsNonuniformACCCircuit2014,
  title = {Nonuniform {{ACC Circuit Lower Bounds}}},
  volume = {61},
  issn = {0004-5411},
  doi = {10.1145/2559903},
  abstract = {The class ACC consists of circuit families with constant depth over unbounded fan-in AND, OR, NOT, and MODm gates, where m $>$ 1 is an arbitrary constant. We prove the following. ---NEXP, the class of languages accepted in nondeterministic exponential time, does not have nonuniform ACC circuits of polynomial size. The size lower bound can be slightly strengthened to quasipolynomials and other less natural functions. ---ENP, the class of languages recognized in 2O(n) time with an NP oracle, doesn't have nonuniform ACC circuits of 2no(1) size. The lower bound gives an exponential size-depth tradeoff: for every d, m there is a $\delta$ $>$ 0 such that ENP doesn't have depth-d ACC circuits of size 2n$\delta$ with MODm gates. Previously, it was not known whether EXPNP had depth-3 polynomial-size circuits made out of only MOD6 gates. The high-level strategy is to design faster algorithms for the circuit satisfiability problem over ACC circuits, then prove that such algorithms entail these lower bounds. The algorithms combine known properties of ACC with fast rectangular matrix multiplication and dynamic programming, while the second step requires a strengthening of the author's prior work.},
  number = {1},
  journal = {J. ACM},
  author = {Williams, Ryan},
  month = jan,
  year = {2014},
  keywords = {lower bounds,ACC,Circuit complexity,NEXP,satisfiability},
  pages = {2:1--2:32},
  file = {/Users/billlai/Folder/Papers/Non-Uniform ACC Circuit Lower Bounds.pdf}
}

@article{BussQuasipolynomialSizeProofs2015,
  title = {Quasipolynomial {{Size Proofs}} of the {{Propositional Pigeonhole Principle}}},
  volume = {576},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2015.02.005},
  abstract = {Cook and Reckhow proved in 1979 that the propositional pigeonhole principle has polynomial size extended Frege proofs. Buss proved in 1987 that it also has polynomial size Frege proofs; these Frege proofs used a completely different proof method based on counting. This paper shows that the original Cook and Reckhow extended Frege proofs can be formulated as quasipolynomial size Frege proofs. The key point is that st-connectivity can be used to define the Cook-Reckhow construction.},
  number = {C},
  journal = {Theor. Comput. Sci.},
  author = {Buss, Sam},
  month = apr,
  year = {2015},
  keywords = {Extended Frege proofs,Frege proofs,Pigeonhole principle,Proof complexity,Propositional proofs},
  pages = {77--84},
  file = {/Users/billlai/Folder/Papers/Quasipolynomial size proofs of the propositional pigeonhole principle.pdf}
}

@book{DeMarcoControllingSoftwareProjects1986,
  address = {Upper Saddle River, NJ, USA},
  title = {Controlling {{Software Projects}}: {{Management}}, {{Measurement}}, and {{Estimates}}},
  isbn = {978-0-13-171711-4},
  shorttitle = {Controlling {{Software Projects}}},
  publisher = {{Prentice Hall PTR}},
  author = {DeMarco, T.},
  year = {1986}
}

@article{FerrerEstimatingSoftwareTesting2013,
  title = {Estimating {{Software Testing Complexity}}},
  volume = {55},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2013.07.007},
  number = {12},
  journal = {Inf. Softw. Technol.},
  author = {Ferrer, Javier and Chicano, Francisco and Alba, Enrique},
  month = dec,
  year = {2013},
  keywords = {Branch coverage,Complexity,Evolutionary algorithms,Evolutionary testing,Search based software engineering,Testability},
  pages = {2125--2139}
}

@inproceedings{RabinSpeedcomputationclassification1959,
  title = {Speed of Computation and Classification of Recursive Sets},
  booktitle = {Third {{Convention Sci}}. {{Soc}}., {{Israel}}},
  author = {Rabin, Michael Oser},
  year = {1959},
  pages = {1--2}
}

@article{RabinDegreedifficultycomputing1960,
  title = {Degree of Difficulty of Computing a Function and a Partial Ordering of Recursive Sets},
  journal = {Technical Report 2},
  author = {Rabin, Michael Oser},
  year = {1960}
}

@article{Blummachineindependenttheorycomplexity1967,
  title = {A Machine-Independent Theory of the Complexity of Recursive Functions},
  volume = {14},
  number = {2},
  journal = {Journal of the ACM (JACM)},
  author = {Blum, Manuel},
  year = {1967},
  pages = {322--336}
}

@article{Hartmaniscomputationalcomplexityalgorithms1965,
  title = {On the Computational Complexity of Algorithms},
  volume = {117},
  journal = {Transactions of the American Mathematical Society},
  author = {Hartmanis, Juris and Stearns, Richard E},
  year = {1965},
  pages = {285--306}
}

@article{Cobhamintrinsiccomputationaldifficulty1965,
  title = {The Intrinsic Computational Difficulty of Functions},
  author = {Cobham, Alan},
  year = {1965}
}

@book{CormenIntroductionalgorithms2009,
  title = {Introduction to Algorithms},
  publisher = {{MIT press}},
  author = {Cormen, Thomas H},
  year = {2009}
}

@article{Shannonsynthesistwoterminalswitching1949,
  title = {The Synthesis of Two-Terminal Switching Circuits},
  volume = {28},
  number = {1},
  journal = {Bell Labs Technical Journal},
  author = {Shannon, Claude and {others}},
  year = {1949},
  pages = {59--98}
}

@book{Savagecomplexitycomputing1976,
  title = {The Complexity of Computing},
  publisher = {{Wiley New York}},
  author = {Savage, John E},
  year = {1976}
}

@book{AroraComputationalcomplexitymodern2009,
  title = {Computational Complexity: A Modern Approach},
  publisher = {{Cambridge University Press}},
  author = {Arora, Sanjeev and Barak, Boaz},
  year = {2009}
}

@book{SipserIntroductionTheoryComputation2006,
  title = {Introduction to the {{Theory}} of {{Computation}}},
  volume = {2},
  publisher = {{Thomson Course Technology Boston}},
  author = {Sipser, Michael},
  year = {2006}
}

@article{VonNeumanncertainzerosumtwoperson1953,
  title = {A Certain Zero-Sum Two-Person Game Equivalent to the Optimal Assignment Problem},
  volume = {2},
  journal = {Contributions to the Theory of Games},
  author = {Von Neumann, John},
  year = {1953},
  pages = {5--12}
}

@article{EdmondsPathstreesflowers1965,
  title = {Paths, Trees, and Flowers},
  volume = {17},
  number = {3},
  journal = {Canadian Journal of mathematics},
  author = {Edmonds, Jack},
  year = {1965},
  pages = {449--467}
}

@incollection{KarpReducibilitycombinatorialproblems1972,
  title = {Reducibility among Combinatorial Problems},
  booktitle = {Complexity of Computer Computations},
  publisher = {{Springer}},
  author = {Karp, Richard M},
  year = {1972},
  pages = {85--103}
}

@article{KhachiyanPolynomialalgorithmslinear1980,
  title = {Polynomial Algorithms in Linear Programming},
  volume = {20},
  number = {1},
  journal = {USSR Computational Mathematics and Mathematical Physics},
  author = {Khachiyan, Leonid G},
  year = {1980},
  pages = {53--72}
}

@article{StrassenGaussianeliminationnot1969,
  title = {Gaussian Elimination Is Not Optimal},
  volume = {13},
  number = {4},
  journal = {Numerische mathematik},
  author = {Strassen, Volker},
  year = {1969},
  pages = {354--356}
}

@inproceedings{CookComplexityTheoremprovingProcedures1971,
  address = {New York, NY, USA},
  series = {STOC '71},
  title = {The {{Complexity}} of {{Theorem}}-Proving {{Procedures}}},
  doi = {10.1145/800157.805047},
  abstract = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be ``reduced'' to the problem of determining whether a given propositional formula is a tautology. Here ``reduced'' means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
  booktitle = {Proceedings of the {{Third Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Cook, Stephen A.},
  year = {1971},
  pages = {151--158},
  file = {/Users/billlai/Zotero/storage/GQUET2F8/Cook - 1971 - The Complexity of Theorem-proving Procedures.pdf}
}

@article{AgrawalPRIMES2004,
  title = {{{PRIMES}} Is in {{P}}},
  journal = {Annals of mathematics},
  author = {Agrawal, Manindra and Kayal, Neeraj and Saxena, Nitin},
  year = {2004},
  pages = {781--793}
}

@article{AroraProofverificationhardness1998,
  title = {Proof Verification and the Hardness of Approximation Problems},
  volume = {45},
  number = {3},
  journal = {Journal of the ACM (JACM)},
  author = {Arora, Sanjeev and Lund, Carsten and Motwani, Rajeev and Sudan, Madhu and Szegedy, Mario},
  year = {1998},
  pages = {501--555}
}

@inproceedings{BabaiGraphisomorphismquasipolynomial2016,
  title = {Graph Isomorphism in Quasipolynomial Time},
  booktitle = {Proceedings of the 48th {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Babai, L\'aszl\'o},
  year = {2016},
  pages = {684--697}
}

@article{Planningeconomicimpactsinadequate2002,
  title = {The Economic Impacts of Inadequate Infrastructure for Software Testing},
  author = {Planning, Strategic},
  year = {2002}
}

@book{Myersartsoftwaretesting2011,
  title = {The Art of Software Testing},
  publisher = {{John Wiley \& Sons}},
  author = {Myers, Glenford J and Sandler, Corey and Badgett, Tom},
  year = {2011}
}

@article{YuExperiencepredictingfaultprone2012,
  title = {Experience in Predicting Fault-Prone Software Modules Using Complexity Metrics},
  volume = {9},
  number = {4},
  journal = {Quality Technology \& Quantitative Management},
  author = {Yu, Liguo and Mishra, Alok},
  year = {2012},
  pages = {421--434}
}

@article{Zhouabilitycomplexitymetrics2010,
  title = {On the Ability of Complexity Metrics to Predict Fault-Prone Classes in Object-Oriented Systems},
  volume = {83},
  number = {4},
  journal = {Journal of Systems and Software},
  author = {Zhou, Yuming and Xu, Baowen and Leung, Hareton},
  year = {2010},
  pages = {660--674}
}

@inproceedings{HassanPredictingfaultsusing2009,
  title = {Predicting Faults Using the Complexity of Code Changes},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Software Engineering}}},
  publisher = {{IEEE Computer Society}},
  author = {Hassan, Ahmed E},
  year = {2009},
  pages = {78--88}
}

@article{YooRegressiontestingminimization2012,
  title = {Regression Testing Minimization, Selection and Prioritization: A Survey},
  volume = {22},
  number = {2},
  journal = {Software Testing, Verification and Reliability},
  author = {Yoo, Shin and Harman, Mark},
  year = {2012},
  pages = {67--120}
}

@article{BertolinoHowmanypaths1996,
  title = {How Many Paths Are Needed for Branch Testing?},
  volume = {35},
  number = {2},
  journal = {Journal of Systems and Software},
  author = {Bertolino, Antonia and Marr\'e, Martina},
  year = {1996},
  pages = {95--106}
}

@article{Malevriscollateralcoveragedata2006,
  title = {The Collateral Coverage of Data Flow Criteria When Branch Testing},
  volume = {48},
  number = {8},
  journal = {Information and Software Technology},
  author = {Malevris, Nicos and Yates, Derek F},
  year = {2006},
  pages = {676--686}
}

@inproceedings{NogueiraPredictingsoftwarecomplexity2012,
  title = {Predicting Software Complexity by Means of Evolutionary Testing},
  booktitle = {Proceedings of the 27th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  publisher = {{ACM}},
  author = {Nogueira, Ana Filipa},
  year = {2012},
  pages = {402--405}
}

@article{McCabecomplexitymeasure1976,
  title = {A Complexity Measure},
  number = {4},
  journal = {IEEE Transactions on software Engineering},
  author = {McCabe, Thomas J},
  year = {1976},
  pages = {308--320}
}

@book{BoehmSoftwareitsimpact1973,
  title = {Software and Its Impact: {{A}} Quantitative Assessment},
  publisher = {{TRW Systems, Engineering and Integration Division}},
  author = {Boehm, Barry W},
  year = {1973}
}

@book{OviedoControlflowdata1984,
  title = {Control Flow, Data Flow and Program Complexity},
  publisher = {{State University of New York at Buffalo}},
  author = {Oviedo, Enrique Ivan},
  year = {1984}
}

@article{HalsteadElementsSoftwareScience1977,
  title = {Elements {{Of Software Science}}},
  journal = {Elsevierence},
  author = {Halstead, M. H},
  year = {1977}
}

@article{R.BasiliQualitativesoftwarecomplexity1980,
  title = {Qualitative Software Complexity Models: A Summary},
  journal = {IEEE Computer Society Press},
  author = {R. Basili, V},
  year = {1980}
}

@article{Shaonewmeasuresoftware2003,
  title = {A New Measure of Software Complexity Based on Cognitive Weights},
  volume = {28},
  number = {2},
  journal = {Canadian Journal of Electrical and Computer Engineering},
  author = {Shao, Jingqiu and Wang, Yingxu},
  year = {2003},
  pages = {69--74}
}

@inproceedings{CardosoControlflowcomplexitymeasurement2005,
  title = {Control-Flow Complexity Measurement of Processes and {{Weyuker}}'s Properties},
  volume = {8},
  booktitle = {6th {{International Enformatika Conference}}},
  author = {Cardoso, Jorge},
  year = {2005},
  pages = {213--218}
}

@article{KushwahaRobustnessanalysiscognitive2006,
  title = {Robustness Analysis of Cognitive Information Complexity Measure Using {{Weyuker}} Properties},
  volume = {31},
  number = {1},
  journal = {ACM SIGSOFT Software Engineering Notes},
  author = {Kushwaha, Dharmender Singh and Misra, Arun Kumar},
  year = {2006},
  pages = {1--6}
}

@article{WeyukerEvaluatingsoftwarecomplexity1988,
  title = {Evaluating Software Complexity Measures},
  volume = {14},
  number = {9},
  journal = {IEEE transactions on Software Engineering},
  author = {Weyuker, Elaine J.},
  year = {1988},
  pages = {1357--1365}
}

@article{KemererReliabilityfunctionpoints1993,
  title = {Reliability of Function Points Measurement: A Field Experiment},
  volume = {36},
  number = {2},
  journal = {Communications of the ACM},
  author = {Kemerer, Chris F},
  year = {1993},
  pages = {85--97}
}

@article{VesseyResearchstructuredprogramming1984,
  title = {Research on Structured Programming: {{An}} Empiricist's Evaluation},
  number = {4},
  journal = {IEEE Transactions on Software Engineering},
  author = {Vessey, Iris and Weber, Ron},
  year = {1984},
  pages = {397--407}
}

@inproceedings{Wandtheorydeepstructure1990,
  title = {Toward a Theory of the Deep Structure of Information Systems},
  booktitle = {{{ICIS}}},
  author = {Wand, Yair and Weber, Ron},
  year = {1990},
  pages = {3}
}

@techreport{EderCouplingcohesionobjectoriented1994,
  title = {Coupling and Cohesion in Object-Oriented Systems},
  institution = {{Technical Report, University of Klagenfurt}},
  author = {Eder, Johann and Kappel, Gerti and Schrefl, Michael},
  year = {1994}
}

@article{Kitchenhamframeworksoftwaremeasurement1995,
  title = {Towards a Framework for Software Measurement Validation},
  volume = {21},
  number = {12},
  journal = {IEEE Transactions on software Engineering},
  author = {Kitchenham, Barbara and Pfleeger, Shari Lawrence and Fenton, Norman},
  year = {1995},
  pages = {929--944}
}

@book{KellyComplexityAdvantage1999,
  title = {Complexity {{Advantage}}},
  publisher = {{McGraw-Hill Professional Publishing}},
  author = {Kelly, Susanne and Allison, Mary Ann},
  year = {1999}
}

@inproceedings{MarreReducingEstimatingCost1996,
  address = {Washington, DC, USA},
  series = {ICSE '96},
  title = {Reducing and {{Estimating}} the {{Cost}} of {{Test Coverage Criteria}}},
  isbn = {978-0-8186-7246-0},
  abstract = {Test coverage criteria define a set of entities of a program flowgraph and require that every entity is covered by some test. We first identify E/sub c/, the set of entities to be covered according to a criterion c, for a family of widely used test coverage criteria. We then present a method to derive a minimum set of entities, called a spanning set, such that a set of test paths covering the entities in this set covers every entity in E/sub c/. We provide a generalised algorithm, which is parametrized by the coverage criterion. We suggest several useful applications of spanning sets of entities to testing. In particular they help to reduce and to estimate the number of tests needed to satisfy test coverage criteria.},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Software Engineering}}},
  publisher = {{IEEE Computer Society}},
  author = {Marr\'e, Martina and Bertolino, Antonia},
  year = {1996},
  keywords = {flowcharting,generalised algorithm,minimum entity set,program flowgraph entities,program testing,software cost estimation,spanning set,test coverage criteria cost estimation,test coverage criteria cost reduction,test paths},
  pages = {486--494},
  file = {/Users/billlai/Zotero/storage/L3XQRS6I/Marré and Bertolino - 1996 - Reducing and Estimating the Cost of Test Coverage .pdf}
}

@article{ElberzhagerReducingtesteffort2012,
  title = {Reducing Test Effort: {{A}} Systematic Mapping Study on Existing Approaches},
  volume = {54},
  issn = {0950-5849},
  shorttitle = {Reducing Test Effort},
  doi = {10.1016/j.infsof.2012.04.007},
  abstract = {Quality assurance effort, especially testing effort, is often a major cost factor during software development, which sometimes consumes more than 50\% of the overall development effort. Consequently, one major goal is often to reduce testing effort. The main goal of the systematic mapping study is the identification of existing approaches that are able to reduce testing effort. Therefore, an overview should be presented both for researchers and practitioners in order to identify, on the one hand, future research directions and, on the other hand, potential for improvements in practical environments. Two researchers performed a systematic mapping study, focusing on four databases with an initial result set of 4020 articles. In total, we selected and categorized 144 articles. Five different areas were identified that exploit different ways to reduce testing effort: approaches that predict defect-prone parts or defect content, automation, test input reduction approaches, quality assurance techniques applied before testing, and test strategy approaches. The results reflect an increased interest in this topic in recent years. A lot of different approaches have been developed, refined, and evaluated in different environments. The highest attention was found with respect to automation and prediction approaches. In addition, some input reduction approaches were found. However, in terms of combining early quality assurance activities with testing to reduce test effort, only a small number of approaches were found. Due to the continuous challenge of reducing test effort, future research in this area is expected.},
  number = {10},
  journal = {Information and Software Technology},
  author = {Elberzhager, Frank and Rosbach, Alla and M\"unch, J\"urgen and Eschbach, Robert},
  month = oct,
  year = {2012},
  keywords = {Efficiency improvement,Mapping study,Quality assurance,Software testing,Test effort reduction},
  pages = {1092-1106},
  file = {/Users/billlai/Zotero/storage/FGITQI9F/Elberzhager et al. - 2012 - Reducing test effort A systematic mapping study o.pdf;/Users/billlai/Zotero/storage/DB6ID2EY/S0950584912000894.html}
}

@article{UrbanoOptimizationTechniquesAutomated2016,
  title = {{Optimization Techniques for Automated Software Test Data Generation}},
  copyright = {info:eu-repo/semantics/openAccess},
  abstract = {Esta tesis propone una variedad de contribuciones al campo de pruebas evolutivas. Hemos abarcados un amplio rango de aspectos relativos a las pruebas de programas: c\'odigo fuente procedimental y orientado a objetos, paradigmas estructural y funcional, problemas mono-objetivo y multi-objetivo, casos de prueba aislados y secuencias de pruebas, y trabajos te\'oricos y experimentales. En relaci\'on a los an\'alisis llevados a cabo, hemos puesto \'enfasis en el an\'alisis estad\'istico de los resultados para evaluar la significancia pr\'actica de los resultados. En resumen, las principales contribuciones de la tesis son:
Definici\'on de una nueva medida de distancia para el operador instanceof en programas orientados a objetos: En este trabajo nos hemos centrado en un aspecto relacionado con el software orientado a objetos, la herencia, para proponer algunos enfoques que pueden ayudar a guiar la b\'usqueda de datos de prueba en el contexto de las pruebas evolutivas. En particular, hemos propuesto una medida de distancia para computar la distancia de ramas en presencia del operador instanceof en programas Java. Tambi\'en hemos propuesto dos operadores de mutaci\'on que modifican las soluciones candidatas basadas en la medida de distancia definida.
Definici\'on de una nueva medida de complejidad llamada ‘‘Branch Coverage Expectation'': En este trabajo nos enfrentamos a la complejidad de pruebas desde un punto de vista original: un programa es m\'as complejo si es m\'as dif\'icil de probar de forma autom\'atica. Consecuentemente, definimos la ‘‘Branch Coverage Expectation'' para proporcionar conocimiento sobre la dificultad de probar programas. La fundaci\'on de esta medida se basa en el modelo de Markov del programa. El modelo de Markov proporciona fundamentos te\'oricos. El an\'alisis de esta medida indica que est\'a m\'as correlacionada con la cobertura de rama que las otras medidas de c\'odigo est\'aticas. Esto significa que esto es un buen modo de estimar la dificultad de probar un programa.

Predicci\'on te\'orica del n\'umero de casos de prueba necesarios para cubrir un porcentaje concreto de un programa: Nuestro modelo de Markov del programa puede ser usado para proporcionar una estimaci\'on del n\'umero de casos de prueba necesarios para cubrir un porcentaje concreto del programa. Hemos comparado nuestra predicci\'on te\'orica con la media de las ejecuciones reales de un generador de datos de prueba. Este modelo puede ayudar a predecir la evoluci\'on de la fase de pruebas, la cual consecuentemente puede ahorrar tiempo y coste del proyecto completo. Esta predicci\'on te\'orica podr\'ia ser tambi\'en muy \'util para determinar el porcentaje del programa cubierto dados un n\'umero de casos de prueba.

Propuesta de enfoques para resolver el problema de generaci\'on de datos de prueba multi-objetivo: En ese cap\'itulo estudiamos el problema de la generaci\'on multi-objetivo con el fin de analizar el rendimiento de un enfoque directo multi-objetivo frente a la aplicaci\'on de un algoritmo mono-objetivo seguido de una selecci\'on de casos de prueba. Hemos evaluado cuatro algoritmos multi-objetivo (MOCell, NSGA-II, SPEA2, y PAES) y dos algoritmos mono-objetivo (GA y ES), y dos algoritmos aleatorios. En t\'erminos de convergencia hac\'ia el frente de Pareto \'optimo, GA y MOCell han sido los mejores resolutores en nuestra comparaci\'on. Queremos destacar que el enfoque mono-objetivo, donde se ataca cada rama por separado, es m\'as efectivo cuando el programa tiene un grado de anidamiento alto.

Comparativa de diferentes estrategias de priorizaci\'on en l\'ineas de productos y \'arboles de clasificaci\'on: En el contexto de pruebas funcionales hemos tratado el tema de la priorizaci\'on de casos de prueba con dos representaciones diferentes, modelos de caracter\'isticas que representan l\'ineas de productos software y \'arboles de clasificaci\'on. Hemos comparado cinco enfoques relativos al m\'etodo de clasificaci\'on con \'arboles y dos relativos a l\'ineas de productos, cuatro de ellos propuestos por nosotros. Los resultados nos indican que las propuestas para ambas representaciones basadas en un algoritmo gen\'etico son mejores que el resto en la mayor\'ia de escenarios experimentales, es la mejor opci\'on cuando tenemos restricciones de tiempo o coste.

Definici\'on de la extensi\'on del m\'etodo de clasificaci\'on con \'arbol para la generaci\'on de secuencias de pruebas: Hemos definido formalmente esta extensi\'on para la generaci\'on de secuencias de pruebas que puede ser \'util para la industria y para la comunidad investigadora. Sus beneficios son claros ya que indudablemente el coste de situar el artefacto bajo pruebas en el siguiente estado no es necesario, a la vez que reducimos significativamente el tama\~no de la secuencia utilizando t\'ecnicas metaheur\'isticas. Particularmente nuestra propuesta basada en colonias de hormigas es el mejor algoritmo de la comparativa, siendo el \'unico algoritmo que alcanza la cobertura m\'axima para todos los modelos y tipos de cobertura.

Exploraci\'on del efecto de diferentes estrategias de seeding en el c\'alculo de frentes de Pareto \'optimos en l\'ineas de productos: Estudiamos el comportamiento de algoritmos cl\'asicos multi-objetivo evolutivos aplicados a las pruebas por pares de l\'ineas de productos. El grupo de algoritmos fue seleccionado para cubrir una amplia y diversa gama de t\'ecnicas. Nuestra evaluaci\'on indica claramente que las estrategias de seeding ayudan al proceso de b\'usqueda de forma determinante. Cuanta m\'as informaci\'on se disponga para crear esta poblaci\'on inicial, mejores ser\'an los resultados obtenidos. Adem\'as, gracias al uso de t\'ecnicas multi-objetivo podemos proporcionar un conjunto de pruebas adecuado mayor o menor, en resumen, que mejor se adapte a sus restricciones econ\'omicas o tecnol\'ogicas.

Propuesta de t\'ecnica exacta para la computaci\'on del frente de Pareto \'optimo en l\'ineas de productos software: Hemos propuesto un enfoque exacto para este c\'alculo en el caso multi-objetivo con cobertura paiwise. Definimos un programa lineal 0-1 y un algoritmo basado en resolutores SAT para obtener el frente de Pareto verdadero. La evaluaci\'on de los resultados nos indica que, a pesar de ser un fant\'astico m\'etodo para el c\'alculo de soluciones \'optimas, tiene el inconveniente de la escalabilidad, ya que para modelos grandes el tiempo de ejecuci\'on sube considerablemente. Tras realizar un estudio de correlaciones, confirmamos nuestras sospechas, existe una alta correlaci\'on entre el tiempo de ejecuci\'on y el n\'umero de productos denotado por el modelo de caracter\'isticas del programa.},
  language = {spa},
  author = {Urbano, Ferrer and Javier, Francisco},
  year = {2016},
  file = {/Users/billlai/Zotero/storage/DH2PKB2V/Urbano and Javier - 2016 - Optimization Techniques for Automated Software Tes.pdf;/Users/billlai/Zotero/storage/Q5NIPY6H/13056.html}
}

@article{MohammadianUsingProgramSlicing2013,
  title = {Using {{Program Slicing Technique}} to {{Reduce}} the {{Cost}} of {{Software Testing}}},
  volume = {2},
  issn = {2345-4652},
  abstract = {Systems of computers and their application in the lives of modern human beings are vastly expanding. In any kind of computer application, failure in computer systems can lead to a range of financial and mortal losses. Indeed, the major origin of software failure can be located in designing or implementing software. With regard to these statistics, 30\% of the software projects have been prosperous and successful. The proposed method is intended to reduce the cost and time of testing and it focuses on enhancing the efficiency of software testing methods. In this paper, we investigated the effect of slicing techniques on the reduction rate of testing cost and time. The results of experiments show that we can cover a large number of program instructions, branches and paths by a small number of test cases in the sliced program},
  number = {7},
  journal = {Journal of Artificial Intelligence in Electrical Engineering},
  author = {Mohammadian, Asghar and Arasteh, Bahman},
  month = nov,
  year = {2013},
  pages = {24-33},
  file = {/Users/billlai/Zotero/storage/3X5MYKBB/Mohammadian and Arasteh - 2013 - Using Program Slicing Technique to Reduce the Cost.pdf}
}

@article{GearyExplorationrelationshiptacit2016,
  title = {Exploration of the Relationship between Tacit Knowledge and Software System Test Complexity},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/3.0/},
  abstract = {This research has explored the relationship between system test complexity and tacit knowledge. It is proposed as part of this thesis, that the process of system testing (comprising of test planning, test development, test execution, test fault analysis, test measurement, and case management), is directly affected by both complexity associated with the system under test, and also by other sources of complexity, independent of the system under test, but related to the wider process of system testing. While a certain amount of knowledge related to the system under test is inherent, tacit in nature, and therefore difficult to make explicit, it has been found that a significant amount of knowledge relating to these other sources of complexity, can indeed be made explicit. While the importance of explicit knowledge has been reinforced by this research, there has been a lack of evidence to suggest that the availability of tacit knowledge to a test team is of any less importance to the process of system testing, when operating in a traditional software development environment. The sentiment was commonly expressed by participants, that even though a considerable amount of explicit knowledge relating to the system is freely available, that a good deal of knowledge relating to the system under test, which is demanded for effective system testing, is actually tacit in nature (approximately 60\% of participants operating in a traditional development environment, and 60\% of participants operating in an agile development environment, expressed similar sentiments). To cater for the availability of tacit knowledge relating to the system under test, and indeed, both explicit and tacit knowledge required by system testing in general, an appropriate knowledge management structure needs to be in place. This would appear to be required, irrespective of the employed development methodology.},
  language = {en},
  author = {Geary, Niall},
  year = {2016},
  file = {/Users/billlai/Zotero/storage/4526QJ9S/3113.html}
}

@book{HolzmannDesignValidationComputer2007,
  address = {Englewood Cliffs, New Jersey 07632},
  title = {Design and {{Validation}} of {{Computer Protocols}}},
  publisher = {{Prentice-Hall}},
  author = {Holzmann, Gerard J},
  year = {2007}
}

@article{FerrerEstimatingsoftwaretesting2013,
  title = {Estimating Software Testing Complexity},
  volume = {55},
  number = {12},
  journal = {Information and Software Technology},
  author = {Ferrer, Javier and Chicano, Francisco and Alba, Enrique},
  year = {2013},
  pages = {2125--2139}
}

@article{Turingcomputablenumbersapplication1937,
  title = {On Computable Numbers, with an Application to the {{Entscheidungsproblem}}},
  volume = {2},
  number = {1},
  journal = {Proceedings of the London mathematical society},
  author = {Turing, Alan Mathison},
  year = {1937},
  pages = {230--265}
}

@inproceedings{AjtaiLogSortingNetwork1983,
  address = {New York, NY, USA},
  series = {STOC '83},
  title = {An 0({{N Log N}}) {{Sorting Network}}},
  isbn = {978-0-89791-099-6},
  doi = {10.1145/800061.808726},
  abstract = {The purpose of this paper is to describe a sorting network of size 0(n log n) and depth 0(log n). A natural way of sorting is through consecutive halvings: determine the upper and lower halves of the set, proceed similarly within the halves, and so on. Unfortunately, while one can halve a set using only 0(n) comparisons, this cannot be done in less than log n (parallel) time, and it is known that a halving network needs (\textonehalf{})n log n comparisons. It is possible, however, to construct a network of 0(n) comparisons which halves in constant time with high accuracy. This procedure (\&egr;-halving) and a derived procedure (\&egr;-nearsort) are described below, and our sorting network will be centered around these elementary steps.},
  booktitle = {Proceedings of the {{Fifteenth Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Ajtai, M. and Koml\'os, J. and Szemer\'edi, E.},
  year = {1983},
  pages = {1--9},
  file = {/Users/billlai/Folder/Papers/p1-ajtai.pdf;/Users/billlai/Zotero/storage/UAHRDZP3/Ajtai et al. - 1983 - An 0(N Log N) Sorting Network.pdf}
}

@inproceedings{GreinerComparisonParallelAlgorithms1994,
  address = {New York, NY, USA},
  series = {SPAA '94},
  title = {A {{Comparison}} of {{Parallel Algorithms}} for {{Connected Components}}},
  isbn = {978-0-89791-671-4},
  doi = {10.1145/181014.181021},
  abstract = {This paper presents a comparison of the pragmatic aspects of some parallel algorithms for finding connected components, together with optimizations on these algorithms. The algorithms being compared are two similar algorithms by Shiloach-Vishkin [22] and Awerbuch-Shiloach [2], a randomized contraction algorithm based on algorithms by Reif [21] and Phillips [20], and a hybrid algorithm [11]. Improvements are given for the first two to improve performance significantly, although without improving their asymptotic complexity. The hybrid combines features of the others and is generally the fastest of those tested. Timings were made using NESL [4] code as executed on a Connection Machine 2 and Cray Y-MP/C90.},
  booktitle = {Proceedings of the {{Sixth Annual ACM Symposium}} on {{Parallel Algorithms}} and {{Architectures}}},
  publisher = {{ACM}},
  author = {Greiner, John},
  year = {1994},
  pages = {16--25},
  file = {/Users/billlai/Zotero/storage/YKJ6WMC8/Greiner - 1994 - A Comparison of Parallel Algorithms for Connected .pdf}
}

@inproceedings{ChongImprovedparallelalgorithms1995,
  title = {Improved Parallel Algorithms for Finding Connected Components},
  volume = {1},
  doi = {10.1109/ICAPP.1995.472217},
  abstract = {Finding the connected components of a graph is a basic computational problem. In recent years, there were several exciting results in breaking the log2 n-time barrier to finding connected components on parallel machines using shared memory without concurrent-write capability. This paper further presents two new parallel algorithms both using less than log2 n time. The merit of the first algorithm is that it uses only a sublinear number of processors, yet retains the time complexity of the fastest existing algorithm. The second algorithm is slightly slower but its work (i.e., the time-processor product) is closer to optimal than all previous algorithms using less than log2 n time},
  booktitle = {Proceedings 1st {{International Conference}} on {{Algorithms}} and {{Architectures}} for {{Parallel Processing}}},
  author = {Chong, K. W. and Lam, T. W.},
  month = apr,
  year = {1995},
  keywords = {Algorithm design and analysis,Application software,basic computational problem,computational complexity,computational geometry,Computer science,Concurrent computing,connected components finding,graph,graph theory,Joining processes,parallel algorithms,Parallel algorithms,Parallel machines,Phase change random access memory,Random access memory,Search methods,shared memory,time complexity,time-processor product},
  pages = {452-459 vol.1}
}

@inproceedings{BusParallelAlgorithmConnected2001,
  series = {Lecture Notes in Computer Science},
  title = {A {{Parallel Algorithm}} for {{Connected Components}} on {{Distributed Memory Machines}}},
  isbn = {978-3-540-42609-7 978-3-540-45417-5},
  doi = {10.1007/3-540-45417-9_39},
  abstract = {Finding connected components (CC) of an undirected graph is a fundamental computational problem. Various CC algorithms exist for PRAM models. An implementation of a PRAM CC algorithm on a coarse-grain MIMD machine with distributed memory brings many problems, since the communication overhead is substantial compared to the local computation. Several implementations of CC algorithms on distributed memory machines have been described in the literature, all in Split-C. We have designed and implemented a CC algorithm in C++ and MPI, by combining the ideas of the previous PRAM and distributed memory algorithms. Our main optimization is based on replacing the conditional hooking by rules for reducing nontrivial cycles during the contraction of components. We have also implemented a method for reducing the number of exchanged messages which is based on buffering messages and on deferred processing of answers.},
  language = {en},
  booktitle = {Recent {{Advances}} in {{Parallel Virtual Machine}} and {{Message Passing Interface}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Bu{\v s}, Libor and Tvrd\'ik, Pavel},
  month = sep,
  year = {2001},
  pages = {280-287},
  file = {/Users/billlai/Zotero/storage/9SDT3XCA/Buš and Tvrdík - 2001 - A Parallel Algorithm for Connected Components on D.pdf}
}

@inproceedings{BoulandTractableParameterizationsGraph2012,
  series = {Lecture Notes in Computer Science},
  title = {On {{Tractable Parameterizations}} of {{Graph Isomorphism}}},
  isbn = {978-3-642-33292-0 978-3-642-33293-7},
  doi = {10.1007/978-3-642-33293-7_21},
  abstract = {The fixed-parameter tractability of graph isomorphism is an open problem with respect to a number of natural parameters, such as tree-width, genus and maximum degree. We show that graph isomorphism is fixed-parameter tractable when parameterized by the tree-depth of the graph. We also extend this result to a parameter generalizing both tree-depth and max-leaf-number by deploying new variants of cops-and-robbers games.},
  language = {en},
  booktitle = {Parameterized and {{Exact Computation}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Bouland, Adam and Dawar, Anuj and Kopczy\'nski, Eryk},
  month = sep,
  year = {2012},
  pages = {218-230},
  file = {/Users/billlai/Zotero/storage/I6PDYSJ4/Bouland et al. - 2012 - On Tractable Parameterizations of Graph Isomorphis.pdf;/Users/billlai/Zotero/storage/GLEY8DGX/978-3-642-33293-7_21.html}
}

@article{EdwardsHarmoniousChromaticNumber1997,
  title = {The {{Harmonious Chromatic Number}} of {{Bounded Degree Graphs}}},
  volume = {55},
  issn = {0024-6107},
  doi = {10.1112/S0024610797004857},
  abstract = {A harmonious colouring of a simple graph G is a proper vertex colouring such that each pair of colours appears together on at most one edge. The harmonious chromatic number h(G) is the least number of colours in such a colouring.Let d be a fixed positive integer, and $\epsilon$\&gt;0. We show that there is a natural number M such that if G is any graph with m$\geq$M edges and maximum degree at most d, then the harmonious chromatic number h(G) satisfies~(2m)1/2$\leq$h(G)$\leq$(1+$\epsilon$)(2m)1/2.},
  language = {en},
  number = {3},
  journal = {Journal of the London Mathematical Society},
  author = {Edwards, Keith},
  month = jun,
  year = {1997},
  pages = {435-447},
  file = {/Users/billlai/Folder/Papers/THE HARMONIOUS CHROMATIC NUMBER OF BOUNDED DEGREE GRAPHS.pdf;/Users/billlai/Zotero/storage/6IPEBE4Y/855827.html}
}

@inproceedings{ElberfeldAlgorithmicMetaTheorems2012,
  address = {Dagstuhl, Germany},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  title = {Algorithmic {{Meta Theorems}} for {{Circuit Classes}} of {{Constant}} and {{Logarithmic Depth}}},
  volume = {14},
  isbn = {978-3-939897-35-4},
  doi = {10.4230/LIPIcs.STACS.2012.66},
  booktitle = {29th {{International Symposium}} on {{Theoretical Aspects}} of {{Computer Science}} ({{STACS}} 2012)},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  author = {Elberfeld, Michael and Jakoby, Andreas and Tantau, Till},
  editor = {D\"urr, Christoph and Wilke, Thomas},
  year = {2012},
  keywords = {algorithmic meta theorem,circuit complexity,monadic second-order logic,tree depth,tree width},
  pages = {66--77},
  file = {/Users/billlai/Folder/Papers/Algorithmic meta theorems for circuit classes of constant and logarithmic depth.pdf;/Users/billlai/Zotero/storage/ZXANQ4P7/Elberfeld et al. - 2012 - Algorithmic Meta Theorems for Circuit Classes of C.pdf;/Users/billlai/Zotero/storage/IKFKL7GS/3405.html}
}

@techreport{MurrayCircuitLowerBounds2017,
  title = {Circuit {{Lower Bounds}} for {{Nondeterministic Quasi}}-{{Polytime}}: {{An Easy Witness Lemma}} for {{NP}} and {{NQP}}},
  shorttitle = {Circuit {{Lower Bounds}} for {{Nondeterministic Quasi}}-{{Polytime}}},
  number = {188},
  author = {Murray, Cody and Williams, Ryan},
  year = {2017},
  keywords = {lower bounds,ACC,satisfiability,circuit complexity,derandomization,quasipolynomial time},
  file = {/Users/billlai/Folder/Papers/Circuit Lower Bounds for Nondeterministic Quasi-Polytime An Easy Witness Lemma for NP and NQP.pdf;/Users/billlai/Zotero/storage/VSPNMGJ5/Murray and Williams - 2017 - Circuit Lower Bounds for Nondeterministic Quasi-Po.pdf;/Users/billlai/Zotero/storage/LCRNAMJA/188.html}
}

@inproceedings{ReingoldEntropywaveszigzag2000,
  title = {Entropy Waves, the Zig-Zag Graph Product, and New Constant-Degree Expanders and Extractors},
  doi = {10.1109/SFCS.2000.892006},
  abstract = {The main contribution is a new type of graph product, which we call the zig-zag product. Taking a product of a large graph with a small graph, the resulting graph inherits (roughly) its size from the large one, its degree from the small one, and its expansion properties from both. Iteration yields simple explicit constructions of constant-degree expanders of every size, starting from one constant-size expander. Crucial to our intuition (and simple analysis) of the properties of this graph product is the view of expanders as functions which act as ``entropy wave'' propagators-they transform probability distributions in which entropy is concentrated in one area to distributions where that concentration is dissipated. In these terms, the graph product affords the constructive interference of two such waves. A variant of this product can be applied to extractors, giving the first explicit extractors whose seed length depends (poly)logarithmically on only the entropy deficiency of the source (rather than its length) and that extract almost all the entropy of high min-entropy sources. These high min-entropy extractors have several interesting applications, including the first constant-degree explicit expanders which beat the ``eigenvalue bound''},
  booktitle = {Proceedings 41st {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Reingold, O. and Vadhan, S. and Wigderson, A.},
  year = {2000},
  keywords = {Computer science,graph theory,Codes,Complexity theory,constant-degree expanders,constant-degree extractors,constructive interference,Cryptography,eigenvalue bound,eigenvalues and eigenfunctions,Eigenvalues and eigenfunctions,entropy,Entropy,entropy waves,explicit extractors,Graph theory,high min-entropy sources,Mathematics,Polynomials,probability,Probability distribution,probability distributions,zig-zag graph product},
  pages = {3-13},
  file = {/Users/billlai/Folder/Papers/Entropy waves, the zig-zag graph product, and new constant-degree expanders and extractors.pdf}
}

@article{ReingoldUndirectedConnectivityLogspace2008,
  title = {Undirected {{Connectivity}} in {{Log}}-Space},
  volume = {55},
  issn = {0004-5411},
  doi = {10.1145/1391289.1391291},
  abstract = {We present a deterministic, log-space algorithm that solves st-connectivity in undirected graphs. The previous bound on the space complexity of undirected st-connectivity was log4/3($\cdot$) obtained by Armoni, Ta-Shma, Wigderson and Zhou (JACM 2000). As undirected st-connectivity is complete for the class of problems solvable by symmetric, nondeterministic, log-space computations (the class SL), this algorithm implies that SL = L (where L is the class of problems solvable by deterministic log-space computations). Independent of our work (and using different techniques), Trifonov (STOC 2005) has presented an O(log n log log n)-space, deterministic algorithm for undirected st-connectivity. Our algorithm also implies a way to construct in log-space a fixed sequence of directions that guides a deterministic walk through all of the vertices of any connected graph. Specifically, we give log-space constructible universal-traversal sequences for graphs with restricted labeling and log-space constructible universal-exploration sequences for general graphs.},
  number = {4},
  journal = {J. ACM},
  author = {Reingold, Omer},
  month = sep,
  year = {2008},
  keywords = {bounded space algorithms,Derandomization,pseudorandom generator},
  pages = {17:1--17:24},
  file = {/Users/billlai/Folder/Papers/Undirected connectivity in log-space.pdf;/Users/billlai/Zotero/storage/HVRKWXQY/Reingold - 2008 - Undirected Connectivity in Log-space.pdf}
}

@inproceedings{SanthaQuantumRandomizedQuery2015,
  series = {Lecture Notes in Computer Science},
  title = {Quantum and {{Randomized Query Complexities}} ({{Extended Abstract}})},
  isbn = {978-3-319-17141-8 978-3-319-17142-5},
  doi = {10.1007/978-3-319-17142-5_3},
  abstract = {Deterministic query complexity is a simplified model of computation where the resource measured is only the number of questions to the input to get information about individual input bits, while all other operations are for free.},
  language = {en},
  booktitle = {Theory and {{Applications}} of {{Models}} of {{Computation}}},
  publisher = {{Springer, Cham}},
  author = {Santha, Miklos},
  month = may,
  year = {2015},
  pages = {18-19},
  file = {/Users/billlai/Zotero/storage/X4K26TEC/Santha - 2015 - Quantum and Randomized Query Complexities (Extende.pdf;/Users/billlai/Zotero/storage/SYY7C4HB/978-3-319-17142-5_3.html}
}

@article{ReidlFasterParameterizedAlgorithm2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.7540},
  primaryClass = {cs},
  title = {A {{Faster Parameterized Algorithm}} for {{Treedepth}}},
  abstract = {The width measure $\backslash$emph\{treedepth\}, also known as vertex ranking, centered coloring and elimination tree height, is a well-established notion which has recently seen a resurgence of interest. We present an algorithm which---given as input an \$n\$-vertex graph, a tree decomposition of the graph of width \$w\$, and an integer \$t\$---decides Treedepth, i.e. whether the treedepth of the graph is at most \$t\$, in time \$2\^\{O(wt)\} $\backslash$cdot n\$. If necessary, a witness structure for the treedepth can be constructed in the same running time. In conjunction with previous results we provide a simple algorithm and a fast algorithm which decide treedepth in time \$2\^\{2\^\{O(t)\}\} $\backslash$cdot n\$ and \$2\^\{O(t\^2)\} $\backslash$cdot n\$, respectively, which do not require a tree decomposition as part of their input. The former answers an open question posed by Ossona de Mendez and Nesetril as to whether deciding Treedepth admits an algorithm with a linear running time (for every fixed \$t\$) that does not rely on Courcelle's Theorem or other heavy machinery. For chordal graphs we can prove a running time of \$2\^\{O(t $\backslash$log t)\}$\backslash$cdot n\$ for the same algorithm.},
  journal = {arXiv:1401.7540 [cs]},
  author = {Reidl, Felix and Rossmanith, Peter and Villaamil, Fernando Sanchez and Sikdar, Somnath},
  month = jan,
  year = {2014},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics},
  file = {/Users/billlai/Folder/Papers/A Faster Parameterized Algorithm for Treedepth.pdf;/Users/billlai/Zotero/storage/KNAY8VGU/Reidl et al. - 2014 - A Faster Parameterized Algorithm for Treedepth.pdf;/Users/billlai/Zotero/storage/CJXEN399/1401.html}
}

@article{Bodlaenderckn5ApproximationAlgorithm2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1304.6321},
  primaryClass = {cs},
  title = {A {{O}}(Ckn) 5-{{Approximation Algorithm}} for {{Treewidth}}},
  abstract = {We give an algorithm that for an input n-vertex graph G and integer k$>$0, in time 2\^[O(k)]n either outputs that the treewidth of G is larger than k, or gives a tree decomposition of G of width at most 5k+4. This is the first algorithm providing a constant factor approximation for treewidth which runs in time single-exponential in k and linear in n. Treewidth based computations are subroutines of numerous algorithms. Our algorithm can be used to speed up many such algorithms to work in time which is single-exponential in the treewidth and linear in the input size.},
  journal = {arXiv:1304.6321 [cs]},
  author = {Bodlaender, Hans and Drange, P\aa{}l G. and Dregi, Markus S. and Fomin, Fedor V. and Lokshtanov, Daniel and Pilipczuk, Micha\l},
  month = apr,
  year = {2013},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics},
  file = {/Users/billlai/Zotero/storage/CALDLQQX/Bodlaender et al. - 2013 - A O(c^k n) 5-Approximation Algorithm for Treewidth.pdf;/Users/billlai/Zotero/storage/W77S5GR6/1304.html}
}

@inproceedings{BodlaenderLinearTimeAlgorithm1993,
  address = {New York, NY, USA},
  series = {STOC '93},
  title = {A {{Linear Time Algorithm}} for {{Finding Tree}}-Decompositions of {{Small Treewidth}}},
  isbn = {978-0-89791-591-5},
  doi = {10.1145/167088.167161},
  booktitle = {Proceedings of the {{Twenty}}-Fifth {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Bodlaender, Hans L.},
  year = {1993},
  keywords = {graph algorithms,graph minors,partial <italic>k</italic>-trees,pathwidth,treewidth},
  pages = {226--234},
  file = {/Users/billlai/Zotero/storage/HVVT7PPI/Bodlaender - 1993 - A Linear Time Algorithm for Finding Tree-decomposi.pdf}
}

@inproceedings{ElberfeldSpaceComplexityParameterized2012,
  series = {Lecture Notes in Computer Science},
  title = {On the {{Space Complexity}} of {{Parameterized Problems}}},
  isbn = {978-3-642-33292-0 978-3-642-33293-7},
  doi = {10.1007/978-3-642-33293-7_20},
  abstract = {Parameterized complexity theory measures the complexity of computational problems predominantly in terms of their parameterized time complexity. The purpose of the present paper is to demonstrate that the study of parameterized space complexity can give new insights into the complexity of well-studied parameterized problems like the feedback vertex set problem. We show that the undirected and the directed feedback vertex set problems have different parameterized space complexities, unless L = NL; which explains why the two problem variants seem to necessitate different algorithmic approaches even though their parameterized time complexity is the same. For a number of further natural parameterized problems, including the longest common subsequence problem and the acceptance problem for multi-head automata, we show that they lie in or are complete for different parameterized space classes; which explains why previous attempts at proving completeness of these problems for parameterized time classes have failed.},
  language = {en},
  booktitle = {Parameterized and {{Exact Computation}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Elberfeld, Michael and Stockhusen, Christoph and Tantau, Till},
  month = sep,
  year = {2012},
  pages = {206-217},
  file = {/Users/billlai/Zotero/storage/JYWLZRHE/Elberfeld et al. - 2012 - On the Space Complexity of Parameterized Problems.pdf;/Users/billlai/Zotero/storage/C4TKWGAT/978-3-642-33293-7_20.html}
}

@inproceedings{LokshtanovFixedParameterTractableCanonization2014,
  title = {Fixed-{{Parameter Tractable Canonization}} and {{Isomorphism Test}} for {{Graphs}} of {{Bounded Treewidth}}},
  doi = {10.1109/FOCS.2014.28},
  abstract = {We give a fixed-parameter tractable algorithm that, given a parameter k and two graphs G1, G2, either concludes that one of these graphs has treewidth at least k, or determines whether G1 and G2 are isomorphic. The running time of the algorithm on an n-vertex graph is 2O(k5 log k) $\cdot$ n5, and this is the first fixed-parameter algorithm for Graph Isomorphism parameterized by treewidth. Our algorithm in fact solves the more general canonization problem. We namely design a procedure working in 2OO(k5 log k) $\cdot$ n5 time that, for a given graph G on n vertices, either concludes that the treewidth of G is at least k, or finds an isomorphism-invariant construction term - an algebraic expression that encodes G together with a tree decomposition of G of width O(k4). Hence, a canonical graph isomorphic to G can be constructed by simply evaluating the obtained construction term, while the isomorphism test reduces to verifying whether the computed construction terms for G1 and G2 are equal.},
  booktitle = {2014 {{IEEE}} 55th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Lokshtanov, D. and Pilipczuk, M. and Pilipczuk, M. and Saurabh, S.},
  month = oct,
  year = {2014},
  keywords = {Algorithm design and analysis,computational complexity,graph theory,Complexity theory,Polynomials,treewidth,2OO(k5 log k) · n5 time,Adhesives,algebraic expression,canonical graph,canonization,fixed-parameter tractable algorithm,fixed-parameter tractable canonization graph,fixed-parameter tractable isomorphism test,graph isomorphism,Heuristic algorithms,isomorphism-invariant construction term,n-vertex graph,O(k4) width,parameterized algorithms,Particle separators,Standards},
  pages = {186-195},
  file = {/Users/billlai/Folder/Papers/Lokshtanov et al. - 2014 - Fixed-Parameter Tractable Canonization and Isomorp.pdf;/Users/billlai/Folder/Papers/Slides _ Fixed-Parameter Tractable Canonization and Isomorphism Test for Graphs of Bounded Treewidth.pdf;/Users/billlai/Zotero/storage/CM6KB5H9/6979003.html}
}

@article{IwataPowerTreeDepthFully2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.04376},
  primaryClass = {cs},
  title = {On the {{Power}} of {{Tree}}-{{Depth}} for {{Fully Polynomial FPT Algorithms}}},
  abstract = {There are many classical problems in P whose time complexities have not been improved over the past decades. Recent studies of "Hardness in P" have revealed that, for several of such problems, the current fastest algorithm is the best possible under some complexity assumptions. To bypass this difficulty, Fomin et al. (SODA 2017) introduced the concept of fully polynomial FPT algorithms. For a problem with the current best time complexity \$O(n\^c)\$, the goal is to design an algorithm running in \$k\^\{O(1)\}n\^\{c'\}\$ time for a parameter \$k\$ and a constant \$c'$<$c\$. In this paper, we investigate the complexity of graph problems in P parameterized by tree-depth, a graph parameter related to tree-width. We show that a simple divide-and-conquer method can solve many graph problems, including Weighted Matching, Negative Cycle Detection, Minimum Weight Cycle, Replacement Paths, and 2-hop Cover, in \$O($\backslash$mathrm\{td\}$\backslash$cdot m)\$ time or \$O($\backslash$mathrm\{td\}$\backslash$cdot (m+n$\backslash$log n))\$ time, where \$$\backslash$mathrm\{td\}\$ is the tree-depth of the input graph. Because any graph of tree-width \$$\backslash$mathrm\{tw\}\$ has tree-depth at most \$($\backslash$mathrm\{tw\}+1)$\backslash$log\_2 n\$, our algorithms also run in \$O($\backslash$mathrm\{tw\}$\backslash$cdot m$\backslash$log n)\$ time or \$O($\backslash$mathrm\{tw\}$\backslash$cdot (m+n$\backslash$log n)$\backslash$log n)\$ time. These results match or improve the previous best algorithms parameterized by tree-width. Especially, we solve an open problem of fully polynomial FPT algorithm for Weighted Matching parameterized by tree-width posed by Fomin et al.},
  journal = {arXiv:1710.04376 [cs]},
  author = {Iwata, Yoichi and Ogasawara, Tomoaki and Ohsaka, Naoto},
  month = oct,
  year = {2017},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics,Computer Science - Computational Complexity},
  file = {/Users/billlai/Zotero/storage/2WSUZ86E/Iwata et al. - 2017 - On the Power of Tree-Depth for Fully Polynomial FP.pdf}
}

@article{ElberfeldCanonizingGraphsBounded2017,
  title = {Canonizing {{Graphs}} of {{Bounded Tree Width}} in {{Logspace}}},
  volume = {9},
  issn = {1942-3454},
  doi = {10.1145/3132720},
  abstract = {Graph canonization is the problem of computing a unique representative, a canon, from the isomorphism class of a given graph. This implies that two graphs are isomorphic exactly if their canons are equal. We show that graphs of bounded tree width can be canonized by logarithmic-space (logspace) algorithms. This implies that the isomorphism problem for graphs of bounded tree width can be decided in logspace. In the light of isomorphism for trees being hard for the complexity class logspace, this makes the ubiquitous class of graphs of bounded tree width one of the few classes of graphs for which the complexity of the isomorphism problem has been exactly determined.},
  number = {3},
  journal = {ACM Trans. Comput. Theory},
  author = {Elberfeld, Michael and Schweitzer, Pascal},
  month = oct,
  year = {2017},
  keywords = {computational complexity,tree width,graph isomorphism,Algorithmic graph theory,graph canonization,logspace algorithms},
  pages = {12:1--12:29},
  file = {/Users/billlai/Zotero/storage/ZJYAFJUZ/Elberfeld and Schweitzer - 2017 - Canonizing Graphs of Bounded Tree Width in Logspac.pdf}
}

@inproceedings{deBeaudrapLinearTimeAlgorithm2016,
  address = {Germany},
  series = {CCC '16},
  title = {A {{Linear Time Algorithm}} for {{Quantum}} 2-{{SAT}}},
  isbn = {978-3-95977-008-8},
  doi = {10.4230/LIPIcs.CCC.2016.27},
  abstract = {The Boolean constraint satisfaction problem 3-SAT is arguably the canonical NP-complete problem. In contrast, 2-SAT can not only be decided in polynomial time, but in fact in deterministic linear time. In 2006, Bravyi proposed a physically motivated generalization of k-SAT to the quantum setting, defining the problem "quantum k-SAT". He showed that quantum 2-SAT is also solvable in polynomial time on a classical computer, in particular in deterministic time O(n4), assuming unit-cost arithmetic over a field extension of the rational numbers, where n is the number of variables. In this paper, we present an algorithm for quantum 2-SAT which runs in linear time, i.e. deterministic time O(n+m) for n and m the number of variables and clauses, respectively. Our approach exploits the transfer matrix techniques of Laumann et al. [QIC, 2010] used in the study of phase transitions for random quantum 2-SAT, and bears similarities with both the linear time 2-SAT algorithms of Even, Itai, and Shamir (based on backtracking) [SICOMP, 1976] and Aspvall, Plass, and Tarjan (based on strongly connected components) [IPL, 1979].},
  booktitle = {Proceedings of the 31st {{Conference}} on {{Computational Complexity}}},
  publisher = {{Schloss Dagstuhl\textendash{}Leibniz-Zentrum fuer Informatik}},
  author = {{de Beaudrap}, Niel and Gharibian, Sevag},
  year = {2016},
  keywords = {limited backtracking,local hamiltonian,quantum 2-SAT,strongly connected components,transfer matrix},
  pages = {27:1--27:21}
}

@article{SundaramLineartimealgorithm,
  title = {Linear Time Algorithm(s) for {{Quantum 2SAT}}},
  journal = {. . .},
  author = {Sundaram, Aarthi},
  pages = {61},
  file = {/Users/billlai/Zotero/storage/H9VZATQH/Sundaram - Linear time algorithm(s) for Quantum 2SAT.pdf}
}

@inproceedings{Beicomplexitytrialerror2013,
  title = {On the Complexity of Trial and Error},
  isbn = {978-1-4503-2029-0},
  doi = {10.1145/2488608.2488613},
  abstract = {Motivated by certain applications from physics, biochemistry, economics, and computer science in which the objects under investigation are unknown or not directly accessible because of various limitations, we propose a trial-and-error model to examine search problems in which inputs are unknown. More specifically, we consider constraint satisfaction problems i Ci, where the constraints Ci are hidden, and the goal is to find a solution satisfying all constraints. We can adaptively propose a candidate solution (i.e., trial), and there is a verification oracle that either confirms that it is a valid solution, or returns the index i of a violated constraint (i.e., error), with the exact content of Ci still hidden.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Bei, Xiaohui and Chen, Ning and Zhang, Shengyu},
  year = {2013},
  pages = {31},
  file = {/Users/billlai/Zotero/storage/KY99M4YS/Bei et al. - 2013 - On the complexity of trial and error.pdf}
}

@book{FlumParameterizedComplexityTheory2006,
  address = {Berlin Heidelberg},
  series = {Texts in Theoretical Computer Science. An EATCS Series},
  title = {Parameterized {{Complexity Theory}}},
  isbn = {978-3-540-29952-3},
  abstract = {Parameterized complexity theory is a recent branch of computational complexity theory that provides a framework for a refined analysis of hard algorithmic problems. The central notion of the theory, fixed-parameter tractability, has led to the development of various new algorithmic techniques and a whole new theory of intractability. This book is a state-of-the-art introduction to both algorithmic techniques for fixed-parameter tractability and the structural theory of parameterized complexity classes, and it presents detailed proofs of recent advanced results that have not appeared in book form before. Several chapters are each devoted to intractability, algorithmic techniques for designing fixed-parameter tractable algorithms, and bounded fixed-parameter tractability and subexponential time complexity. The treatment is comprehensive, and the reader is supported with exercises, notes, a detailed index, and some background on complexity theory and logic. The book will be of interest to computer scientists, mathematicians and graduate students engaged with algorithms and problem complexity.},
  language = {en},
  publisher = {{Springer-Verlag}},
  author = {Flum, J. and Grohe, M.},
  year = {2006},
  file = {/Users/billlai/Folder/Papers/Flum and Grohe - 2006 - Parameterized Complexity Theory.pdf}
}

@article{AlonColorcoding1995,
  title = {Color-Coding},
  volume = {42},
  issn = {0004-5411},
  doi = {10.1145/210332.210337},
  number = {4},
  journal = {J. ACM},
  author = {Alon, Noga and Yuster, Raphael and Zwick, Uri},
  month = jul,
  year = {1995},
  keywords = {derandomization,perfect hashing,subgraph isomorphism,tree-width},
  pages = {844--856},
  file = {/Users/billlai/Zotero/storage/3MW4PB9D/Alon et al. - 1995 - Color-coding.pdf}
}

@inproceedings{ReingoldUndirectedSTconnectivityLogspace2005,
  address = {New York, NY, USA},
  series = {STOC '05},
  title = {Undirected {{ST}}-Connectivity in {{Log}}-Space},
  isbn = {978-1-58113-960-0},
  doi = {10.1145/1060590.1060647},
  abstract = {We present a deterministic, log-space algorithm that solves st-connectivity in undirected graphs. The previous bound on the space complexity of undirected st-connectivity was log4/3 obtained by Armoni, Ta-Shma, Wigderson and Zhou [9]. As undirected st-connectivity is complete for the class of problems solvable by symmetric, non-deterministic, log-space computations (the class SL), this algorithm implies that SL = L (where L is the class of problems solvable by deterministic log-space computations). Independent of our work (and using different techniques), Trifonov [45] has presented an O(log n log log n)-space, deterministic algorithm for undirected st-connectivity.Our algorithm also implies a way to construct in log-space a fixed sequence of directions that guides a deterministic walk through all of the vertices of any connected graph. Specifically, we give log-space constructible universal-traversal sequences for graphs with restricted labelling and log-space constructible universal-exploration sequences for general graphs.},
  booktitle = {Proceedings of the {{Thirty}}-Seventh {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Reingold, Omer},
  year = {2005},
  pages = {376--385},
  file = {/Users/billlai/Zotero/storage/QM7GEWGE/Reingold - 2005 - Undirected ST-connectivity in Log-space.pdf}
}

@article{FederComputationalStructureMonotone1998,
  title = {The {{Computational Structure}} of {{Monotone Monadic SNP}} and {{Constraint Satisfaction}}: {{A Study}} through {{Datalog}} and {{Group Theory}}},
  volume = {28},
  issn = {0097-5397},
  shorttitle = {The {{Computational Structure}} of {{Monotone Monadic SNP}} and {{Constraint Satisfaction}}},
  doi = {10.1137/S0097539794266766},
  abstract = {This paper starts with the project of finding a large subclass of NP which exhibits a dichotomy. The approach is to find this subclass via syntactic prescriptions. While the paper does not achieve this goal, it does isolate a class (of problems specified by) "monotone monadic SNP without inequality" which may exhibit this dichotomy. We justify the placing of all these restrictions by showing, essentially using Ladner's theorem, that classes obtained by using only two of the above three restrictions do not show this dichotomy. We then explore the structure of this class. We show that all problems in this class reduce to the seemingly simpler class CSP. We divide CSP into subclasses and try to unify the collection of all known polytime algorithms for CSP problems and extract properties that make CSP problems NP-hard. This is where the second part of the title, "a study through Datalog and group theory," comes in. We present conjectures about this class which would end in showing the dichotomy.},
  number = {1},
  journal = {SIAM Journal on Computing},
  author = {Feder, T. and Vardi, M.},
  month = jan,
  year = {1998},
  pages = {57-104},
  file = {/Users/billlai/Zotero/storage/WAWP7GX3/Feder and Vardi - 1998 - The Computational Structure of Monotone Monadic SN.pdf}
}

@article{KempeComplexityLocalHamiltonian2004,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {quant-ph/0406180},
  title = {The {{Complexity}} of the {{Local Hamiltonian Problem}}},
  abstract = {The k-local Hamiltonian problem is a natural complete problem for the complexity class QMA, the quantum analog of NP. It is similar in spirit to MAX-k-SAT, which is NP-complete for k$<$=2. It was known that the problem is QMA-complete for any k $<$= 3. On the other hand 1-local Hamiltonian is in P, and hence not believed to be QMA-complete. The complexity of the 2-local Hamiltonian problem has long been outstanding. Here we settle the question and show that it is QMA-complete. We provide two independent proofs; our first proof uses only elementary linear algebra. Our second proof uses a powerful technique for analyzing the sum of two Hamiltonians; this technique is based on perturbation theory and we believe that it might prove useful elsewhere. Using our techniques we also show that adiabatic computation with two-local interactions on qubits is equivalent to standard quantum computation.},
  journal = {arXiv:quant-ph/0406180},
  author = {Kempe, Julia and Kitaev, Alexei and Regev, Oded},
  month = jun,
  year = {2004},
  keywords = {Computer Science - Computational Complexity,Quantum Physics},
  file = {/Users/billlai/Zotero/storage/CYMGY3LY/Kempe et al. - 2004 - The Complexity of the Local Hamiltonian Problem.pdf}
}

@article{BravyiEfficientalgorithmquantum2006,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {quant-ph/0602108},
  title = {Efficient Algorithm for a Quantum Analogue of 2-{{SAT}}},
  abstract = {Complexity of a quantum analogue of the satisfiability problem is studied. Quantum k-SAT is a problem of verifying whether there exists n-qubit pure state such that its k-qubit reduced density matrices have support on prescribed subspaces. We present a classical algorithm solving quantum 2-SAT in a polynomial time. It generalizes the well-known algorithm for the classical 2-SAT. Besides, we show that for any k$>$=4 quantum k-SAT is complete in the complexity class QMA with one-sided error.},
  journal = {arXiv:quant-ph/0602108},
  author = {Bravyi, Sergey},
  month = feb,
  year = {2006},
  keywords = {Quantum Physics},
  file = {/Users/billlai/Zotero/storage/46YQAVF4/Bravyi - 2006 - Efficient algorithm for a quantum analogue of 2-SA.pdf;/Users/billlai/Zotero/storage/RQEQXYU8/0602108.html}
}

@article{GossetQuantum3SATQMA1complete2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1302.0290},
  primaryClass = {quant-ph},
  title = {Quantum 3-{{SAT}} Is {{QMA1}}-Complete},
  doi = {10.1109/FOCS.2013.86},
  abstract = {Quantum satisfiability is a constraint satisfaction problem that generalizes classical boolean satisfiability. In the quantum k-SAT problem, each constraint is specified by a k-local projector and is satisfied by any state in its nullspace. Bravyi showed that quantum 2-SAT can be solved efficiently on a classical computer and that quantum k-SAT with k greater than or equal to 4 is QMA1-complete. Quantum 3-SAT was known to be contained in QMA1, but its computational hardness was unknown until now. We prove that quantum 3-SAT is QMA1-hard, and therefore complete for this complexity class.},
  journal = {arXiv:1302.0290 [quant-ph]},
  author = {Gosset, David and Nagaj, Daniel},
  month = oct,
  year = {2013},
  keywords = {Computer Science - Computational Complexity,Quantum Physics},
  pages = {756-765},
  file = {/Users/billlai/Zotero/storage/2ZDFDTWE/Gosset and Nagaj - 2013 - Quantum 3-SAT is QMA1-complete.pdf;/Users/billlai/Zotero/storage/MZ3FHZ4C/1302.html}
}

@article{BravyiEfficientalgorithmquantum2006a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {quant-ph/0602108},
  title = {Efficient Algorithm for a Quantum Analogue of 2-{{SAT}}},
  abstract = {Complexity of a quantum analogue of the satisfiability problem is studied. Quantum k-SAT is a problem of verifying whether there exists n-qubit pure state such that its k-qubit reduced density matrices have support on prescribed subspaces. We present a classical algorithm solving quantum 2-SAT in a polynomial time. It generalizes the well-known algorithm for the classical 2-SAT. Besides, we show that for any k$>$=4 quantum k-SAT is complete in the complexity class QMA with one-sided error.},
  journal = {arXiv:quant-ph/0602108},
  author = {Bravyi, Sergey},
  month = feb,
  year = {2006},
  keywords = {Quantum Physics},
  file = {/Users/billlai/Zotero/storage/NVWRB6CC/Bravyi - 2006 - Efficient algorithm for a quantum analogue of 2-SA.pdf;/Users/billlai/Zotero/storage/69TLWYX6/0602108.html}
}

@article{HagerupCharacterizingMultiterminalFlow1998,
  title = {Characterizing {{Multiterminal Flow Networks}} and {{Computing Flows}} in {{Networks}} of {{Small Treewidth}}},
  volume = {57},
  issn = {0022-0000},
  doi = {10.1006/jcss.1998.1592},
  abstract = {We show that if a flow network haskinput/output terminals (for the traditional maximum-flow problem,k=2), its external flow pattern (the possible values of flow into and out of the terminals) has two characterizations of size independent of the total number of vertices: a set of 2k+1 inequalities inkvariables representing flow values at the terminals, and a mimicking network with at most 22kvertices and the same external flow pattern as the original network. For the case in which the underlying graph has bounded treewidth, we present sequential and parallel algorithms that can compute these characterizations as well as a flow consistent with any desired feasible external flow (including a maximum flow between two given terminals). For constantk, the sequential algorithm runs inO(n) time onn-vertex networks, and the parallel algorithm runs inO(logn) time on an EREW PRAM withO(n/logn) processors if an explicit tree decomposition of the network of sizeO(n) is given; if not, known algorithms can compute such a tree decomposition inO((logn)2) time usingO(n/(logn)2) processors.},
  number = {3},
  journal = {J. Comput. Syst. Sci.},
  author = {Hagerup, Torben and Katajainen, Jyrki and Nishimura, Naomi and Ragde, Prabhakar},
  month = dec,
  year = {1998},
  pages = {366--375},
  file = {/Users/billlai/Folder/Papers/Hagerup et al. - 1998 - Characterizing Multiterminal Flow Networks and Com.pdf}
}

@article{SchoningGraphisomorphismlow1988,
  title = {Graph Isomorphism Is in the Low Hierarchy},
  volume = {37},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(88)90010-4},
  abstract = {It is shown that the graph isomorphism problem is located in level L2p of the low hierarchy in NP. This implies that this problem is not NP-complete (\ldots},
  language = {en},
  number = {3},
  journal = {Journal of Computer and System Sciences},
  author = {Schoning, Uwe},
  month = dec,
  year = {1988},
  pages = {312-323},
  file = {/Users/billlai/Folder/Papers/1988 - Graph isomorphism is in the low hierarchy.pdf;/Users/billlai/Zotero/storage/JSV59LN6/0022000088900104.html}
}

@article{FlumDescribingParameterizedComplexity2003,
  title = {Describing {{Parameterized Complexity Classes}}},
  volume = {187},
  issn = {0890-5401},
  doi = {10.1016/S0890-5401(03)00161-5},
  abstract = {We describe parameterized complexity classes by means of classical complexity theory and descriptive complexity theory. For every classical complexity class we introduce a parameterized analogue in a natural way. In particular, the analogue of polynomial time is the class of all fixed-parameter tractable problems. We develop a basic complexity theory for the parameterized analogues of classical complexity classes and give, among other things, complete problems and logical descriptions. We then show that most of the well-known intractable parameterized complexity classes are not analogues of classical classes. Nevertheless, for all these classes we can provide natural logical descriptions.},
  number = {2},
  journal = {Inf. Comput.},
  author = {Flum, J\"org and Grohe, Martin},
  month = dec,
  year = {2003},
  pages = {291--319},
  file = {/Users/billlai/Folder/Papers/Flum and Grohe - 2003 - Describing Parameterized Complexity Classes.pdf}
}

@inproceedings{RossmanConstantdepthComplexityKclique2008,
  address = {New York, NY, USA},
  series = {STOC '08},
  title = {On the {{Constant}}-Depth {{Complexity}} of {{K}}-Clique},
  isbn = {978-1-60558-047-0},
  doi = {10.1145/1374376.1374480},
  abstract = {We prove a lower bound of $\omega$(nk/4) on the size of constant-depth circuits solving the k-clique problem on n-vertex graphs (for every constant k). This improves a lower bound of $\omega$(nk/89d2) due to Beame where d is the circuit depth. Our lower bound has the advantage that it does not depend on the constant d in the exponent of n, thus breaking the mold of the traditional size-depth tradeoff. Our k-clique lower bound derives from a stronger result of independent interest. Suppose fn :0,1n/2 $\rightarrow$ \{0,1\} is a sequence of functions computed by constant-depth circuits of size O(nt). Let G be an Erdos-Renyi random graph with vertex set \{1,...,n\} and independent edge probabilities n-$\alpha$ where $\alpha$ $\leq$ 1/2t-1. Let A be a uniform random k-element subset of \{1,...,n\} (where k is any constant independent of n) and let KA denote the clique supported on A. We prove that fn(G) = fn(G $\cup$ KA) asymptotically almost surely. These results resolve a long-standing open question in finite model theory (going back at least to Immerman in 1982). The m-variable fragment of first-order logic, denoted by FOm, consists of the first-order sentences which involve at most m variables. Our results imply that the bounded variable hierarchy FO1 $\subset$ FO2 $\subset$ ... $\subset$ FOm $\subset$ ... is strict in terms of expressive power on finite ordered graphs. It was previously unknown that FO3 is less expressive than full first-order logic on finite ordered graphs.},
  booktitle = {Proceedings of the {{Fortieth Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Rossman, Benjamin},
  year = {2008},
  keywords = {circuit complexity,ac0,bounded variable hierarchy,constant-depth circuits,first-order logic,k-clique},
  pages = {721--730},
  file = {/Users/billlai/Zotero/storage/BRHHX8XD/Rossman - 2008 - On the Constant-depth Complexity of K-clique.pdf}
}

@article{FominComputingTreedepthFaster2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1306.3857},
  primaryClass = {cs, math},
  title = {Computing {{Tree}}-Depth {{Faster Than}} \$2\^\{n\}\$},
  abstract = {A connected graph has tree-depth at most \$k\$ if it is a subgraph of the closure of a rooted tree whose height is at most \$k\$. We give an algorithm which for a given \$n\$-vertex graph \$G\$, in time \$$\backslash$mathcal\{O\}(1.9602\^n)\$ computes the tree-depth of \$G\$. Our algorithm is based on combinatorial results revealing the structure of minimal rooted trees whose closures contain \$G\$.},
  journal = {arXiv:1306.3857 [cs, math]},
  author = {Fomin, Fedor V. and Giannopoulou, Archontia C. and Pilipczuk, Micha\l},
  month = jun,
  year = {2013},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Combinatorics},
  file = {/Users/billlai/Zotero/storage/4TV4VL9Y/Fomin et al. - 2013 - Computing Tree-depth Faster Than $2^ n $.pdf;/Users/billlai/Zotero/storage/XU777V67/1306.html}
}

@article{NesetrilLowTreeDepthDecompositions2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.1581},
  title = {On {{Low Tree}}-{{Depth Decompositions}}},
  volume = {31},
  issn = {0911-0119, 1435-5914},
  doi = {10.1007/s00373-015-1569-7},
  abstract = {The theory of sparse structures usually uses tree like structures as building blocks. In the context of sparse/dense dichotomy this role is played by graphs with bounded tree depth. In this paper we survey results related to this concept and particularly explain how these graphs are used to decompose and construct more complex graphs and structures. In more technical terms we survey some of the properties and applications of low tree depth decomposition of graphs.},
  number = {6},
  journal = {Graphs and Combinatorics},
  author = {Nesetril, Jaroslav and De Mendez, Patrice Ossona},
  month = nov,
  year = {2015},
  keywords = {Mathematics - Combinatorics},
  pages = {1941-1963},
  file = {/Users/billlai/Zotero/storage/7DJLZUDW/Nesetril and De Mendez - 2015 - On Low Tree-Depth Decompositions.pdf;/Users/billlai/Zotero/storage/86CVHER3/1412.html}
}

@article{BannachFastParallelFixedParameter2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.06984},
  primaryClass = {cs},
  title = {Fast {{Parallel Fixed}}-{{Parameter Algorithms}} via {{Color Coding}}},
  abstract = {Fixed-parameter algorithms have been successfully applied to solve numerous difficult problems within acceptable time bounds on large inputs. However, most fixed-parameter algorithms are inherently $\backslash$emph\{sequential\} and, thus, make no use of the parallel hardware present in modern computers. We show that parallel fixed-parameter algorithms do not only exist for numerous parameterized problems from the literature -- including vertex cover, packing problems, cluster editing, cutting vertices, finding embeddings, or finding matchings -- but that there are parallel algorithms working in $\backslash$emph\{constant\} time or at least in time $\backslash$emph\{depending only on the parameter\} (and not on the size of the input) for these problems. Phrased in terms of complexity classes, we place numerous natural parameterized problems in parameterized versions of AC\$\^0\$. On a more technical level, we show how the $\backslash$emph\{color coding\} method can be implemented in constant time and apply it to embedding problems for graphs of bounded tree-width or tree-depth and to model checking first-order formulas in graphs of bounded degree.},
  journal = {arXiv:1509.06984 [cs]},
  author = {Bannach, Max and Stockhusen, Christoph and Tantau, Till},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Computational Complexity,F.1.3},
  file = {/Users/billlai/Zotero/storage/RFVPGWYR/Bannach et al. - 2015 - Fast Parallel Fixed-Parameter Algorithms via Color.pdf;/Users/billlai/Zotero/storage/7RYBLRI8/1509.html}
}

@article{BannachFastParallelFixedParameter2015a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.06984},
  primaryClass = {cs},
  title = {Fast {{Parallel Fixed}}-{{Parameter Algorithms}} via {{Color Coding}}},
  abstract = {Fixed-parameter algorithms have been successfully applied to solve numerous difficult problems within acceptable time bounds on large inputs. However, most fixed-parameter algorithms are inherently $\backslash$emph\{sequential\} and, thus, make no use of the parallel hardware present in modern computers. We show that parallel fixed-parameter algorithms do not only exist for numerous parameterized problems from the literature -- including vertex cover, packing problems, cluster editing, cutting vertices, finding embeddings, or finding matchings -- but that there are parallel algorithms working in $\backslash$emph\{constant\} time or at least in time $\backslash$emph\{depending only on the parameter\} (and not on the size of the input) for these problems. Phrased in terms of complexity classes, we place numerous natural parameterized problems in parameterized versions of AC\$\^0\$. On a more technical level, we show how the $\backslash$emph\{color coding\} method can be implemented in constant time and apply it to embedding problems for graphs of bounded tree-width or tree-depth and to model checking first-order formulas in graphs of bounded degree.},
  journal = {arXiv:1509.06984 [cs]},
  author = {Bannach, Max and Stockhusen, Christoph and Tantau, Till},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Computational Complexity,F.1.3},
  file = {/Users/billlai/Zotero/storage/6P9EI2NN/Bannach et al. - 2015 - Fast Parallel Fixed-Parameter Algorithms via Color.pdf;/Users/billlai/Zotero/storage/VG9D4JFM/1509.html}
}

@article{Chenlowerboundsparameterized2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.08014},
  primaryClass = {cs},
  title = {Some Lower Bounds in Parameterized \$\{$\backslash$rm \vphantom\}{{AC}}\vphantom\{\}\^0\$},
  abstract = {We demonstrate some lower bounds for parameterized problems via parameterized classes corresponding to the classical \$\{$\backslash$rm AC\}\^0\$. Among others, we derive such a lower bound for all fpt-approximations of the parameterized clique problem and for a parameterized halting problem, which recently turned out to link problems of computational complexity, descriptive complexity, and proof theory. To show the first lower bound, we prove a strong \$\{$\backslash$rm AC\}\^0\$ version of the planted clique conjecture: \$\{$\backslash$rm AC\}\^0\$-circuits asymptotically almost surely can not distinguish between a random graph and this graph with a randomly planted clique of any size \$$\backslash$le n\^$\backslash$xi\$ (where \$0 $\backslash$le $\backslash$xi $<$ 1\$).},
  journal = {arXiv:1606.08014 [cs]},
  author = {Chen, Yijia and Flum, Joerg},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Computational Complexity},
  file = {/Users/billlai/Zotero/storage/82EKCRQD/Chen and Flum - 2016 - Some lower bounds in parameterized $ rm AC ^0$.pdf;/Users/billlai/Zotero/storage/4SPVHR5H/1606.html}
}

@article{LiACComplexitySubgraph2017,
  title = {On the \${{AC}}\^0\$ {{Complexity}} of {{Subgraph Isomorphism}}},
  volume = {46},
  issn = {0097-5397},
  doi = {10.1137/14099721X},
  abstract = {Let \$P\$ be a fixed graph (hereafter called a ``pattern''), and let \$\{$\backslash$sc Subgraph\}(P)\$ denote the problem of deciding whether a given graph \$G\$ contains a subgraph isomorphic to \$P\$. We are interested in \$AC\^0\$-complexity of this problem, determined by the smallest possible exponent \$C(P)\$ for which \$\{$\backslash$sc Subgraph\}(P)\$ possesses bounded-depth circuits of size \$n\^\{C(P)+o(1)\}\$. Motivated by the previous research in the area, we also consider its ``colorful'' version \$\{$\backslash$sc Subgraph\}\_$\backslash$mathsf\{col\}(P)\$ in which the target graph \$G\$ is \$V(P)\$-colored, and the average-case version \$\{$\backslash$sc Subgraph\}\_$\backslash$mathsf\{ave\}(P)\$ under the distribution \$G(n,n\^\{-$\backslash$theta(P)\})\$, where \$$\backslash$theta(P)\$ is the threshold exponent of \$P\$. Defining \$C\_$\backslash$mathsf\{col\}(P)\$ and \$C\_$\backslash$mathsf\{ave\}(P)\$ analogously to \$C(P)\$, our main contributions can be summarized as follows: (1) \$C\_$\backslash$mathsf\{col\}(P)\$ coincides with the treewidth of the pattern \$P\$ up to a logarithmic factor. This shows that the previously known upper bound by Alon, Yuster, and Zwick [J. ACM, 42 (1995), pp. 844--856] is almost tight. (2) We give a characterization of \$C\_$\backslash$mathsf\{ave\}(P)\$ in purely combinatorial terms up to a multiplicative factor of 2. This shows that the lower bound technique of Rossman [Proceedings of the  40th ACM Symposium on Theory of Computing, 2008, pp. 721--730] is essentially tight for any pattern \$P\$ whatsoever. (3) We prove that if \$Q\$ is a minor of \$P\$, then \$\{$\backslash$sc Subgraph\}\_$\backslash$mathsf\{col\}(Q)\$ is reducible to \$\{$\backslash$sc Subgraph\}\_$\backslash$mathsf\{col\}(P)\$ via a linear-size monotone projection. At the same time, we show that there is no monotone projection whatsoever that reduces \$\{$\backslash$sc Subgraph\}(M\_3)\$ to \$\{$\backslash$sc Subgraph\}(P\_3 + M\_2)\$ (\$P\_3\$ is a path on three vertices,  \$M\_k\$ is a matching with \$k\$ edges, and ``+'' stands for the disjoint union). This result strongly suggests that the colorful version of the subgraph isomorphism problem is much better structured and well-behaved than the standard (worst-case, uncolored) one.},
  number = {3},
  journal = {SIAM Journal on Computing},
  author = {Li, Y. and Razborov, A. and Rossman, B.},
  month = jan,
  year = {2017},
  pages = {936-971},
  file = {/Users/billlai/Zotero/storage/3GHGSWRN/Li et al. - 2017 - On the $AC^0$ Complexity of Subgraph Isomorphism.pdf;/Users/billlai/Zotero/storage/LE6NNT28/14099721X.html}
}

@article{BeyersdorffParameterizedComplexityDPLL2013,
  title = {Parameterized {{Complexity}} of {{DPLL Search Procedures}}},
  volume = {14},
  issn = {1529-3785},
  doi = {10.1145/2499937.2499941},
  abstract = {We study the performance of DPLL algorithms on parameterized problems. In particular, we investigate how difficult it is to decide whether small solutions exist for satisfiability and other combinatorial problems. For this purpose we develop a Prover-Delayer game that models the running time of DPLL procedures and we establish an information-theoretic method to obtain lower bounds to the running time of parameterized DPLL procedures. We illustrate this technique by showing lower bounds to the parameterized pigeonhole principle and to the ordering principle. As our main application we study the DPLL procedure for the problem of deciding whether a graph has a small clique. We show that proving the absence of a k-clique requires n$\Omega$(k) steps for a nontrivial distribution of graphs close to the critical threshold. For the restricted case of tree-like Parameterized Resolution, this result answers a question asked by Beyersdorff et al. [2012] of understanding the Resolution complexity of this family of formulas.},
  number = {3},
  journal = {ACM Trans. Comput. Logic},
  author = {Beyersdorff, Olaf and Galesi, Nicola and Lauria, Massimo},
  month = aug,
  year = {2013},
  keywords = {Proof complexity,parameterized complexity,prover-delayer games,resolution},
  pages = {20:1--20:21},
  file = {/Users/billlai/Zotero/storage/4RGLPVMC/Beyersdorff et al. - 2013 - Parameterized Complexity of DPLL Search Procedures.pdf}
}

@article{AllenderAmplifyingLowerBounds2010,
  title = {Amplifying {{Lower Bounds}} by {{Means}} of {{Self}}-Reducibility},
  volume = {57},
  issn = {0004-5411},
  doi = {10.1145/1706591.1706594},
  abstract = {We observe that many important computational problems in NC1 share a simple self-reducibility property. We then show that, for any problem A having this self-reducibility property, A has polynomial-size TC0 circuits if and only if it has TC0 circuits of size n1+\&epsis; for every \&epsis;$>$ 0 (counting the number of wires in a circuit as the size of the circuit). As an example of what this observation yields, consider the Boolean Formula Evaluation problem (BFE), which is complete for NC1 and has the self-reducibility property. It follows from a lower bound of Impagliazzo, Paturi, and Saks, that BFE requires depth d TC0 circuits of size n1+\&epsis;d. If one were able to improve this lower bound to show that there is some constant \&epsis;$>$ 0 (independent of the depth d) such that every TC0 circuit family recognizing BFE has size at least n1+\&epsis;, then it would follow that TC0 $\not =$ NC1. We show that proving lower bounds of the form n1+\&epsis; is not ruled out by the Natural Proof framework of Razborov and Rudich and hence there is currently no known barrier for separating classes such as ACC0, TC0 and NC1 via existing ``natural'' approaches to proving circuit lower bounds. We also show that problems with small uniform constant-depth circuits have algorithms that simultaneously have small space and time bounds. We then make use of known time-space tradeoff lower bounds to show that SAT requires uniform depth d TC0 and AC0[6] circuits of size n1+c for some constant c depending on d.},
  number = {3},
  journal = {J. ACM},
  author = {Allender, Eric and Kouck\'y, Michal},
  month = mar,
  year = {2010},
  keywords = {lower bounds,Circuit complexity,natural proofs,self-reducibility,time-space tradeoffs},
  pages = {14:1--14:36},
  file = {/Users/billlai/Zotero/storage/9N8YZQVU/Allender and Koucký - 2010 - Amplifying Lower Bounds by Means of Self-reducibil.pdf}
}

@article{ImpagliazzoSearchEasyWitness2002,
  title = {In {{Search}} of an {{Easy Witness}}: {{Exponential Time}} vs. {{Probabilistic Polynomial Time}}},
  volume = {65},
  issn = {0022-0000},
  shorttitle = {In {{Search}} of an {{Easy Witness}}},
  doi = {10.1016/S0022-0000(02)00024-7},
  abstract = {Restricting the search space \{0,1\}n to the set of truth tables of "easy" Boolean functions on log n variables, as well as using some known hardness-randomness tradeoffs, we establish a number of results relating the complexity of exponential-time and probabilistic polynomial-time complexity classes. In particular, we show that NEXP$\subset$P/poly $\leftarrow$ NEXP = MA; this can be interpreted as saying that no derandomization of MA (and, hence, of promise-BPP) is possible unless NEXP contains a hard Boolean function. We also prove several downward closure results for ZPP, RP, BPP, and MA; e.g., we show EXP = BPP $\leftarrow$ EE = BPE, where EE is the double-exponential time class and BPE is the exponential-time analogue of BPP.},
  number = {4},
  journal = {J. Comput. Syst. Sci.},
  author = {Impagliazzo, Russell and Kabanets, Valentine and Wigderson, Avi},
  month = dec,
  year = {2002},
  pages = {672--694},
  file = {/Users/billlai/Folder/Papers/Impagliazzo et al. - 2002 - In Search of an Easy Witness Exponential Time vs..pdf}
}

@article{NisanHardnessvsRandomness1994,
  title = {Hardness vs {{Randomness}}},
  volume = {49},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(05)80043-1},
  abstract = {We present a simple new construction of a pseudorandom bit generator. It stretches a short string of truly random bits into a long string that looks random to any algorithm from a complexity class C (e.g., P, NC, PSPACE, ...) using an arbitrary function that is hard for C. This construction reveals an equivalence between the problem of proving lower bounds and the problem of generating good pseudorandom sequences. Our construction has many consequences. The most direct one is that efficient deterministic simulation of randomized algorithms is possible under much weaker assumptions than previously known. The efficiency of the simulations depends on the strength of the assumptions, and may achieve P = BPP. We believe that our results are very strong evidence that the gap between randomized and deterministic complexity is not large. Using the known lower bounds for constant depth circuits, our construction yields an unconditionally proven pseudorandom generator for constant depth circuits. As an application of this generator we characterize the power of NP with a random oracle.},
  number = {2},
  journal = {J. Comput. Syst. Sci.},
  author = {Nisan, Noam and Wigderson, Avi},
  month = oct,
  year = {1994},
  pages = {149--167}
}

@article{GroheFasterIsomorphismTest2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.04659},
  primaryClass = {cs, math},
  title = {A {{Faster Isomorphism Test}} for {{Graphs}} of {{Small Degree}}},
  abstract = {Luks's algorithm (JCSS 1982) to test isomorphism of bounded degree graphs in polynomial time is one of the most important results in the context of the Graph Isomorphism Problem and has been repeatedly used as a basic building block for many other algorithms. In particular, for graphs of logarithmic degree, Babai's quasipolynomial isomorphism test (STOC 2016) essentially boils down to Luks's algorithm, and any improvement of Babai's algorithm requires an improved isomorphism test for graphs of (poly)logarithmic degree. In this work, we obtain such an improvement: we give an algorithm that solves the Graph Isomorphism Problem in time \$n\^\{$\backslash$mathcal\{O\}(($\backslash$log d)\^\{c\})\}\$, where \$n\$ is the number of vertices of the input graphs, \$d\$ is the maximum degree of the input graphs, and \$c\$ is an absolute constant. The best previous isomorphism test for graphs of maximum degree \$d\$ due to Babai, Kantor and Luks (FOCS 1983) runs in time \$n\^\{$\backslash$mathcal\{O\}(d/ $\backslash$log d)\}\$. Our result generalizes the quasipolynomial-time algorithm for the general isomorphism problem due to Babai.},
  journal = {arXiv:1802.04659 [cs, math]},
  author = {Grohe, Martin and Neuen, Daniel and Schweitzer, Pascal},
  month = feb,
  year = {2018},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics,Mathematics - Combinatorics,Mathematics - Group Theory},
  file = {/Users/billlai/Zotero/storage/Z9ZIPSLU/Grohe et al. - 2018 - A Faster Isomorphism Test for Graphs of Small Degr.pdf;/Users/billlai/Zotero/storage/ZV5RQH79/1802.html}
}

@inproceedings{Ben-SassonShortPCPsProjection2014,
  series = {Lecture Notes in Computer Science},
  title = {Short {{PCPs}} with {{Projection Queries}}},
  isbn = {978-3-662-43947-0 978-3-662-43948-7},
  doi = {10.1007/978-3-662-43948-7_14},
  abstract = {We construct a PCP for NTIME(2 n ) with constant soundness, 2 n poly(n) proof length, and poly(n) queries where the verifier's computation is simple: the queries are a projection of the input randomness, and the computation on the prover's answers is a 3CNF. The previous upper bound for these two computations was polynomial-size circuits. Composing this verifier with a proof oracle increases the circuit-depth of the latter by 2. Our PCP is a simple variant of the PCP by Ben-Sasson, Goldreich, Harsha, Sudan, and Vadhan (CCC 2005). We also give a more modular exposition of the latter, separating the combinatorial from the algebraic arguments.If our PCP is taken as a black box, we obtain a more direct proof of the result by Williams, later with Santhanam (CCC 2013) that derandomizing circuits on n bits from a class C in time 2 n /n $\omega$(1) yields that NEXP is not in a related circuit class C${'}$. Our proof yields a tighter connection: C is an And-Or of circuits from C${'}$. Along the way we show that the same lower bound follows if the satisfiability of the And of any 3 circuits from C${'}$ can be solved in time 2 n /n $\omega$(1).},
  language = {en},
  booktitle = {Automata, {{Languages}}, and {{Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {{Ben-Sasson}, Eli and Viola, Emanuele},
  month = jul,
  year = {2014},
  pages = {163-173},
  file = {/Users/billlai/Zotero/storage/GHFND28J/Ben-Sasson and Viola - 2014 - Short PCPs with Projection Queries.pdf;/Users/billlai/Zotero/storage/DYCHL2UX/10.html}
}

@article{BengtssonQuantumComputationComputer2005,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {quant-ph/0511274},
  title = {Quantum {{Computation}}: {{A Computer Science Perspective}}},
  shorttitle = {Quantum {{Computation}}},
  abstract = {The theory of quantum computation is presented in a self contained way from a computer science perspective. The basics of classical computation and quantum mechanics is reviewed. The circuit model of quantum computation is presented in detail. Throughout there is an emphasis on the physical as well as the abstract aspects of computation and the interplay between them. This report is presented as a Master's thesis at the department of Computer Science and Engineering at G\{$\backslash$"o\}teborg University, G\{$\backslash$"o\}teborg, Sweden. The text is part of a larger work that is planned to include chapters on quantum algorithms, the quantum Turing machine model and abstract approaches to quantum computation.},
  journal = {arXiv:quant-ph/0511274},
  author = {Bengtsson, Anders K. H.},
  month = nov,
  year = {2005},
  keywords = {Quantum Physics},
  file = {/Users/billlai/Zotero/storage/B6DSYV76/Bengtsson - 2005 - Quantum Computation A Computer Science Perspectiv.pdf;/Users/billlai/Zotero/storage/9IDJ7DKH/0511274.html}
}

@article{VaziraniQuantumComputationCS,
  title = {Quantum {{Computation}}: {{A CS Perspective}}},
  language = {en},
  author = {Vazirani, Umesh V and Berkeley, U C},
  pages = {21},
  file = {/Users/billlai/Zotero/storage/BKMEN85J/Vazirani and Berkeley - Quantum Computation A CS Perspective.pdf}
}

@article{MerminQuantumComputerScience,
  title = {Quantum {{Computer Science}}: {{An Introduction}}},
  language = {en},
  author = {Mermin, N David},
  pages = {237},
  file = {/Users/billlai/Zotero/storage/WBRT7AQJ/Mermin - Quantum Computer Science An Introduction.pdf}
}

@inproceedings{DasLogspaceFPTAlgorithms2015,
  series = {Lecture Notes in Computer Science},
  title = {Logspace and {{FPT Algorithms}} for {{Graph Isomorphism}} for {{Subclasses}} of {{Bounded Tree}}-{{Width Graphs}}},
  isbn = {978-3-319-15611-8 978-3-319-15612-5},
  doi = {10.1007/978-3-319-15612-5_30},
  abstract = {We give a deterministic logspace algorithm for the graph isomorphism problem for graphs with bounded tree-depth. We also show that the graph isomorphism problem is fixed parameter tractable for a related parameterized graph class where the graph parameter is the length of the longest cycle.},
  language = {en},
  booktitle = {{{WALCOM}}: {{Algorithms}} and {{Computation}}},
  publisher = {{Springer, Cham}},
  author = {Das, Bireswar and Enduri, Murali Krishna and Reddy, I. Vinod},
  month = feb,
  year = {2015},
  pages = {329-334},
  file = {/Users/billlai/Folder/Papers/Das et al. - 2015 - Logspace and FPT Algorithms for Graph Isomorphism .pdf;/Users/billlai/Zotero/storage/CADLS7QI/978-3-319-15612-5_30.html}
}

@incollection{ChenBoundedVariableLogic2014,
  address = {Berlin, Heidelberg},
  title = {Bounded {{Variable Logic}}, {{Parameterized Logarithmic Space}}, and {{Savitch}}'s {{Theorem}}},
  volume = {8634},
  isbn = {978-3-662-44521-1 978-3-662-44522-8},
  abstract = {We study the parameterized space complexity of model-checking first-order logic with a bounded number of variables. By restricting the number of the quantifier alternations we obtain problems complete for a natural hierarchy between parameterized logarithmic space and FPT. We call this hierarchy the tree hierarchy, provide a machine characterization, and link it to the recently introduced classes PATH and TREE. We show that the lowest class PATH collapses to parameterized logarithmic space only if Savitch's theorem can be improved. Finally, we settle the complexity with respect to the tree-hierarchy of finding short undirected paths and small undirected trees.},
  language = {en},
  booktitle = {Mathematical {{Foundations}} of {{Computer Science}} 2014},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Chen, Yijia and M\"uller, Moritz},
  editor = {{Csuhaj-Varj\'u}, Erzs\'ebet and Dietzfelbinger, Martin and \'Esik, Zolt\'an},
  year = {2014},
  pages = {183-195},
  file = {/Users/billlai/Zotero/storage/829VIM46/Chen and Müller - 2014 - Bounded Variable Logic, Parameterized Logarithmic .pdf},
  doi = {10.1007/978-3-662-44522-8_16}
}

@article{LokshtanovFixedparametertractablecanonization,
  title = {Fixed-Parameter Tractable Canonization and Isomorphism Test for Graphs of Bounded Treewidth${_\ast}$},
  abstract = {We give a fixed-parameter tractable algorithm that, given a parameter k and two graphs G1, G2, either concludes that one of these graphs has treewidth at least k, or determines whether G1 and G2 are isomorphic. The running time of the algorithm on an n-vertex graph is 2O(k5 log k) $\cdot$ n5, and this is the first fixed-parameter algorithm for Graph Isomorphism parameterized by treewidth.},
  language = {en},
  author = {Lokshtanov, Daniel and Pilipczuk, Marcin and Pilipczuk, Michal and Saurabh, Saket},
  pages = {30},
  file = {/Users/billlai/Zotero/storage/G3EUMDAE/Lokshtanov et al. - Fixed-parameter tractable canonization and isomorp.pdf}
}

@article{Groheimprovedisomorphismtest2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.06858},
  primaryClass = {cs, math},
  title = {An Improved Isomorphism Test for Bounded-Tree-Width Graphs},
  abstract = {We give a new fpt algorithm testing isomorphism of \$n\$-vertex graphs of tree width \$k\$ in time \$2\^\{k$\backslash$operatorname\{polylog\} (k)\}$\backslash$operatorname\{poly\} (n)\$, improving the fpt algorithm due to Lokshtanov, Pilipczuk, Pilipczuk, and Saurabh (FOCS 2014), which runs in time \$2\^\{$\backslash$mathcal\{O\}(k\^5$\backslash$log k)\}$\backslash$operatorname\{poly\} (n)\$. Based on an improved version of the isomorphism-invariant graph decomposition technique introduced by Lokshtanov et al., we prove restrictions on the structure of the automorphism groups of graphs of tree width \$k\$. Our algorithm then makes heavy use of the group theoretic techniques introduced by Luks (JCSS 1982) in his isomorphism test for bounded degree graphs and Babai (STOC 2016) in his quasipolynomial isomorphism test. In fact, we even use Babai's algorithm as a black box in one place. We also give a second algorithm which, at the price of a slightly worse running time \$2\^\{$\backslash$mathcal\{O\}(k\^2 $\backslash$log k)\}$\backslash$operatorname\{poly\} (n)\$, avoids the use of Babai's algorithm and, more importantly, has the additional benefit that it can also used as a canonization algorithm.},
  journal = {arXiv:1803.06858 [cs, math]},
  author = {Grohe, Martin and Neuen, Daniel and Schweitzer, Pascal and Wiebking, Daniel},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics,Mathematics - Combinatorics},
  file = {/Users/billlai/Zotero/storage/63R5UPYY/Grohe et al. - 2018 - An improved isomorphism test for bounded-tree-widt.pdf;/Users/billlai/Zotero/storage/65UCMT69/1803.html}
}

@article{BerryIntroductionCliqueMinimal2010,
  title = {An {{Introduction}} to {{Clique Minimal Separator Decomposition}}},
  volume = {3},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  doi = {10.3390/a3020197},
  abstract = {This paper is a review which presents and explains the decomposition of graphs by clique minimal separators. The pace is leisurely, we give many examples and figures. Easy algorithms are provided to implement this decomposition. The historical and theoretical background is given, as well as sketches of proofs of the structural results involved.},
  language = {en},
  number = {2},
  journal = {Algorithms},
  author = {Berry, Anne and Pogorelcnik, Romain and Simonet, Genevi\`eve},
  month = may,
  year = {2010},
  keywords = {clique minimal separator,graph decomposition,minimal triangulation},
  pages = {197-215},
  file = {/Users/billlai/Zotero/storage/6TJCPGWK/Berry et al. - 2010 - An Introduction to Clique Minimal Separator Decomp.pdf;/Users/billlai/Zotero/storage/43PZVHFF/197.html}
}

@article{WilliamsNewalgorithmslower2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.2444},
  primaryClass = {cs},
  title = {New Algorithms and Lower Bounds for Circuits with Linear Threshold Gates},
  abstract = {Let ACC$\smwhtcircle$ THR be the class of constant-depth circuits comprised of AND, OR, and MODm gates (for some constant m $>$ 1), with a bottom layer of gates computing arbitrary linear threshold functions. This class of circuits can be seen as a ``midpoint'' between ACC (where we know nontrivial lower bounds) and depth-two linear threshold circuits (where nontrivial lower bounds remain open).},
  language = {en},
  journal = {arXiv:1401.2444 [cs]},
  author = {Williams, Ryan},
  month = jan,
  year = {2014},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Computational Complexity},
  file = {/Users/billlai/Zotero/storage/T3K8X6PX/Williams - 2014 - New algorithms and lower bounds for circuits with .pdf}
}

@article{LokshtanovFixedParameterTractableCanonization2017,
  title = {Fixed-{{Parameter Tractable Canonization}} and {{Isomorphism Test}} for {{Graphs}} of {{Bounded Treewidth}}},
  volume = {46},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/140999980},
  abstract = {We give a fixed-parameter tractable algorithm that, given a parameter k and two graphs G1, G2, either concludes that one of these graphs has treewidth at least k or determines whether G1 and G2 are isomorphic. The running time of the algorithm on an n-vertex graph is 2O(k5 log k) $\cdot$n5, and this is the first fixed-parameter algorithm for Graph Isomorphism parameterized by treewidth. Our algorithm in fact solves the more general canonization problem. We namely design a procedure working in 2O(k5 log k) $\cdot$ n5 time that, for a given graph G on n vertices, either concludes that the treewidth of G is at least k or (i) finds in an isomorphic-invariant way a graph c(G) that is isomorphic to G; (ii) finds an isomorphism-invariant construction term\textemdash{}an algebraic expression that encodes G together with a tree decomposition of G of width less than k. Hence, the isomorphism test reduces to verifying whether the computed isomorphic copies or the construction terms for G1 and G2 are equal.},
  language = {en},
  number = {1},
  journal = {SIAM Journal on Computing},
  author = {Lokshtanov, Daniel and Pilipczuk, Marcin and Pilipczuk, Micha\l{} and Saurabh, Saket},
  month = jan,
  year = {2017},
  pages = {161-189},
  file = {/Users/billlai/Zotero/storage/87X4TRVR/Lokshtanov et al. - 2017 - Fixed-Parameter Tractable Canonization and Isomorp.pdf}
}

@article{HaastadAverageCaseDepthHierarchy2017,
  title = {An {{Average}}-{{Case Depth Hierarchy Theorem}} for {{Boolean Circuits}}},
  volume = {64},
  issn = {0004-5411},
  doi = {10.1145/3095799},
  abstract = {We prove an average-case depth hierarchy theorem for Boolean circuits over the standard basis of AND, OR, and NOT gates. Our hierarchy theorem says that for every d $\geq$ 2, there is an explicit n-variable Boolean function f, computed by a linear-size depth-d formula, which is such that any depth-(d-1) circuit that agrees with f on (1/2 + on(1)) fraction of all inputs must have size exp(n$\Omega$ (1/d)). This answers an open question posed by H\aa{}stad in his Ph.D. thesis (H\aa{}stad 1986b). Our average-case depth hierarchy theorem implies that the polynomial hierarchy is infinite relative to a random oracle with probability 1, confirming a conjecture of H\aa{}stad (1986a), Cai (1986), and Babai (1987). We also use our result to show that there is no ``approximate converse'' to the results of Linial, Mansour, Nisan (Linial et al. 1993) and (Boppana 1997) on the total influence of bounded-depth circuits. A key ingredient in our proof is a notion of random projections which generalize random restrictions.},
  number = {5},
  journal = {J. ACM},
  author = {{H$\backslash$a astad}, Johan and Rossman, Benjamin and Servedio, Rocco A. and Tan, Li-Yang},
  month = aug,
  year = {2017},
  keywords = {Boolean circuit complexity,polynomial hierarchy,random oracles,random projections},
  pages = {35:1--35:27},
  file = {/Users/billlai/Zotero/storage/IV9RB4IC/Ha astad et al. - 2017 - An Average-Case Depth Hierarchy Theorem for Boolea.pdf}
}

@inproceedings{LibertySimpleDeterministicMatrix2013,
  address = {New York, NY, USA},
  series = {KDD '13},
  title = {Simple and {{Deterministic Matrix Sketching}}},
  isbn = {978-1-4503-2174-7},
  doi = {10.1145/2487575.2487623},
  abstract = {A sketch of a matrix A is another matrix B which is significantly smaller than A but still approximates it well. Finding such sketches efficiently is an important building block in modern algorithms for approximating, for example, the PCA of massive matrices. This task is made more challenging in the streaming model, where each row of the input matrix can only be processed once and storage is severely limited. In this paper we adapt a well known streaming algorithm for approximating item frequencies to the matrix sketching setting. The algorithm receives n rows of a large matrix A $\epsilon$ $\mathfrak{R}$ n x m one after the other in a streaming fashion. It maintains a sketch B $\mathfrak{R}$ l x m containing only l $<<$ n rows but still guarantees that ATA BTB. More accurately, $\forall$x || x,||=1 0$\leq$||Ax||2 - ||Bx||2 $\leq$ 2||A||\_f 2 l Or BTB prec ATA and ||ATA - BTB|| $\leq$ 2 ||A||f2 l. This gives a streaming algorithm whose error decays proportional to 1/l using O(ml) space. For comparison, random-projection, hashing or sampling based algorithms produce convergence bounds proportional to 1/$\surd$l. Sketch updates per row in A require amortized O(ml) operations and the algorithm is perfectly parallelizable. Our experiments corroborate the algorithm's scalability and improved convergence rate. The presented algorithm also stands out in that it is deterministic, simple to implement and elementary to prove.},
  booktitle = {Proceedings of the 19th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  author = {Liberty, Edo},
  year = {2013},
  keywords = {sketching,streaming},
  pages = {581--588},
  file = {/Users/billlai/Zotero/storage/EU97Q4N2/Liberty - 2013 - Simple and Deterministic Matrix Sketching.pdf}
}

@inproceedings{ShahafConnectingDotsNews2010,
  address = {New York, NY, USA},
  series = {KDD '10},
  title = {Connecting the {{Dots Between News Articles}}},
  isbn = {978-1-4503-0055-1},
  doi = {10.1145/1835804.1835884},
  abstract = {The process of extracting useful knowledge from large datasets has become one of the most pressing problems in today's society. The problem spans entire sectors, from scientists to intelligence analysts and web users, all of whom are constantly struggling to keep up with the larger and larger amounts of content published every day. With this much data, it is often easy to miss the big picture. In this paper, we investigate methods for automatically connecting the dots -- providing a structured, easy way to navigate within a new topic and discover hidden connections. We focus on the news domain: given two news articles, our system automatically finds a coherent chain linking them together. For example, it can recover the chain of events starting with the decline of home prices (January 2007), and ending with the ongoing health-care debate. We formalize the characteristics of a good chain and provide an efficient algorithm (with theoretical guarantees) to connect two fixed endpoints. We incorporate user feedback into our framework, allowing the stories to be refined and personalized. Finally, we evaluate our algorithm over real news data. Our user studies demonstrate the algorithm's effectiveness in helping users understanding the news.},
  booktitle = {Proceedings of the 16th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  author = {Shahaf, Dafna and Guestrin, Carlos},
  year = {2010},
  keywords = {coherence,news},
  pages = {623--632},
  file = {/Users/billlai/Zotero/storage/BPZPRWTX/Shahaf and Guestrin - 2010 - Connecting the Dots Between News Articles.pdf}
}

@article{Bar-YehudaDistributedepsilonApproximation2017,
  title = {A {{Distributed}} (2 + \$$\backslash$epsilon\$)-{{Approximation}} for {{Vertex Cover}} in {{O}}({{Log}} \$$\backslash${{Delta}}\$ / \$$\backslash$epsilon\$ {{Log Log}} \$$\backslash${{Delta}}\$) {{Rounds}}},
  volume = {64},
  issn = {0004-5411},
  doi = {10.1145/3060294},
  abstract = {We present a simple deterministic distributed (2 + $\epsilon$)-approximation algorithm for minimum-weight vertex cover, which completes in O(log $\Delta$/$\epsilon$log log $\Delta$) rounds, where $\Delta$ is the maximum degree in the graph, for any $\epsilon$ $>$ 0 that is at most O(1). For a constant $\epsilon$, this implies a constant approximation in O(log $\Delta$/log log $\Delta$) rounds, which contradicts the lower bound of [KMW10].},
  number = {3},
  journal = {J. ACM},
  author = {{Bar-Yehuda}, Reuven and {Censor-Hillel}, Keren and Schwartzman, Gregory},
  month = jun,
  year = {2017},
  keywords = {graph algorithms,approximation algorithms,Distributed computing,local-ratio,vertex cover},
  pages = {23:1--23:11},
  file = {/Users/billlai/Zotero/storage/WDWZ5PLR/Bar-Yehuda et al. - 2017 - A Distributed (2 + $epsilon$)-Approximation for V.pdf}
}

@article{GalWhichFairestRent2017,
  title = {Which {{Is}} the {{Fairest}} ({{Rent Division}}) of {{Them All}}?},
  volume = {64},
  issn = {0004-5411},
  doi = {10.1145/3131361},
  abstract = {``Mirror, mirror, on the wall, who is the fairest of them all?'' The Evil Queen What is a fair way to assign rooms to several housemates and divide the rent between them? This is not just a theoretical question: many people have used the Spliddit website to obtain envy-free solutions to rent division instances. But envy freeness, in and of itself, is insufficient to guarantee outcomes that people view as intuitive and acceptable. We therefore focus on solutions that optimize a criterion of social justice, subject to the envy-freeness constraint, in order to pinpoint the ``fairest'' solutions. We develop a general algorithmic framework that enables the computation of such solutions in polynomial time. We then study the relations between natural optimization objectives and identify the maximin solution, which maximizes the minimum utility subject to envy freeness, as the most attractive. We demonstrate, in theory and using experiments on real data from Spliddit, that the maximin solution gives rise to significant gains in terms of our optimization objectives. Finally, a user study with Spliddit users as subjects demonstrates that people find the maximin solution to be significantly fairer than arbitrary envy-free solutions; this user study is unprecedented in that it asks people about their real-world rent division instances. Based on these results, the maximin solution has been deployed on Spliddit since April 2015.},
  number = {6},
  journal = {J. ACM},
  author = {Gal, Ya'akov (Kobi) and Mash, Moshe and Procaccia, Ariel D. and Zick, Yair},
  month = nov,
  year = {2017},
  keywords = {Computational,division,fair},
  pages = {39:1--39:22},
  file = {/Users/billlai/Zotero/storage/5SHR3WWK/Gal et al. - 2017 - Which Is the Fairest (Rent Division) of Them All.pdf}
}

@article{AwasthiPowerLocalizationEfficiently2017,
  title = {The {{Power}} of {{Localization}} for {{Efficiently Learning Linear Separators}} with {{Noise}}},
  volume = {63},
  issn = {0004-5411},
  doi = {10.1145/3006384},
  abstract = {We introduce a new approach for designing computationally efficient learning algorithms that are tolerant to noise, and we demonstrate its effectiveness by designing algorithms with improved noise tolerance guarantees for learning linear separators. We consider both the malicious noise model of Valiant [1985] and Kearns and Li [1988] and the adversarial label noise model of Kearns, Schapire, and Sellie [1994]. For malicious noise, where the adversary can corrupt both the label and the features, we provide a polynomial-time algorithm for learning linear separators in $\mathfrak{R}$d under isotropic log-concave distributions that can tolerate a nearly information-theoretically optimal noise rate of $\eta$ = $\Omega$($\epsilon$), improving on the $\Omega$ (\&frac;$\epsilon$3/log2(d/$\epsilon$)) noise-tolerance of Klivans et al. [2009a]. In the case that the distribution is uniform over the unit ball, this improves on the $\Omega$ (\&frac;$\epsilon$/d1/4) noise-tolerance of Kalai et al. [2005] and the $\Omega$ (\&frac;$\epsilon$2/log(d/$\epsilon$)) of Klivans et al. [2009a]. For the adversarial label noise model, where the distribution over the feature vectors is unchanged and the overall probability of a noisy label is constrained to be at most $\eta$, we also give a polynomial-time algorithm for learning linear separators in $\mathfrak{R}$d under isotropic log-concave distributions that can handle a noise rate of $\eta$ = $\Omega$($\epsilon$). In the case of uniform distribution, this improves over the results of Kalai et al. [2005], which either required runtime super-exponential in 1/$\epsilon$ (ours is polynomial in 1/$\epsilon$) or tolerated less noise.1 Our algorithms are also efficient in the active learning setting, where learning algorithms only receive the classifications of examples when they ask for them. We show that, in this model, our algorithms achieve a label complexity whose dependence on the error parameter $\epsilon$ is polylogarithmic (and thus exponentially better than that of any passive algorithm). This provides the first polynomial-time active learning algorithm for learning linear separators in the presence of malicious noise or adversarial label noise. Our algorithms and analysis combine several ingredients including aggressive localization, minimization of a progressively rescaled hinge loss, and a novel localized and soft outlier removal procedure. We use localization techniques (previously used for obtaining better sample complexity results) to obtain better noise-tolerant polynomial-time algorithms.},
  number = {6},
  journal = {J. ACM},
  author = {Awasthi, Pranjal and Balcan, Maria Florina and Long, Philip M.},
  month = jan,
  year = {2017},
  keywords = {active learning,agnostic learning,Learning theory,linear classification,localization,malicious noise,noise-tolerant learning},
  pages = {50:1--50:27},
  file = {/Users/billlai/Zotero/storage/XYANLGSB/Awasthi et al. - 2017 - The Power of Localization for Efficiently Learning.pdf}
}

@article{RazborovNewKindTradeoffs2016,
  title = {A {{New Kind}} of {{Tradeoffs}} in {{Propositional Proof Complexity}}},
  volume = {63},
  issn = {0004-5411},
  doi = {10.1145/2858790},
  abstract = {We exhibit an unusually strong tradeoff in propositional proof complexity that significantly deviates from the established pattern of almost all results of this kind. Namely, restrictions on one resource (width, in our case) imply an increase in another resource (tree-like size) that is exponential not only with respect to the complexity of the original problem, but also to the whole class of all problems of the same bit size. More specifically, we show that for any parameter k = k(n), there are unsatisfiable k-CNFs that possess refutations of width O(k), but such that any tree-like refutation of width n1 - $\epsilon$/k must necessarily have doubly exponential size exp\,(n$\Omega$(k)). This means that there exist contradictions that allow narrow refutations, but in order to keep the size of such a refutation even within a single exponent, it must necessarily use a high degree of parallelism. Our construction and proof methods combine, in a non-trivial way, two previously known techniques: the hardness escalation method based on substitution formulas and expansion. This combination results in a hardness compression approach that strives to preserve hardness of a contradiction while significantly decreasing the number of its variables.},
  number = {2},
  journal = {J. ACM},
  author = {Razborov, Alexander},
  month = apr,
  year = {2016},
  keywords = {resolution,Hardness compression,tradeoff},
  pages = {16:1--16:14},
  file = {/Users/billlai/Zotero/storage/67EDQ2LB/Razborov - 2016 - A New Kind of Tradeoffs in Propositional Proof Com.pdf}
}

@article{HaeuplerSimpleFastDeterministic2015,
  title = {Simple, {{Fast}} and {{Deterministic Gossip}} and {{Rumor Spreading}}},
  volume = {62},
  issn = {0004-5411},
  doi = {10.1145/2767126},
  abstract = {We study gossip algorithms for the rumor spreading problem, which asks each node to deliver a rumor to all nodes in an unknown network. Gossip algorithms allow nodes only to call one neighbor per round and have recently attracted attention as message efficient, simple, and robust solutions to the rumor spreading problem. A long series of papers analyzed the performance of uniform random gossip in which nodes repeatedly call a random neighbor to exchange all rumors with. A main result of this investigation was that uniform gossip completes in O(log n/$\Phi$) rounds where $\Phi$ is the conductance of the network. Nonuniform random gossip schemes were devised to allow efficient rumor spreading in networks with bottlenecks. In particular, [Censor-Hillel et al., STOC\&12] gave an O(log3 n) algorithm to solve the 1-local broadcast problem in which each node wants to exchange rumors locally with its 1-neighborhood. By repeatedly applying this protocol, one can solve the global rumor spreading quickly for all networks with small diameter, independently of the conductance. All these algorithms are inherently randomized in their design and analysis. A parallel research direction has been to reduce and determine the amount of randomness needed for efficient rumor spreading. This has been done via lower bounds for restricted models and by designing gossip algorithms with a reduced need for randomness, for instance, by using pseudorandom generators with short random seeds. The general intuition and consensus of these results has been that randomization plays a important role in effectively spreading rumors and that at least a polylogarithmic number of random bit are crucially needed. In this article improves over the state of the art in several ways by presenting a deterministic gossip algorithm that solves the the k-local broadcast problem in 2(k + log2 n) log2 n rounds. Besides being the first efficient deterministic solution to the rumor spreading problem this algorithm is interesting in many aspects: It is simpler, more natural, more robust, and faster than its randomized pendant and guarantees success with certainty instead of with high probability. Its analysis is furthermore simple, self-contained, and fundamentally different from prior works.},
  number = {6},
  journal = {J. ACM},
  author = {Haeupler, Bernhard},
  month = dec,
  year = {2015},
  keywords = {distributed computing,Gossip,information dissemination,LOCAL model,rumor spreading},
  pages = {47:1--47:18},
  file = {/Users/billlai/Zotero/storage/W495U6ZE/Haeupler - 2015 - Simple, Fast and Deterministic Gossip and Rumor Sp.pdf}
}

@article{Bodlaenderpartialkarboretumgraphs1998,
  title = {A Partial K-Arboretum of Graphs with Bounded Treewidth},
  volume = {209},
  issn = {03043975},
  doi = {10.1016/S0304-3975(97)00228-4},
  language = {en},
  number = {1-2},
  journal = {Theoretical Computer Science},
  author = {Bodlaender, Hans L.},
  month = dec,
  year = {1998},
  pages = {1-45},
  file = {/Users/billlai/Zotero/storage/S4B9L4S8/Bodlaender - 1998 - A partial k-arboretum of graphs with bounded treew.pdf}
}

@article{OtachiReductionTechniquesGraph2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1403.7238},
  primaryClass = {cs, math},
  title = {Reduction {{Techniques}} for {{Graph Isomorphism}} in the {{Context}} of {{Width Parameters}}},
  abstract = {We study the parameterized complexity of the graph isomorphism problem when parameterized by width parameters related to tree decompositions. We apply the following technique to obtain fixed-parameter tractability for such parameters. We first compute an isomorphism invariant set of potential bags for a decomposition and then apply a restricted version of the Weisfeiler-Lehman algorithm to solve isomorphism. With this we show fixed-parameter tractability for several parameters and provide a unified explanation for various isomorphism results concerned with parameters related to tree decompositions. As a possibly first step towards intractability results for parameterized graph isomorphism we develop an fpt Turing-reduction from strong tree width to the a priori unrelated parameter maximum degree.},
  journal = {arXiv:1403.7238 [cs, math]},
  author = {Otachi, Yota and Schweitzer, Pascal},
  month = mar,
  year = {2014},
  keywords = {Computer Science - Discrete Mathematics,Mathematics - Combinatorics,05C60; 05C85; 68R10;},
  file = {/Users/billlai/Zotero/storage/8HX3HQSI/Otachi and Schweitzer - 2014 - Reduction Techniques for Graph Isomorphism in the .pdf;/Users/billlai/Zotero/storage/YDLVZXX9/1403.html}
}

@article{BodlaenderPolynomialalgorithmsGraph,
  title = {Polynomial Algorithms for {{Graph Isomorphism}} and {{Chromatic Index}} on Partial K-Trees},
  abstract = {In this paper we show that GRAPH ISOMORPHISMand CHROMATICINDEXare solvable in polynomial time when restricted to the class of graphs with treewidth $<$ k (k a constant) (or equivalently, the class of partial k-trees). Also, we show that there exist algorithms that find tree-decompositionswith treewidth $<$ k of graphs with treewidth $<$ k, in O(n 3) time, (k constant).},
  language = {en},
  author = {Bodlaender, Hans L},
  pages = {10},
  file = {/Users/billlai/Zotero/storage/LV9D5MLW/Bodlaender - Polynomial algorithms for Graph Isomorphism and Ch.pdf}
}

@article{LokshtanovFixedparametertractablecanonization2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1404.0818},
  primaryClass = {cs},
  title = {Fixed-Parameter Tractable Canonization and Isomorphism Test for Graphs of Bounded Treewidth},
  abstract = {We give a fixed-parameter tractable algorithm that, given a parameter \$k\$ and two graphs \$G\_1,G\_2\$, either concludes that one of these graphs has treewidth at least \$k\$, or determines whether \$G\_1\$ and \$G\_2\$ are isomorphic. The running time of the algorithm on an \$n\$-vertex graph is \$2\^\{O(k\^5$\backslash$log k)\}$\backslash$cdot n\^5\$, and this is the first fixed-parameter algorithm for Graph Isomorphism parameterized by treewidth. Our algorithm in fact solves the more general canonization problem. We namely design a procedure working in \$2\^\{O(k\^5$\backslash$log k)\}$\backslash$cdot n\^5\$ time that, for a given graph \$G\$ on \$n\$ vertices, either concludes that the treewidth of \$G\$ is at least \$k\$, or: * finds in an isomorphic-invariant way a graph \$$\backslash$mathfrak\{c\}(G)\$ that is isomorphic to \$G\$; * finds an isomorphism-invariant construction term --- an algebraic expression that encodes \$G\$ together with a tree decomposition of \$G\$ of width \$O(k\^4)\$. Hence, the isomorphism test reduces to verifying whether the computed isomorphic copies or the construction terms for \$G\_1\$ and \$G\_2\$ are equal.},
  journal = {arXiv:1404.0818 [cs]},
  author = {Lokshtanov, Daniel and Pilipczuk, Marcin and Pilipczuk, Micha\l{} and Saurabh, Saket},
  month = apr,
  year = {2014},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Computational Complexity},
  file = {/Users/billlai/Zotero/storage/FSVB3I9L/Lokshtanov et al. - 2014 - Fixed-parameter tractable canonization and isomorp.pdf;/Users/billlai/Zotero/storage/N86P8UEC/1404.html}
}

@article{SanthanamCircuitLowerBounds2009,
  title = {Circuit {{Lower Bounds}} for {{Merlin}}\textendash{{Arthur Classes}}},
  volume = {39},
  issn = {0097-5397},
  doi = {10.1137/070702680},
  abstract = {We show that for each \$k$>$0\$, \$$\backslash$mathsf\{MA\}/1\$ (\$$\backslash$mathsf\{MA\}\$ with 1 bit of advice) does not have circuits of size \$n\^k\$. This implies the first superlinear circuit lower bounds for the promise versions of the classes \$$\backslash$mathsf\{MA\}\$, \$$\backslash$mathsf\{AM\}\$, and \$$\backslash$mathsf\{ZPP\}\_\{$\backslash$parallel\}\^\{$\backslash$mathsf\{NP\}\}\$. We extend our main result in several ways. For each k, we give an explicit language in \$($\backslash$mathsf\{MA\}$\backslash$cap$\backslash$mathsf\{coMA\})/1\$ which does not have circuits of size \$n\^k\$. We also adapt our lower bound to the average-case setting; i.e., we show that \$$\backslash$mathsf\{MA\}/1\$ cannot be solved on more than \$1/2+1/n\^k\$ fraction of inputs of length n by circuits of size \$n\^k\$. Furthermore, we prove that \$$\backslash$mathsf\{MA\}\$ does not have arithmetic circuits of size \$n\^k\$ for any k. As a corollary to our main result, we obtain that derandomization of \$$\backslash$mathsf\{MA\}/O(1)\$ implies the existence of pseudorandom generators computable using \$O(1)\$ bits of advice.},
  number = {3},
  journal = {SIAM Journal on Computing},
  author = {Santhanam, R.},
  month = jan,
  year = {2009},
  pages = {1038-1061},
  file = {/Users/billlai/Zotero/storage/T7EIJBQN/Santhanam - 2009 - Circuit Lower Bounds for Merlin–Arthur Classes.pdf;/Users/billlai/Zotero/storage/JYZF9CGE/070702680.html}
}

@book{Mas-ColellMicroeconomictheory1995,
  title = {Microeconomic Theory},
  volume = {1},
  publisher = {{Oxford university press New York}},
  author = {{Mas-Colell}, Andreu and Whinston, Michael Dennis and Green, Jerry R.},
  year = {1995},
  file = {/Users/billlai/Zotero/storage/VUBSURWU/Mas-Colell et al. - 1995 - Microeconomic theory.pdf;/Users/billlai/Zotero/storage/XYVW6HD2/Mas-Colell et al. - 1995 - Microeconomic theory.pdf}
}

@techreport{KlempererAuctionsTheoryPractice2004,
  title = {Auctions: {{Theory}} and {{Practice}}},
  shorttitle = {Auctions},
  abstract = {This book is a non-technical introduction to auction theory; its practical application in auction design (including many examples); and its uses in other parts of economics. It can be used for a graduate course on auction theory, or \textendash{} by picking selectively \textendash{} an advanced undergraduate or MBA course on auctions and auction design. Part A introduces the basic theory. Part B shows how modern auction-theoretic tools illuminate a range of mainstream economic questions that are superficially unconnected with auctions. Part C discusses practical auction design. Part D describes the one-hundred-billion dollar 3G mobile-phone license auctions. None of the writing is technical, except in the Appendices. The material was presented as the inaugural (2003) Toulouse Lectures in Economics and is forthcoming at Princeton University Press. This document contains the Contents, Preface and Introduction to the book. A draft of the FULL BOOK is available at http://www.paulklemperer.org.},
  language = {en},
  number = {2004-W09},
  institution = {{Economics Group, Nuffield College, University of Oxford}},
  author = {Klemperer, Paul},
  month = mar,
  year = {2004},
  keywords = {3G,Auction Theory,Auctions,Bidding,Mechanism Design,Spectrum Auctions,Telecommunications,UMTS},
  file = {/Users/billlai/Zotero/storage/SMNYLPGF/Klemperer - 2004 - Auctions Theory and Practice.pdf;/Users/billlai/Zotero/storage/SQ4G9WBB/049.html}
}

@article{MilgromRationalexpectationsinformation1981,
  title = {Rational Expectations, Information Acquisition, and Competitive Bidding},
  journal = {Econometrica: Journal of the Econometric Society},
  author = {Milgrom, Paul R.},
  year = {1981},
  pages = {921--943},
  file = {/Users/billlai/Zotero/storage/2YRA4G87/Milgrom - 1981 - Rational expectations, information acquisition, an.pdf;/Users/billlai/Zotero/storage/HG8QPF5C/1912511.html}
}

@article{MaskintheoryimplementationNash1985,
  title = {The Theory of Implementation in {{Nash}} Equilibrium: {{A}} Survey},
  shorttitle = {The Theory of Implementation in {{Nash}} Equilibrium},
  journal = {Social goals and social organization},
  author = {Maskin, Eric},
  year = {1985},
  pages = {173--204},
  file = {/Users/billlai/Zotero/storage/HKHU8XE3/Maskin - 1985 - The theory of implementation in Nash equilibrium .pdf;/Users/billlai/Zotero/storage/9BPLRZS9/books.html}
}

@inproceedings{Lavicharacterizationtruthfulcombinatorial2003,
  title = {Towards a Characterization of Truthful Combinatorial Auctions},
  booktitle = {Foundations of {{Computer Science}}, 2003. {{Proceedings}}. 44th {{Annual IEEE Symposium}} On},
  publisher = {{IEEE}},
  author = {Lavi, Ron and Mu'Alem, Ahuva and Nisan, Noam},
  year = {2003},
  pages = {574--583},
  file = {/Users/billlai/Zotero/storage/XDQ4ZG98/Lavi et al. - 2003 - Towards a characterization of truthful combinatori.pdf;/Users/billlai/Zotero/storage/7MBQGIGC/1238230.html}
}

@article{HarsanyiGamesincompleteinformation1967,
  title = {Games with Incomplete Information Played by ``{{Bayesian}}'' Players, {{I}}\textendash{{III Part I}}. {{The}} Basic Model},
  volume = {14},
  number = {3},
  journal = {Management science},
  author = {Harsanyi, John C.},
  year = {1967},
  pages = {159--182},
  file = {/Users/billlai/Zotero/storage/37NIRENP/Harsanyi - 1967 - Games with incomplete information played by “Bayes.pdf;/Users/billlai/Zotero/storage/ZV5N55NX/mnsc.14.3.html}
}

@article{BikhchandaniWeakmonotonicitycharacterizes2006,
  title = {Weak Monotonicity Characterizes Deterministic Dominant-Strategy Implementation},
  volume = {74},
  number = {4},
  journal = {Econometrica},
  author = {Bikhchandani, Sushil and Chatterji, Shurojit and Lavi, Ron and Mu'alem, Ahuva and Nisan, Noam and Sen, Arunava},
  year = {2006},
  pages = {1109--1132},
  file = {/Users/billlai/Zotero/storage/CFHS5Q3V/Bikhchandani et al. - 2006 - Weak monotonicity characterizes deterministic domi.pdf}
}

@article{Robertscharacterizationimplementablechoice1979,
  title = {The Characterization of Implementable Choice Rules},
  volume = {12},
  number = {2},
  journal = {Aggregation and revelation of preferences},
  author = {Roberts, Kevin},
  year = {1979},
  pages = {321--348}
}

@article{MooreSubgameperfectimplementation1988,
  title = {Subgame Perfect Implementation},
  journal = {Econometrica: Journal of the Econometric Society},
  author = {Moore, John and Repullo, Rafael},
  year = {1988},
  pages = {1191--1220},
  file = {/Users/billlai/Zotero/storage/BY4GRRTU/Moore and Repullo - 1988 - Subgame perfect implementation.pdf;/Users/billlai/Zotero/storage/3DCQ8CKE/1911364.html}
}

@article{HoltJrCompetitivebiddingcontracts1980,
  title = {Competitive Bidding for Contracts under Alternative Auction Procedures},
  volume = {88},
  number = {3},
  journal = {Journal of political Economy},
  author = {Holt Jr, Charles A.},
  year = {1980},
  pages = {433--445},
  file = {/Users/billlai/Zotero/storage/VU89FQPJ/260878.html}
}

@article{Wilsonbiddingmodelperfect1977,
  title = {A Bidding Model of Perfect Competition},
  journal = {The Review of Economic Studies},
  author = {Wilson, Robert},
  year = {1977},
  pages = {511--518},
  file = {/Users/billlai/Zotero/storage/RRTXGE93/Wilson - 1977 - A bidding model of perfect competition.pdf;/Users/billlai/Zotero/storage/BKP6GB27/2296904.html}
}

@article{Milgromtheoryauctionscompetitive1982,
  title = {A Theory of Auctions and Competitive Bidding},
  journal = {Econometrica: Journal of the Econometric Society},
  author = {Milgrom, Paul R. and Weber, Robert J.},
  year = {1982},
  pages = {1089--1122},
  file = {/Users/billlai/Zotero/storage/VJCC7JTY/Milgrom and Weber - 1982 - A theory of auctions and competitive bidding.pdf;/Users/billlai/Zotero/storage/LTL85Q24/1911865.html}
}

@article{LehmannTruthrevelationapproximately2002,
  title = {Truth Revelation in Approximately Efficient Combinatorial Auctions},
  volume = {49},
  number = {5},
  journal = {Journal of the ACM (JACM)},
  author = {Lehmann, Daniel and O\'callaghan, Liadan Ita and Shoham, Yoav},
  year = {2002},
  pages = {577--602},
  file = {/Users/billlai/Zotero/storage/VCKNFZXB/citation.html}
}

@article{GreenCharacterizationsatisfactorymechanisms1977,
  title = {Characterization of Satisfactory Mechanisms for the Revelation of Preferences for Public Goods},
  journal = {Econometrica: Journal of the Econometric Society},
  author = {Green, Jerry and Laffont, Jean-Jacques},
  year = {1977},
  pages = {427--438},
  file = {/Users/billlai/Zotero/storage/TPBTR423/Green and Laffont - 1977 - Characterization of satisfactory mechanisms for th.pdf;/Users/billlai/Zotero/storage/4IKZD8T6/1911219.html}
}

@inproceedings{SaksWeakmonotonicitysuffices2005,
  title = {Weak Monotonicity Suffices for Truthfulness on Convex Domains},
  booktitle = {Proceedings of the 6th {{ACM}} Conference on {{Electronic}} Commerce},
  publisher = {{ACM}},
  author = {Saks, Michael and Yu, Lan},
  year = {2005},
  pages = {286--293},
  file = {/Users/billlai/Zotero/storage/DV4KUDE3/Saks and Yu - 2005 - Weak monotonicity suffices for truthfulness on con.pdf;/Users/billlai/Zotero/storage/8Q7KH6XY/citation.html}
}

@article{MyersonOptimalauctiondesign1981,
  title = {Optimal Auction Design},
  volume = {6},
  number = {1},
  journal = {Mathematics of operations research},
  author = {Myerson, Roger B.},
  year = {1981},
  pages = {58--73},
  file = {/Users/billlai/Zotero/storage/GPB3UCP6/Myerson - 1981 - Optimal auction design.pdf;/Users/billlai/Zotero/storage/78JTDNWP/moor.6.1.html}
}

@article{Dasguptaimplementationsocialchoice1979,
  title = {The Implementation of Social Choice Rules: {{Some}} General Results on Incentive Compatibility},
  volume = {46},
  shorttitle = {The Implementation of Social Choice Rules},
  number = {2},
  journal = {The Review of Economic Studies},
  author = {Dasgupta, Partha and Hammond, Peter and Maskin, Eric},
  year = {1979},
  pages = {185--216},
  file = {/Users/billlai/Zotero/storage/HVFWKQCK/Dasgupta et al. - 1979 - The implementation of social choice rules Some ge.pdf;/Users/billlai/Zotero/storage/WYLPA7UW/2297045.html}
}

@article{MyersonEfficientmechanismsbilateral1983,
  title = {Efficient Mechanisms for Bilateral Trading},
  volume = {29},
  number = {2},
  journal = {Journal of economic theory},
  author = {Myerson, Roger B. and Satterthwaite, Mark A.},
  year = {1983},
  pages = {265--281},
  file = {/Users/billlai/Zotero/storage/ACLIRHBX/Myerson and Satterthwaite - 1983 - Efficient mechanisms for bilateral trading.pdf;/Users/billlai/Zotero/storage/U989VMI7/0022053183900480.html}
}

@article{Bartholdicomputationaldifficultymanipulating1989,
  title = {The Computational Difficulty of Manipulating an Election},
  volume = {6},
  number = {3},
  journal = {Social Choice and Welfare},
  author = {Bartholdi, John J. and Tovey, Craig A. and Trick, Michael A.},
  year = {1989},
  pages = {227--241},
  file = {/Users/billlai/Zotero/storage/STQDZRZ6/Bartholdi et al. - 1989 - The computational difficulty of manipulating an el.pdf;/Users/billlai/Zotero/storage/22Z9KZV2/BF00295861.html}
}

@article{GrovesIncentivesteams1973,
  title = {Incentives in Teams},
  journal = {Econometrica: Journal of the Econometric Society},
  author = {Groves, Theodore},
  year = {1973},
  pages = {617--631},
  file = {/Users/billlai/Zotero/storage/CBCER8CZ/Groves - 1973 - Incentives in teams.pdf;/Users/billlai/Zotero/storage/V6IPUNVX/1914085.html}
}

@article{VickreyCounterspeculationauctionscompetitive1961,
  title = {Counterspeculation, Auctions, and Competitive Sealed Tenders},
  volume = {16},
  number = {1},
  journal = {The Journal of finance},
  author = {Vickrey, William},
  year = {1961},
  pages = {8--37}
}

@article{ClarkeMultipartpricingpublic1971,
  title = {Multipart Pricing of Public Goods},
  volume = {11},
  number = {1},
  journal = {Public choice},
  author = {Clarke, Edward H.},
  year = {1971},
  pages = {17--33}
}

@article{SatterthwaiteStrategyproofnessArrowconditions1975,
  title = {Strategy-Proofness and {{Arrow}}'s Conditions: {{Existence}} and Correspondence Theorems for Voting Procedures and Social Welfare Functions},
  volume = {10},
  shorttitle = {Strategy-Proofness and {{Arrow}}'s Conditions},
  number = {2},
  journal = {Journal of economic theory},
  author = {Satterthwaite, Mark Allen},
  year = {1975},
  pages = {187--217},
  file = {/Users/billlai/Zotero/storage/CL47AQC3/Satterthwaite - 1975 - Strategy-proofness and Arrow's conditions Existen.pdf}
}

@book{ArrowSocialchoiceindividual1951,
  title = {Social Choice and Individual Values},
  publisher = {{Yale University Press}},
  author = {Arrow, Kenneth J.},
  year = {1951},
  file = {/Users/billlai/Zotero/storage/XAXRS4E8/books.html}
}

@article{GeanakoplosThreebriefproofs2005,
  title = {Three Brief Proofs of {{Arrow}}'s Impossibility Theorem},
  volume = {26},
  number = {1},
  journal = {Economic Theory},
  author = {Geanakoplos, John},
  year = {2005},
  pages = {211--215},
  file = {/Users/billlai/Zotero/storage/GPM4HP7D/Geanakoplos - 2005 - Three brief proofs of Arrow’s impossibility theore.pdf;/Users/billlai/Zotero/storage/QM36JJPS/s00199-004-0556-7.html}
}

@article{NisanAlgorithmicmechanismdesign2001,
  title = {Algorithmic Mechanism Design},
  volume = {35},
  number = {1-2},
  journal = {Games and Economic Behavior},
  author = {Nisan, Noam and Ronen, Amir},
  year = {2001},
  pages = {166--196},
  file = {/Users/billlai/Zotero/storage/2BIKHUFL/Nisan and Ronen - 2001 - Algorithmic mechanism design.pdf;/Users/billlai/Zotero/storage/WSZ5IUS9/S089982569990790X.html}
}

@article{GibbardManipulationvotingschemes1973,
  title = {Manipulation of Voting Schemes: A General Result},
  shorttitle = {Manipulation of Voting Schemes},
  journal = {Econometrica: journal of the Econometric Society},
  author = {Gibbard, Allan},
  year = {1973},
  pages = {587--601},
  file = {/Users/billlai/Zotero/storage/94DP98U6/Gibbard - 1973 - Manipulation of voting schemes a general result.pdf;/Users/billlai/Zotero/storage/I9U85KSG/1914083.html}
}

@book{KrishnaAuctiontheory2009,
  title = {Auction Theory},
  publisher = {{Academic press}},
  author = {Krishna, Vijay},
  year = {2009},
  file = {/Users/billlai/Zotero/storage/RM8QYECR/books.html}
}

@article{ArrowIndividualvaluessocial1951,
  title = {Individual Values and Social Choice},
  author = {Arrow, Kenneth},
  year = {1951}
}

@article{AgrawalFurtherOptimalRegret2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1209.3353},
  primaryClass = {cs, stat},
  title = {Further {{Optimal Regret Bounds}} for {{Thompson Sampling}}},
  abstract = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. In this paper, we provide a novel regret analysis for Thompson Sampling that simultaneously proves both the optimal problem-dependent bound of \$(1+$\backslash$epsilon)$\backslash$sum\_i $\backslash$frac\{$\backslash$ln T\}\{$\backslash$Delta\_i\}+O($\backslash$frac\{N\}\{$\backslash$epsilon\^2\})\$ and the first near-optimal problem-independent bound of \$O($\backslash$sqrt\{NT$\backslash$ln T\})\$ on the expected regret of this algorithm. Our near-optimal problem-independent bound solves a COLT 2012 open problem of Chapelle and Li. The optimal problem-dependent regret bound for this problem was first proven recently by Kaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques are conceptually simple, easily extend to distributions other than the Beta distribution, and also extend to the more general contextual bandits setting [Manuscript, Agrawal and Goyal, 2012].},
  journal = {arXiv:1209.3353 [cs, stat]},
  author = {Agrawal, Shipra and Goyal, Navin},
  month = sep,
  year = {2012},
  keywords = {Computer Science - Data Structures and Algorithms,68W40; 68Q25,Computer Science - Learning,F.2.0,Statistics - Machine Learning},
  file = {/Users/billlai/Zotero/storage/G2MGU7DF/Agrawal and Goyal - 2012 - Further Optimal Regret Bounds for Thompson Samplin.pdf;/Users/billlai/Zotero/storage/64HERMID/1209.html}
}

@article{AuerNonstochasticMultiarmedBandit2002,
  title = {The {{Nonstochastic Multiarmed Bandit Problem}}},
  volume = {32},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/S0097539701398375},
  abstract = {In the multiarmed bandit problem, a gambler must decide which arm of K nonidentical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines.},
  language = {en},
  number = {1},
  journal = {SIAM Journal on Computing},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicol\`o and Freund, Yoav and Schapire, Robert E.},
  month = jan,
  year = {2002},
  pages = {48-77},
  file = {/Users/billlai/Zotero/storage/MWMVYKN9/Auer et al. - 2002 - The Nonstochastic Multiarmed Bandit Problem.pdf}
}

@article{FlajoletHyperLogLoganalysisnearoptimal,
  title = {{{HyperLogLog}}: The Analysis of a near-Optimal Cardinality Estimation Algorithm},
  language = {en},
  author = {Flajolet, Philippe and Fusy, \'Eric and Gandouet, Olivier},
  pages = {20},
  file = {/Users/billlai/Zotero/storage/TFKJ37R5/Flajolet et al. - HyperLogLog the analysis of a near-optimal cardin.pdf}
}

@article{FlajoletUnderstandingHyperLogLogNearOptimal,
  title = {Understanding the {{HyperLogLog}}: A {{Near}}-{{Optimal Cardinality Estimation Algorithm}}},
  abstract = {The HyperLogLog algorithm (HLL) is a method to estimate the number of distinct elements in large datasets i.e. cardinality, in a single pass, and using a very small amount of memory. The HLL algorithm is an optimization of the method presented in 2003 by Durand and Flajolet in the paper LogLog Counting of Large Cardinalities. In this report we analyze the so called cornerstone of Big Data infrastructures, give a detailed description of the algorithm and explain the mathematical intuition behind it.},
  language = {en},
  author = {Flajolet, Philippe and Fusy, Eric and Gandouet, Olivier and Meunier, Frederic},
  pages = {14},
  file = {/Users/billlai/Zotero/storage/44BGHCNG/Flajolet et al. - Understanding the HyperLogLog a Near-Optimal Card.pdf}
}

@inproceedings{HeuleHyperLogLogpracticealgorithmic2013,
  title = {{{HyperLogLog}} in Practice: Algorithmic Engineering of a State of the Art Cardinality Estimation Algorithm},
  isbn = {978-1-4503-1597-5},
  shorttitle = {{{HyperLogLog}} in Practice},
  doi = {10.1145/2452376.2452456},
  abstract = {Cardinality estimation has a wide range of applications and is of particular importance in database systems. Various algorithms have been proposed in the past, and the HyperLogLog algorithm is one of them. In this paper, we present a series of improvements to this algorithm that reduce its memory requirements and significantly increase its accuracy for an important range of cardinalities. We have implemented our proposed algorithm for a system at Google and evaluated it empirically, comparing it to the original HyperLogLog algorithm. Like HyperLogLog, our improved algorithm parallelizes perfectly and computes the cardinality estimate in a single pass.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Heule, Stefan and Nunkesser, Marc and Hall, Alexander},
  year = {2013},
  pages = {683},
  file = {/Users/billlai/Zotero/storage/2RI9BY3J/Heule et al. - 2013 - HyperLogLog in practice algorithmic engineering o.pdf}
}

@article{KejariwalRealtimeanalytics2015,
  title = {Real Time Analytics: Algorithms and Systems},
  volume = {8},
  issn = {21508097},
  shorttitle = {Real Time Analytics},
  doi = {10.14778/2824032.2824132},
  abstract = {Velocity is one of the 4 Vs commonly used to characterize Big Data [5]. In this regard, Forrester remarked the following in Q3 2014 [8]: ``The high velocity, white-water flow of data from innumerable real-time data sources such as market data, Internet of Things, mobile, sensors, clickstream, and even transactions remain largely unnavigated by most firms. The opportunity to leverage streaming analytics has never been greater.'' Example use cases of streaming analytics include, but not limited to: (a) visualization of business metrics in real-time (b) facilitating highly personalized experiences (c) expediting response during emergencies. Streaming analytics is extensively used in a wide variety of domains such as healthcare, e-commerce, financial services, telecommunications, energy and utilities, manufacturing, government and transportation.},
  language = {en},
  number = {12},
  journal = {Proceedings of the VLDB Endowment},
  author = {Kejariwal, Arun and Kulkarni, Sanjeev and Ramasamy, Karthik},
  month = aug,
  year = {2015},
  pages = {2040-2041},
  file = {/Users/billlai/Zotero/storage/GVHX3S7U/Kejariwal et al. - 2015 - Real time analytics algorithms and systems.pdf}
}

@inproceedings{TingStreamedapproximatecounting2014,
  title = {Streamed Approximate Counting of Distinct Elements: Beating Optimal Batch Methods},
  isbn = {978-1-4503-2956-9},
  shorttitle = {Streamed Approximate Counting of Distinct Elements},
  doi = {10.1145/2623330.2623669},
  abstract = {Counting the number of distinct elements in a large dataset is a common task in web applications and databases. This problem is difficult in limited memory settings where storing a large hash table table is intractable. This paper advances the state of the art in probabilistic methods for estimating the number of distinct elements in a streaming setting New streaming algorithms are given that provably beat the ''optimal'' errors for Min-count and HyperLogLog while using the same sketch.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Ting, Daniel},
  year = {2014},
  pages = {442-451},
  file = {/Users/billlai/Zotero/storage/LVUIBJ9M/Ting - 2014 - Streamed approximate counting of distinct elements.pdf}
}

@unpublished{ChabchoubSlidingHyperLogLogEstimating2010,
  title = {Sliding {{HyperLogLog}}: {{Estimating}} Cardinality in a Data Stream},
  shorttitle = {Sliding {{HyperLogLog}}},
  abstract = {In this paper, a new algorithm estimating the number of active flows in a data stream is proposed. This algorithm adapts the HyperLogLog algorithm of Flajolet et al to the data stream processing by adding a sliding window mechanism. It has the advantage to estimate at any time the number of flows seen over any duration bounded by the length of the sliding window. The estimate is very accurate with a standard error of about 1.04/$\backslash$sqrt\{m\} (the same as in HyperLogLog algorithm). As the new algorithm answers more flexible queries, it needs an additional memory storage compared to HyerLogLog algorithm. It is proved that this additional memory is at most equal to 5mln(n/m) bytes, where n is the real number of flows in the sliding window. For instance, with an additional memory of only 35kB, a standard error of about 3\% can be achieved for a data stream of several million flows. Theoretical results are validated on both real and synthetic traffic.},
  author = {Chabchoub, Yousra and H\'ebrail, Georges},
  month = mar,
  year = {2010},
  file = {/Users/billlai/Zotero/storage/YUTYQ3DX/Chabchoub and Hébrail - 2010 - Sliding HyperLogLog Estimating cardinality in a d.pdf}
}

@article{ErtlNewCardinalityEstimation2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.07290},
  primaryClass = {cs},
  title = {New {{Cardinality Estimation Methods}} for {{HyperLogLog Sketches}}},
  abstract = {This work presents new cardinality estimation methods for data sets recorded by HyperLogLog sketches. A simple derivation of the original estimator was found, that also gives insight how to correct its deficiencies. The result is an improved estimator that is unbiased over the full cardinality range, is easy computable, and does not rely on empirically determined data as previous approaches. Based on the maximum likelihood principle a second unbiased estimation method is presented which can also be extended to estimate cardinalities of union, intersection, or relative complements of two sets that are both represented as HyperLogLog sketches. Experimental results show that this approach is more precise than the conventional technique using the inclusion-exclusion principle.},
  journal = {arXiv:1706.07290 [cs]},
  author = {Ertl, Otmar},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/billlai/Zotero/storage/YTTIUCN7/Ertl - 2017 - New Cardinality Estimation Methods for HyperLogLog.pdf;/Users/billlai/Zotero/storage/2T72WBXR/1706.html}
}

@inproceedings{KarpRandomizedrumorspreading2000,
  title = {Randomized Rumor Spreading},
  isbn = {978-0-7695-0850-4},
  doi = {10.1109/SFCS.2000.892324},
  language = {en},
  publisher = {{IEEE Comput. Soc}},
  author = {Karp, R. and Schindelhauer, C. and Shenker, S. and Vocking, B.},
  year = {2000},
  pages = {565-574},
  file = {/Users/billlai/Zotero/storage/4VSZ92SN/Karp et al. - 2000 - Randomized rumor spreading.pdf}
}

@article{DoerrRandomizedRumorSpreading2017,
  title = {Randomized {{Rumor Spreading Revisited}}},
  doi = {10.4230/lipics.icalp.2017.138},
  abstract = {We develop a simple and generic method to analyze randomized rumor spreading processes in fully connected networks. In contrast to all previous works, which heavily exploit the precise definition of the process under investigation, we only need to understand the probability and the covariance of the events that uninformed nodes become informed. This universality allows us to easily analyze the classic push, pull, and push-pull protocols both in their pure version and in several variations such as messages failing with constant probability or nodes calling a random number of others each round. Some dynamic models can be analyzed as well, e.g., when the network is a G(n, p) random graph sampled independently each round [Clementi et al. (ESA 2013)].},
  language = {en},
  author = {Doerr, Benjamin and Kostrygin, Anatolii},
  editor = {Herbstritt, Marc},
  year = {2017},
  file = {/Users/billlai/Zotero/storage/7ME5N948/Doerr and Kostrygin - 2017 - Randomized Rumor Spreading Revisited.pdf}
}

@incollection{AvinFasterRumorSpreading2013,
  series = {Lecture Notes in Computer Science},
  title = {Faster {{Rumor Spreading}}: {{Breaking}} the Logn {{Barrier}}},
  isbn = {978-3-642-41526-5 978-3-642-41527-2},
  shorttitle = {Faster {{Rumor Spreading}}},
  abstract = {O(logn) rounds has been a well known upper bound for rumor spreading using push\&pull in the random phone call model (i.e., uniform gossip in the complete graph). A matching lower bound of $\Omega$(logn) is also known for this special case. Under the assumptions of this model and with a natural addition that nodes can call a partner once they learn its address (e.g., its IP address) we present a new distributed, address-oblivious and robust algorithm that uses push\&pull with pointer jumping to spread a rumor to all nodes in only O(logn-----$\surd$)O(log⁡n)O($\backslash$sqrt\{$\backslash$log n\}) rounds, w.h.p. This algorithm can also cope with F=o(n/2logn$\surd$)F=o(n/2log⁡n)F= o(n/2\^\{$\backslash$sqrt\{$\backslash$log n\}\}) node failures, in which case all but O(F) nodes become informed within O(logn-----$\surd$)O(log⁡n)O($\backslash$sqrt\{$\backslash$log n\}) rounds, w.h.p.},
  language = {en},
  booktitle = {Distributed {{Computing}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Avin, Chen and Els\"asser, Robert},
  month = oct,
  year = {2013},
  pages = {209-223},
  file = {/Users/billlai/Zotero/storage/VIBE5D6R/Avin and Elsässer - 2013 - Faster Rumor Spreading Breaking the logEmphasis .pdf;/Users/billlai/Zotero/storage/YH7ZDLXH/978-3-642-41527-2_15.html},
  doi = {10.1007/978-3-642-41527-2_15}
}

@inproceedings{HaeuplerOptimalGossipDirect2014,
  address = {New York, NY, USA},
  series = {PODC '14},
  title = {Optimal {{Gossip}} with {{Direct Addressing}}},
  isbn = {978-1-4503-2944-6},
  doi = {10.1145/2611462.2611489},
  abstract = {Gossip algorithms spread information in distributed networks by nodes repeatedly forwarding information to a few random contacts. By their very nature, gossip algorithms tend to be distributed and fault tolerant. If done right, they can also be fast and message-efficient. A common model for gossip communication is the random phone call model, in which in each synchronous round each node can PUSH or PULL information to or from a random other node. For example, Karp et al. [FOCS 2000] gave algorithms in this model that spread a message to all nodes in $\Theta$(log n) rounds while sending only O(log log n) messages per node on average. They also showed that at least $\Theta$(log n) rounds are necessary in this model and that algorithms achieving this round-complexity need to send $\omega$(1) messages per node on average. Recently, Avin and Elsasser [DISC 2013], studied the random phone call model with the natural and commonly used assumption of direct addressing. Direct addressing allows nodes to directly contact nodes whose ID (e.g., IP address) was learned before. They show that in this setting, one can "break the log n barrier" and achieve a gossip algorithm running in O($\surd$log n) rounds, albeit while using O($\surd$log n) messages per node. In this paper we study the same model and give a simple gossip algorithm which spreads a message in only O(log log n) rounds. We furthermore prove a matching $\Omega$(log log n) lower bound which shows that this running time is best possible. In particular we show that any gossip algorithm takes with high probability at least 0.99 log log n rounds to terminate. Lastly, our algorithm can be tweaked to send only O(1) messages per node on average with only O(log n) bits per message. Our algorithm therefore simultaneously achieves the optimal round-, message-, and bit-complexity for this setting. As all prior gossip algorithms, our algorithm is also robust against failures. In particular, if in the beginning an oblivious adversary fails any F nodes our algorithm still, with high probability, informs all but o(F) surviving nodes.},
  booktitle = {Proceedings of the 2014 {{ACM Symposium}} on {{Principles}} of {{Distributed Computing}}},
  publisher = {{ACM}},
  author = {Haeupler, Bernhard and Malkhi, Dahlia},
  year = {2014},
  keywords = {information dissemination,rumor spreading,direct addressing,gossip,peer-to-peer (P2P),pointer jumping},
  pages = {176--185},
  file = {/Users/billlai/Zotero/storage/5YIM3ZPN/Haeupler and Malkhi - 2014 - Optimal Gossip with Direct Addressing.pdf}
}

@article{HaeuplerAnalyzingNetworkCoding2016,
  title = {Analyzing {{Network Coding}} ({{Gossip}}) {{Made Easy}}},
  volume = {63},
  issn = {0004-5411},
  doi = {10.1145/2629696},
  abstract = {We introduce projection analysis\textemdash{}a new technique to analyze the stopping time of protocols that are based on random linear network coding (RLNC). Projection analysis drastically simplifies, extends, and strengthens previous results on RLNC gossip protocols. We analyze RLNC gossip in a general framework for network and communication models that encompasses and unifies the models used previously in this context. We show, in most settings for the first time, that the RLNC gossip converges with high probability in optimal time. Most stopping times are of the form O(k + T), where k is the number of messages to be distributed and T is the time it takes to disseminate one message. This means RLNC gossip achieves ``perfect pipelining.'' Our analysis directly extends to highly dynamic networks in which the topology can change completely at any time. This remains true, even if the network dynamics are controlled by a fully adaptive adversary that knows the complete network state. Virtually nothing besides simple O(kT) sequential flooding protocols was previously known for such a setting. While RLNC gossip works in this wide variety of networks our analysis remains the same and extremely simple. This contrasts with more complex proofs that were put forward to give less strong results for various special cases.},
  number = {3},
  journal = {J. ACM},
  author = {Haeupler, Bernhard},
  month = aug,
  year = {2016},
  keywords = {gossip,dynamic networks,multicast,Random linear network coding},
  pages = {26:1--26:22},
  file = {/Users/billlai/Zotero/storage/SRB6JGAT/Haeupler - 2016 - Analyzing Network Coding (Gossip) Made Easy.pdf}
}

@inproceedings{GuoGossipvsMarkov2015,
  address = {Philadelphia, PA, USA},
  series = {SODA '15},
  title = {Gossip vs. {{Markov Chains}}, and {{Randomness}}-Efficient {{Rumor Spreading}}},
  abstract = {We study gossip algorithms for the rumor spreading problem which asks one node to deliver a rumor to all nodes in an unknown network, and every node is only allowed to call one neighbor in each round. In this work we introduce two fundamentally new techniques in studying the rumor spreading problem: First, we establish a new connection between the rumor spreading process in an arbitrary graph and certain Markov chains. While most previous work analyzed the rumor spreading time in general graphs by studying the rate of the number of (un-)informed nodes after every round, we show that the mixing time of a certain Markov chain suffices to bound the rumor spreading time in an arbitrary graph. Second, we construct a reduction from rumor spreading processes to branching programs. This reduction gives us a general framework to derandomize the rumor spreading and other gossip processes. In particular, we show that, for any n-vertex expander graph, there is a protocol which informs every node in O(log n) rounds with high probability, and uses O(log n $\cdot$ log log n) random bits in total. The runtime of our protocol is tight, and the randomness requirement of O(log n $\cdot$ log log n) random bits almost matches the lower bound of $\Omega$(log n) random bits. We further show that, for many graph families (defined with respect to the expansion and the degree), O(poly log n) random bits in total suffice for fast rumor spreading. These results give us an almost complete understanding of the role of randomness in the rumor spreading process, which was extensively studied over the past years.},
  booktitle = {Proceedings of the {{Twenty}}-Sixth {{Annual ACM}}-{{SIAM Symposium}} on {{Discrete Algorithms}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Guo, Zeyu and Sun, He},
  year = {2015},
  pages = {411--430},
  file = {/Users/billlai/Zotero/storage/QV5WESHU/Guo and Sun - 2015 - Gossip vs. Markov Chains, and Randomness-efficient.pdf}
}

@inproceedings{ElsasserInfluenceGraphDensity2015,
  title = {On the {{Influence}} of {{Graph Density}} on {{Randomized Gossiping}}},
  doi = {10.1109/IPDPS.2015.32},
  abstract = {Information dissemination is a fundamental problem in parallel and distributed computing. In its simplest variant, known as the broadcasting problem, a single message has to be spread among all nodes of a graph. A prominent communication protocol for this problem is based on the socalled random phone call model (Karp et al., FOCS 2000). In each step, every node opens a communication channel to a randomly chosen neighbor, which can then be used for bidirectional communication. Motivated by replicated databases and peer-to-peer networks, Berenbrink et al., ICALP 2010, considered the so-called gossiping problem in the random phone call model. There, each node starts with its own message and all messages have to be disseminated to all nodes in the network. They showed that any O(log n)-time algorithm in complete graphs requires $\Omega$(log n) message transmissions per node to complete gossiping, with high probability, while it is known that in the case of broadcasting the average number of message transmissions per node is O(log log n). Furthermore, they explored different possibilities on how to reduce the communication overhead of randomized gossiping in complete graphs. It is known that the O(nloglogn) bound on the number of message transmissions produced by randomized broadcasting in complete graphs cannot be achieved in sparse graphs even if they have best expansion and connectivity properties. In this paper, we analyze whether a similar influence of the graph density also holds w.r.t. the performance of gossiping. We study analytically and empirically the communication overhead generated by gossiping algorithms w.r.t. the random phone call model in random graphs and also consider simple modifications of the random phone call model in these graphs. Our results indicate that, unlike in broadcasting, there seems to be no significant difference between the performance of randomized gossiping in complete graphs and sparse random graphs. Furthermore, our simulations - llustrate that by tuning the parameters of our algorithms, we can significantly reduce the communication overhead compared to the traditional push-pull approach in the graphs we consider.},
  booktitle = {2015 {{IEEE International Parallel}} and {{Distributed Processing Symposium}}},
  author = {Els\"asser, R. and Kaaser, D.},
  month = may,
  year = {2015},
  keywords = {Algorithm design and analysis,graph theory,parallel algorithms,Complexity theory,probability,distributed computing,information dissemination,Analytical models,bidirectional communication,Broadcasting,broadcasting problem,communication channel,Communication channels,communication complexity,communication overhead,communication protocol,complete graph,connectivity property,graph density,graph nodes,message dissemination,message spreading,message transmission,network nodes,network theory (graphs),node message,parallel computing,parameter tuning,Peer-to-peer computing,Protocols,random phone call model,random processes,randomised algorithms,randomized gossiping,randomly chosen neighbor,sparse random graph},
  pages = {521-531},
  file = {/Users/billlai/Zotero/storage/95JKB5PQ/7161540.html}
}

@article{MiltersenSuperpolynomialhalfexponentialcircuit,
  title = {Super-Polynomial versus Half-Exponential Circuit Size in the Exponential Hierarchy},
  abstract = {Lower bounds on circuit size were previously established for functions in $\Sigma$p2, ZPPNP, $\Sigma$e2xp, ZPEXPNP and MAexp. We investigate the general question: Given a time bound f (n). What is the best circuit size lower bound that can be shown for the classes MA-TIME[f ], ZP-TIMENP[f ], . . . using the techniques currently known? For the classes MAexp, ZPEXPNP and $\Sigma$e2xp, the answer we get is ``halfexponential''. Informally, a function f is said to be half-exponential if f composed with itself is exponential.},
  language = {en},
  author = {Miltersen, Peter Bro and Vinodchandran, N V and Watanabe, Osamu},
  pages = {17},
  file = {/Users/billlai/Zotero/storage/8LD6BTD8/Miltersen et al. - Super-polynomial versus half-exponential circuit s.pdf}
}

@article{BabaiBPPHASSUBEXPONENTIAL,
  title = {{{BPP HAS SUBEXPONENTIAL TIME SIMULATIONS UNLESS EXPTIME HAS PUBLISHABLE PROOFS}}},
  language = {en},
  author = {Babai, Laszlo and Fortnow, Lance and Nisan, Noam and Wigderson, Avi},
  pages = {12},
  file = {/Users/billlai/Zotero/storage/VQSY4SA6/Babai et al. - BPP HAS SUBEXPONENTIAL TIME SIMULATIONS UNLESS EXP.pdf}
}

@inproceedings{BabaiCheckingcomputationspolylogarithmic1991,
  title = {Checking Computations in Polylogarithmic Time},
  isbn = {978-0-89791-397-3},
  doi = {10.1145/103418.103428},
  abstract = {Motivated by Manuel Blum's concept of instance checking, we consider new, very fast and generic mechanisms of checking computations. Our results exploit recent advances in interactive proof protocols LFKN92], Sha92], and especially the MIP = NEXP protocol from BFL91].},
  language = {en},
  publisher = {{ACM Press}},
  author = {Babai, L\'aszl\'o and Fortnow, Lance and Levin, Leonid A. and Szegedy, Mario},
  year = {1991},
  pages = {21-32},
  file = {/Users/billlai/Zotero/storage/E5E5W46U/Babai et al. - 1991 - Checking computations in polylogarithmic time.pdf}
}

@article{CoudertRevisitingDecompositionClique2018,
  title = {Revisiting {{Decomposition}} by {{Clique Separators}}},
  volume = {32},
  issn = {0895-4801},
  doi = {10.1137/16M1059837},
  abstract = {We study the complexity of decomposing a graph by means of clique separators. This common algorithmic tool, first introduced by Tarjan, allows one to cut a graph into smaller pieces, and so it can be applied to preprocess the graph in the computation of optimization problems. However, the best-known algorithms for computing a decomposition have respective \$\{$\backslash$cal O\}(nm)\$-time and \$\{$\backslash$cal O\}(n\^\{(3+$\backslash$alpha)/2\}) = o(n\^\{2.69\})\$-time complexity with \$$\backslash$alpha $<$ 2.3729\$ being the exponent for matrix multiplication. Such running times are prohibitive for large graphs. Here we prove that for every graph \$G\$, a decomposition can be computed in \$\{$\backslash$cal O\}(T(G) + $\backslash$min$\backslash$\{n\^\{$\backslash$alpha\},$\backslash$omega\^2 n$\backslash$\})\$-time with \$T(G)\$ and \$$\backslash$omega\$ being, respectively, the time needed to compute a minimal triangulation of \$G\$ and the clique-number of \$G\$. In particular, it implies that every graph can be decomposed by clique separators in \$\{$\backslash$cal O\}(n\^\{$\backslash$alpha\}$\backslash$log n)\$-time. Based on prior work from Kratsch et al., we prove in addition that decomposing a graph by clique-separators is as least as hard as triangle detection. Therefore, the existence of any \$o(n\^\{$\backslash$alpha\})\$-time algorithm for this problem would be a significant breakthrough in the algorithmic field. Finally, our main result implies that planar graphs, bounded-treewidth graphs, and bounded-degree graphs can be decomposed by clique separators in linear or quasi-linear time.},
  number = {1},
  journal = {SIAM Journal on Discrete Mathematics},
  author = {Coudert, D. and Ducoffe, G.},
  month = jan,
  year = {2018},
  pages = {682-694},
  file = {/Users/billlai/Zotero/storage/RN75KACJ/Coudert and Ducoffe - 2018 - Revisiting Decomposition by Clique Separators.pdf;/Users/billlai/Zotero/storage/Y74JVTT3/16M1059837.html}
}

@article{LeimerOptimaldecompositionclique1993,
  title = {Optimal Decomposition by Clique Separators},
  volume = {113},
  issn = {0012-365X},
  doi = {10.1016/0012-365X(93)90510-Z},
  abstract = {Decompositions of a graph by clique separators are investigated which have the additional property that they do not generate new maximal prime subgraphs. Using such decompositions is preferable in many applications, since they lead to a minimal system of derived subgraphs. The methods used in the proofs are familiar from the investigations of chordal graphs and acyclic hypergraphs and some well-known results for these (hyper-) graphs are shown to be simple special cases of results for maximal prime subgraphs. Tarjan has described an O(nm)-time algorithm to decompose a graph with n vertices and m edges by means of clique separators. This algorithm is modified, so that no new maximal prime subgraphs are generated, i.e. so that a graph is decomposed exactly into its maximal prime subgraphs which is the unique minimal derived system of prime subgraphs.},
  number = {1},
  journal = {Discrete Mathematics},
  author = {Leimer, Hanns-Georg},
  month = apr,
  year = {1993},
  pages = {99-123},
  file = {/Users/billlai/Zotero/storage/IZUYPCFR/Leimer - 1993 - Optimal decomposition by clique separators.pdf;/Users/billlai/Zotero/storage/HNXC26PY/0012365X9390510Z.html}
}

@article{HaeuplerSimpleFastDeterministic2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1210.1193},
  primaryClass = {cs, math},
  title = {Simple, {{Fast}} and {{Deterministic Gossip}} and {{Rumor Spreading}}},
  doi = {10.1137/1.9781611973105.51},
  abstract = {We study gossip algorithms for the rumor spreading problem which asks each node to deliver a rumor to all nodes in an unknown network. Gossip algorithms allow nodes only to call one neighbor per round and have recently attracted attention as message efficient, simple and robust solutions to the rumor spreading problem. Recently, non-uniform random gossip schemes were devised to allow efficient rumor spreading in networks with bottlenecks. In particular, [Censor-Hillel et al., STOC'12] gave an O(log\^3 n) algorithm to solve the 1-local broadcast problem in which each node wants to exchange rumors locally with its 1-neighborhood. By repeatedly applying this protocol one can solve the global rumor spreading quickly for all networks with small diameter, independently of the conductance. This and all prior gossip algorithms for the rumor spreading problem have been inherently randomized in their design and analysis. This resulted in a parallel research direction trying to reduce and determine the amount of randomness needed for efficient rumor spreading. This has been done via lower bounds for restricted models and by designing gossip algorithms with a reduced need for randomness. The general intuition and consensus of these results has been that randomization plays a important role in effectively spreading rumors. In this paper we improves over this state of the art in several ways by presenting a deterministic gossip algorithm that solves the the k-local broadcast problem in 2(k+log n)log n rounds. Besides being the first efficient deterministic solution to the rumor spreading problem this algorithm is interesting in many aspects: It is simpler, more natural, more robust and faster than its randomized pendant and guarantees success with certainty instead of with high probability. Its analysis is furthermore simple, self-contained and fundamentally different from prior works.},
  journal = {arXiv:1210.1193 [cs, math]},
  author = {Haeupler, Bernhard},
  month = jan,
  year = {2013},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Combinatorics,Computer Science - Distributed; Parallel; and Cluster Computing},
  pages = {705-716},
  file = {/Users/billlai/Zotero/storage/EMX636EU/Haeupler - 2013 - Simple, Fast and Deterministic Gossip and Rumor Sp.pdf;/Users/billlai/Zotero/storage/QKV9466L/1210.html}
}

@article{HaeuplerSimpleFastDeterministic2013a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1210.1193},
  primaryClass = {cs, math},
  title = {Simple, {{Fast}} and {{Deterministic Gossip}} and {{Rumor Spreading}}},
  doi = {10.1137/1.9781611973105.51},
  abstract = {We study gossip algorithms for the rumor spreading problem which asks each node to deliver a rumor to all nodes in an unknown network. Gossip algorithms allow nodes only to call one neighbor per round and have recently attracted attention as message efficient, simple and robust solutions to the rumor spreading problem. Recently, non-uniform random gossip schemes were devised to allow efficient rumor spreading in networks with bottlenecks. In particular, [Censor-Hillel et al., STOC'12] gave an O(log\^3 n) algorithm to solve the 1-local broadcast problem in which each node wants to exchange rumors locally with its 1-neighborhood. By repeatedly applying this protocol one can solve the global rumor spreading quickly for all networks with small diameter, independently of the conductance. This and all prior gossip algorithms for the rumor spreading problem have been inherently randomized in their design and analysis. This resulted in a parallel research direction trying to reduce and determine the amount of randomness needed for efficient rumor spreading. This has been done via lower bounds for restricted models and by designing gossip algorithms with a reduced need for randomness. The general intuition and consensus of these results has been that randomization plays a important role in effectively spreading rumors. In this paper we improves over this state of the art in several ways by presenting a deterministic gossip algorithm that solves the the k-local broadcast problem in 2(k+log n)log n rounds. Besides being the first efficient deterministic solution to the rumor spreading problem this algorithm is interesting in many aspects: It is simpler, more natural, more robust and faster than its randomized pendant and guarantees success with certainty instead of with high probability. Its analysis is furthermore simple, self-contained and fundamentally different from prior works.},
  journal = {arXiv:1210.1193 [cs, math]},
  author = {Haeupler, Bernhard},
  month = jan,
  year = {2013},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Combinatorics,Computer Science - Distributed; Parallel; and Cluster Computing},
  pages = {705-716},
  file = {/Users/billlai/Zotero/storage/KSP7SXWM/Haeupler - 2013 - Simple, Fast and Deterministic Gossip and Rumor Sp.pdf}
}

@article{HaeuplerSimpleFastDeterministic2013b,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1210.1193},
  primaryClass = {cs, math},
  title = {Simple, {{Fast}} and {{Deterministic Gossip}} and {{Rumor Spreading}}},
  doi = {10.1137/1.9781611973105.51},
  abstract = {We study gossip algorithms for the rumor spreading problem, which asks each node to deliver a rumor to all nodes in an unknown network. Gossip algorithms allow nodes only to call one neighbor per round and have recently attracted attention as message efficient, simple, and robust solutions to the rumor spreading problem.},
  language = {en},
  journal = {arXiv:1210.1193 [cs, math]},
  author = {Haeupler, Bernhard},
  month = jan,
  year = {2013},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Combinatorics,Computer Science - Distributed; Parallel; and Cluster Computing},
  pages = {705-716},
  file = {/Users/billlai/Zotero/storage/KXMIUSN6/Haeupler - SIMPLE, FAST AND DETERMINISTIC GOSSIP AND RUMOR SP.pdf;/Users/billlai/Zotero/storage/W5TBQSJC/Haeupler - 2013 - Simple, Fast and Deterministic Gossip and Rumor Sp.pdf}
}

@article{HaeuplerSIMPLEFASTDETERMINISTIC,
  title = {{{SIMPLE}}, {{FAST AND DETERMINISTIC GOSSIP AND RUMOR SPREADING}}},
  language = {en},
  author = {Haeupler, Bernhard},
  pages = {79}
}

@article{ChierichettiRumorSpreadingConductance2018,
  title = {Rumor {{Spreading}} and {{Conductance}}},
  volume = {65},
  issn = {00045411},
  doi = {10.1145/3173043},
  language = {en},
  number = {4},
  journal = {Journal of the ACM},
  author = {Chierichetti, Flavio and Giakkoupis, George and Lattanzi, Silvio and Panconesi, Alessandro},
  month = apr,
  year = {2018},
  pages = {1-21},
  file = {/Users/billlai/Zotero/storage/VBB8IC24/Chierichetti et al. - 2018 - Rumor Spreading and Conductance.pdf}
}

@article{HaeuplerNewConstructiveAspects2011,
  title = {New {{Constructive Aspects}} of the {{Lov\'asz Local Lemma}}},
  volume = {58},
  issn = {00045411},
  doi = {10.1145/2049697.2049702},
  language = {en},
  number = {6},
  journal = {Journal of the ACM},
  author = {Haeupler, Bernhard and Saha, Barna and Srinivasan, Aravind},
  month = dec,
  year = {2011},
  pages = {1-28},
  file = {/Users/billlai/Zotero/storage/C7CFEAKG/Haeupler et al. - 2011 - New Constructive Aspects of the Lovász Local Lemma.pdf}
}

@article{LindSpreadinggossipsocial2007,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0705.3224},
  title = {Spreading Gossip in Social Networks},
  volume = {76},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.76.036117},
  abstract = {We study a simple model of information propagation in social networks, where two quantities are introduced: the spread factor, which measures the average maximal fraction of neighbors of a given node that interchange information among each other, and the spreading time needed for the information to reach such fraction of nodes. When the information refers to a particular node at which both quantities are measured, the model can be taken as a model for gossip propagation. In this context, we apply the model to real empirical networks of social acquaintances and compare the underlying spreading dynamics with different types of scale-free and small-world networks. We find that the number of friendship connections strongly influences the probability of being gossiped. Finally, we discuss how the spread factor is able to be applied to other situations.},
  number = {3},
  journal = {Physical Review E},
  author = {Lind, Pedro G. and {da Silva}, Luciano R. and Andrade Jr., Jos\'e S. and Herrmann, Hans J.},
  month = sep,
  year = {2007},
  keywords = {Physics - Physics and Society},
  file = {/Users/billlai/Zotero/storage/3UBSNSAC/Lind et al. - 2007 - Spreading gossip in social networks.pdf;/Users/billlai/Zotero/storage/8CG5QJC5/0705.html}
}

@article{LindSpreadinggossipsocial2007a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0705.3224},
  title = {Spreading Gossip in Social Networks},
  volume = {76},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.76.036117},
  abstract = {We study a simple model of information propagation in social networks, where two quantities are introduced: the spread factor, which measures the average maximal fraction of neighbors of a given node that interchange information among each other, and the spreading time needed for the information to reach such fraction of nodes. When the information refers to a particular node at which both quantities are measured, the model can be taken as a model for gossip propagation. In this context, we apply the model to real empirical networks of social acquaintances and compare the underlying spreading dynamics with different types of scale-free and small-world networks. We find that the number of friendship connections strongly influences the probability of being gossiped. Finally, we discuss how the spread factor is able to be applied to other situations.},
  number = {3},
  journal = {Physical Review E},
  author = {Lind, Pedro G. and {da Silva}, Luciano R. and Andrade Jr., Jos\'e S. and Herrmann, Hans J.},
  month = sep,
  year = {2007},
  keywords = {Physics - Physics and Society},
  file = {/Users/billlai/Zotero/storage/BT6H6UD5/Lind et al. - 2007 - Spreading gossip in social networks.pdf;/Users/billlai/Zotero/storage/TAFAJ6CZ/0705.html}
}

@article{DunbarGossipevolutionaryperspective2004,
  title = {Gossip in Evolutionary Perspective},
  volume = {8},
  issn = {1939-1552(Electronic),1089-2680(Print)},
  doi = {10.1037/1089-2680.8.2.100},
  abstract = {Conversation is a uniquely human phenomenon. Analyses of freely forming conversations indicate that approximately two thirds of conversation time is devoted to social topics, most of which can be given the generic label gossip. This article first explores the origins of gossip as a mechanism for bonding social groups, tracing these origins back to social grooming among primates. It then asks why social gossip in this sense should form so important a component of human interaction and presents evidence to suggest that, aside from servicing social networks, a key function may be related explicitly to controlling free riders. Finally, the author reviews briefly the role of social cognition in facilitating conversations of this kind. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  number = {2},
  journal = {Review of General Psychology},
  author = {Dunbar, R. I. M.},
  year = {2004},
  keywords = {Gossip,Attachment Behavior,Conversation,Interpersonal Interaction,Social Groups,Social Networks},
  pages = {100-110},
  file = {/Users/billlai/Zotero/storage/7ZG2PCX6/2004-14314-004.html}
}

@article{Feinbergvirtuesgossipreputational2012,
  title = {The Virtues of Gossip: Reputational Information Sharing as Prosocial Behavior},
  volume = {102},
  issn = {1939-1315},
  shorttitle = {The Virtues of Gossip},
  doi = {10.1037/a0026650},
  abstract = {Reputation systems promote cooperation and deter antisocial behavior in groups. Little is known, however, about how and why people share reputational information. Here, we seek to establish the existence and dynamics of prosocial gossip, the sharing of negative evaluative information about a target in a way that protects others from antisocial or exploitative behavior. We present a model of prosocial gossip and the results of 4 studies testing the model's claims. Results of Studies 1 through 3 demonstrate that (a) individuals who observe an antisocial act experience negative affect and are compelled to share information about the antisocial actor with a potentially vulnerable person, (b) sharing such information reduces negative affect created by observing the antisocial behavior, and (c) individuals possessing more prosocial orientations are the most motivated to engage in such gossip, even at a personal cost, and exhibit the greatest reduction in negative affect as a result. Study 4 demonstrates that prosocial gossip can effectively deter selfishness and promote cooperation. Taken together these results highlight the roles of prosocial motivations and negative affective reactions to injustice in maintaining reputational information sharing in groups. We conclude by discussing implications for reputational theories of the maintenance of cooperation in human groups.},
  language = {eng},
  number = {5},
  journal = {Journal of Personality and Social Psychology},
  author = {Feinberg, Matthew and Willer, Robb and Stellar, Jennifer and Keltner, Dacher},
  month = may,
  year = {2012},
  keywords = {Communication,Cooperative Behavior,Emotions,Female,Group Processes,Humans,Information Dissemination,Logistic Models,Male,Motivation,Multivariate Analysis,Social Behavior,Social Control; Informal,Social Values,United States},
  pages = {1015-1030},
  pmid = {22229458}
}

@inproceedings{Censor-HillelGlobalComputationPoorly2012,
  address = {New York, NY, USA},
  series = {STOC '12},
  title = {Global {{Computation}} in a {{Poorly Connected World}}: {{Fast Rumor Spreading}} with {{No Dependence}} on {{Conductance}}},
  isbn = {978-1-4503-1245-5},
  shorttitle = {Global {{Computation}} in a {{Poorly Connected World}}},
  doi = {10.1145/2213977.2214064},
  abstract = {In this paper, we study the question of how efficiently a collection of interconnected nodes can perform a global computation in the GOSSIP model of communication. In this model, nodes do not know the global topology of the network, and they may only initiate contact with a single neighbor in each round. This model contrasts with the much less restrictive LOCAL model, where a node may simultaneously communicate with all of its neighbors in a single round. A basic question in this setting is how many rounds of communication are required for the information dissemination problem, in which each node has some piece of information and is required to collect all others. In the LOCAL model, this is quite simple: each node broadcasts all of its information in each round, and the number of rounds required will be equal to the diameter of the underlying communication graph. In the GOSSIP model, each node must independently choose a single neighbor to contact, and the lack of global information makes it difficult to make any sort of principled choice. As such, researchers have focused on the uniform gossip algorithm, in which each node independently selects a neighbor uniformly at random. When the graph is well-connected, this works quite well. In a string of beautiful papers, researchers proved a sequence of successively stronger bounds on the number of rounds required in terms of the conductance $\varphi$ and graph size n, culminating in a bound of O($\varphi$-1 log n). In this paper, we show that a fairly simple modification of the protocol gives an algorithm that solves the information dissemination problem in at most O(D + polylog (n)) rounds in a network of diameter D, with no dependence on the conductance. This is at most an additive polylogarithmic factor from the trivial lower bound of D, which applies even in the LOCAL model. In fact, we prove that something stronger is true: any algorithm that requires T rounds in the LOCAL model can be simulated in O(T + polylog(n)) rounds in the GOSSIP model. We thus prove that these two models of distributed computation are essentially equivalent.},
  booktitle = {Proceedings of the {{Forty}}-Fourth {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {{Censor-Hillel}, Keren and Haeupler, Bernhard and Kelner, Jonathan and Maymounkov, Petar},
  year = {2012},
  keywords = {gossip,broadcast,expander decomposition,information spreading,sparse spanners},
  pages = {961--970},
  file = {/Users/billlai/Zotero/storage/RMTHAUQJ/Censor-Hillel et al. - 2012 - Global Computation in a Poorly Connected World Fa.pdf}
}

@book{DiestelGraphTheory2017,
  address = {Berlin Heidelberg},
  edition = {5},
  series = {Graduate Texts in Mathematics},
  title = {Graph {{Theory}}},
  isbn = {978-3-662-53621-6},
  abstract = {This standard textbook of modern graph theory, now in its fifth edition, combines the authority of a classic with the engaging freshness of style that is the hallmark of active mathematics. It covers the core material of the subject with concise yet reliably complete proofs, while offering glimpses of more advanced methods in each field by one or two deeper results, again with proofs given in full detail. The book can be used as a reliable text for an introductory course, as a graduate text, and for self-study. From the reviews: ``This outstanding book cannot be substituted with any other book on the present textbook market. It has every chance of becoming the standard textbook for graph theory.'' Acta Scientiarum Mathematiciarum ``Deep, clear, wonderful. This is a serious book about the heart of graph theory. It has depth and integrity.'' Persi Diaconis \& Ron Graham, SIAM Review ``The book has received a very enthusiastic reception, which it amply deserves. A masterly elucidation of modern graph theory.'' Bulletin of the Institute of Combinatorics and its Applications ``Succeeds dramatically ... a hell of a good book.'' MAA Reviews ``A highlight of the book is what is by far the best account in print of the Seymour-Robertson theory of graph minors.'' Mathematika `` ... like listening to someone explain mathematics.'' Bulletin of the AMS},
  language = {en},
  publisher = {{Springer-Verlag}},
  author = {Diestel, Reinhard},
  year = {2017},
  file = {/Users/billlai/Zotero/storage/JMXCV2IG/9783662536216.html}
}

@article{JerrumApproximatingPermanent1989,
  title = {Approximating the {{Permanent}}},
  volume = {18},
  issn = {0097-5397},
  doi = {10.1137/0218077},
  abstract = {A randomised approximation scheme for the permanent of a 0\textendash{}1s presented. The task of estimating a permanent is reduced to that of almost uniformly generating perfect matchings in a graph; the latter is accomplished by simulating a Markov chain whose states are the matchings in the graph. For a wide class of 0\textendash{}1 matrices the approximation scheme is fully-polynomial, i.e., runs in time polynomial in the size of the matrix and a parameter that controls the accuracy of the output. This class includes all dense matrices (those that contain sufficiently many 1's) and almost all sparse matrices in some reasonable probabilistic model for 0\textendash{}1 matrices of given density.For the approach sketched above to be computationally efficient, the Markov chain must be rapidly mixing: informally, it must converge in a short time to its stationary distribution. A major portion of the paper is devoted to demonstrating that the matchings chain is rapidly mixing, apparently the first such result for a Markov chain with genuinely complex structure. The techniques used seem to have general applicability, and are applied again in the paper to validate a fully-polynomial randomised approximation scheme for the partition function of an arbitrary monomer-dimer system.},
  number = {6},
  journal = {SIAM Journal on Computing},
  author = {Jerrum, M. and Sinclair, A.},
  month = dec,
  year = {1989},
  pages = {1149-1178},
  file = {/Users/billlai/Zotero/storage/W9ZG3XV4/Jerrum and Sinclair - 1989 - Approximating the Permanent.pdf;/Users/billlai/Zotero/storage/XWI557KJ/0218077.html}
}

@inproceedings{DemersEpidemicAlgorithmsReplicated1987,
  address = {New York, NY, USA},
  series = {PODC '87},
  title = {Epidemic {{Algorithms}} for {{Replicated Database Maintenance}}},
  isbn = {978-0-89791-239-6},
  doi = {10.1145/41840.41841},
  booktitle = {Proceedings of the {{Sixth Annual ACM Symposium}} on {{Principles}} of {{Distributed Computing}}},
  publisher = {{ACM}},
  author = {Demers, Alan and Greene, Dan and Hauser, Carl and Irish, Wes and Larson, John and Shenker, Scott and Sturgis, Howard and Swinehart, Dan and Terry, Doug},
  year = {1987},
  pages = {1--12},
  file = {/Users/billlai/Zotero/storage/37SE8NIR/Demers et al. - 1987 - Epidemic Algorithms for Replicated Database Mainte.pdf}
}

@article{Erdosevolutionrandomgraphs1960,
  title = {On the Evolution of Random Graphs},
  volume = {5},
  journal = {Publ. Math. Inst. Hung. Acad. Sci},
  author = {Erd{\H o}s, Paul and R\'enyi, Alfr\'ed},
  year = {1960},
  pages = {17--61},
  file = {/Users/billlai/Zotero/storage/H99JERGY/Erds and Rényi - 1960 - On the evolution of random graphs.pdf;/Users/billlai/Zotero/storage/XUUGP5KH/Erds and Rényi - 1960 - On the evolution of random graphs.pdf}
}

@article{JonassonLollipopgraphsare2002,
  title = {Lollipop Graphs Are Extremal for Commute Times},
  volume = {16},
  copyright = {Copyright \textcopyright{} 2000 John Wiley \& Sons, Inc.},
  issn = {1098-2418},
  doi = {10.1002/(SICI)1098-2418(200003)16:2<131::AID-RSA1>3.0.CO;2-3},
  language = {en},
  number = {2},
  journal = {Random Structures \& Algorithms},
  author = {Jonasson, Johan},
  year = {2002},
  pages = {131-142},
  file = {/Users/billlai/Zotero/storage/EFVUTGH5/(SICI)1098-2418(200003)162131AID-RSA13.0.html}
}

@article{AlonManyRandomWalks2007,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0705.0467},
  primaryClass = {math},
  title = {Many {{Random Walks Are Faster Than One}}},
  abstract = {We pose a new and intriguing question motivated by distributed computing regarding random walks on graphs: How long does it take for several independent random walks, starting from the same vertex, to cover an entire graph? We study the cover time - the expected time required to visit every node in a graph at least once - and we show that for a large collection of interesting graphs, running many random walks in parallel yields a speed-up in the cover time that is linear in the number of parallel walks. We demonstrate that an exponential speed-up is sometimes possible, but that some natural graphs allow only a logarithmic speed-up. A problem related to ours (in which the walks start from some probabilistic distribution on vertices) was previously studied in the context of space efficient algorithms for undirected s-t connectivity and our results yield, in certain cases, an improvement upon some of the earlier bounds.},
  journal = {arXiv:0705.0467 [math]},
  author = {Alon, Noga and Avin, Chen and Koucky, Michal and Kozma, Gady and Lotker, Zvi and Tuttle, Mark R.},
  month = may,
  year = {2007},
  keywords = {Mathematics - Probability},
  file = {/Users/billlai/Zotero/storage/VNS28NEA/Alon et al. - 2007 - Many Random Walks Are Faster Than One.pdf;/Users/billlai/Zotero/storage/FNIUEXBP/0705.html}
}

@article{CENSOR-HILLELFASTINFORMATIONSPREADING2012,
  title = {{{FAST INFORMATION SPREADING IN GRAPHS WITH LARGE WEAK CONDUCTANCE}}${_\ast}$},
  abstract = {Gathering data from nodes in a network is at the heart of many distributed applications, most notably, while performing a global task. We consider information spreading among n nodes of a network, where each node v has a message m(v) which must be received by all other nodes. The time required for information spreading has been previously upper-bounded with an inverse relationship to the conductance of the underlying communication graph. This implies high running time bounds for graphs with small conductance.},
  language = {en},
  author = {{CENSOR-HILLEL}, KEREN and SHACHNAI, HADAS},
  year = {2012},
  pages = {15},
  file = {/Users/billlai/Zotero/storage/CRU4K95F/CENSOR-HILLEL and SHACHNAI - FAST INFORMATION SPREADING IN GRAPHS WITH LARGE WE.pdf}
}

@article{Cheegerlowerboundsmallest1969,
  title = {{A lower bound for the smallest eigenvalue of the Laplacian}},
  language = {English (US)},
  journal = {Proceedings of the Princeton conference in honor of Professor S. Bochner},
  author = {Cheeger, Jeff},
  year = {1969},
  pages = {195-199},
  file = {/Users/billlai/Zotero/storage/D7TIMGMU/a-lower-bound-for-the-smallest-eigenvalue-of-the-laplacian.html}
}

@inproceedings{Censor-HillelFastInformationSpreading2011,
  address = {Philadelphia, PA, USA},
  series = {SODA '11},
  title = {Fast {{Information Spreading}} in {{Graphs}} with {{Large Weak Conductance}}},
  abstract = {Gathering data from nodes in a network is at the heart of many distributed applications, most notably, while performing a global task. We consider information spreading among n nodes of a network, where each node v has a message m(v) which must be received by all other nodes. The time required for information spreading has been previously upper-bounded with an inverse relationship to the conductance of the underlying communication graph. This implies high running times for graphs with small conductance. The main contribution of this paper is an information spreading algorithm which overcomes communication bottlenecks and thus achieves fast information spreading for a wide class of graphs, despite their small conductance. As a key tool in our study we use the recently defined concept of weak conductance, a generalization of classic graph conductance which measures how well-connected the components of a graph are. Our hybrid algorithm, which alternates between random and deterministic communication phases, exploits the connectivity within components by first applying partial information spreading, after which messages are sent across bottlenecks, thus spreading further throughout the network. This yields substantial improvements over the best known running times of algorithms for information spreading on any graph that has a large weak conductance, from polynomial to polylogarithmic number of rounds. We demonstrate the power of fast information spreading in accomplishing global tasks on the leader election problem, which lies at the core of distributed computing. Our results yield an algorithm for leader election that has a scalable running time on graphs with large weak conductance, improving significantly upon previous results.},
  booktitle = {Proceedings of the {{Twenty}}-Second {{Annual ACM}}-{{SIAM Symposium}} on {{Discrete Algorithms}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {{Censor-Hillel}, Keren and Shachnai, Hadas},
  year = {2011},
  keywords = {distributed computing,information spreading,leader election,randomized algorithms,weak conductance},
  pages = {440--448},
  file = {/Users/billlai/Zotero/storage/R8TJ8VC2/Censor-Hillel and Shachnai - 2011 - Fast Information Spreading in Graphs with Large We.pdf}
}

@incollection{RobinsonCountingunlabeledacyclic1977,
  series = {Lecture Notes in Mathematics},
  title = {Counting Unlabeled Acyclic Digraphs},
  isbn = {978-3-540-08524-9 978-3-540-37020-8},
  abstract = {The previously known ways to count acyclic digraphs, both labeled and unlabeled, are reviewed. Then a new method of enumerating unlabeled acyclic digraphs is developed. It involves computing the sum of the cyclic indices of the automorphism groups of the acyclic digraphs, achieving a considerable gain in efficiency through an application of the inclusion-exclusion principle. Numerical results are reported on, and a table of the numbers of unlabeled acyclic digraphs on up to 18 points is included.},
  language = {en},
  booktitle = {Combinatorial {{Mathematics V}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Robinson, R. W.},
  year = {1977},
  pages = {28-43},
  file = {/Users/billlai/Zotero/storage/2RVURW65/Robinson - 1977 - Counting unlabeled acyclic digraphs.pdf;/Users/billlai/Zotero/storage/924CXYAB/BFb0069178.html},
  doi = {10.1007/BFb0069178}
}

@article{SantiniHOWMANYDAGs,
  title = {{{HOW MANY DAGs ARE THERE}}?},
  language = {en},
  author = {Santini, Simone},
  pages = {2},
  file = {/Users/billlai/Zotero/storage/83MDE5JZ/Santini - HOW MANY DAGs ARE THERE.pdf}
}

@article{TellQuantifiedDerandomizationLinear2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.07635},
  primaryClass = {cs},
  title = {Quantified {{Derandomization}} of {{Linear Threshold Circuits}}},
  abstract = {One of the prominent current challenges in complexity theory is the attempt to prove lower bounds for \$TC\^0\$, the class of constant-depth, polynomial-size circuits with majority gates. Relying on the results of Williams (2013), an appealing approach to prove such lower bounds is to construct a non-trivial derandomization algorithm for \$TC\^0\$. In this work we take a first step towards the latter goal, by proving the first positive results regarding the derandomization of \$TC\^0\$ circuits of depth \$d$>$2\$. Our first main result is a quantified derandomization algorithm for \$TC\^0\$ circuits with a super-linear number of wires. Specifically, we construct an algorithm that gets as input a \$TC\^0\$ circuit \$C\$ over \$n\$ input bits with depth \$d\$ and \$n\^\{1+$\backslash$exp(-d)\}\$ wires, runs in almost-polynomial-time, and distinguishes between the case that \$C\$ rejects at most \$2\^\{n\^\{1-1/5d\}\}\$ inputs and the case that \$C\$ accepts at most \$2\^\{n\^\{1-1/5d\}\}\$ inputs. In fact, our algorithm works even when the circuit \$C\$ is a linear threshold circuit, rather than just a \$TC\^0\$ circuit (i.e., \$C\$ is a circuit with linear threshold gates, which are stronger than majority gates). Our second main result is that even a modest improvement of our quantified derandomization algorithm would yield a non-trivial algorithm for standard derandomization of all of \$TC\^0\$, and would consequently imply that \$NEXP$\backslash$not$\backslash$subseteq TC\^0\$. Specifically, if there exists a quantified derandomization algorithm that gets as input a \$TC\^0\$ circuit with depth \$d\$ and \$n\^\{1+O(1/d)\}\$ wires (rather than \$n\^\{1+$\backslash$exp(-d)\}\$ wires), runs in time at most \$2\^\{n\^\{$\backslash$exp(-d)\}\}\$, and distinguishes between the case that \$C\$ rejects at most \$2\^\{n\^\{1-1/5d\}\}\$ inputs and the case that \$C\$ accepts at most \$2\^\{n\^\{1-1/5d\}\}\$ inputs, then there exists an algorithm with running time \$2\^\{n\^\{1-$\backslash$Omega(1)\}\}\$ for standard derandomization of \$TC\^0\$.},
  journal = {arXiv:1709.07635 [cs]},
  author = {Tell, Roei},
  month = sep,
  year = {2017},
  keywords = {Computer Science - Computational Complexity},
  file = {/Users/billlai/Zotero/storage/ERZN3NIH/Tell - 2017 - Quantified Derandomization of Linear Threshold Cir.pdf}
}

@article{VillaamilTreedepthRelatedNotions,
  title = {About {{Treedepth}} and {{Related Notions}}},
  language = {en},
  author = {Villaamil, Fernando S\'anchez},
  pages = {186},
  file = {/Users/billlai/Zotero/storage/2G63F87Z/Villaamil and University - About Treedepth and Related Notions.pdf}
}

@misc{CullerConnectedcomponentsdistributed,
  title = {Connected Components on Distributed Memory Machines},
  howpublished = {https://people.eecs.berkeley.edu/\textasciitilde{}yelick/papers/connect-submit95.ps},
  author = {Culler, David},
  file = {/Users/billlai/Zotero/storage/XJ69M6Q8/connect-submit95.html}
}

@article{StockmeyerSimulationParallelRandom1984,
  title = {Simulation of {{Parallel Random Access Machines}} by {{Circuits}}},
  volume = {13},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/0213027},
  abstract = {A relationship is established between (i) parallel random-access machines that allow many processors to concurrently read from or write into a common memory including simultaneous reading or writing into the same memory location (CRCW PRAM), and (ii) combinational logic circuits that contain AND's, OR's and NOT's, with no bound placed on the fan-in of AND-gates and OR-gates. Parallel time and number of processors for CRCW PRAM's are shown to correspond respectively (and simultaneously) to depth and size for circuits, where the time-depth correspondence is to within a constant factor and the processors-size correspondence is to within a polynomial. By applying a recent result of Furst, Saxe and Sipser, we obtain the corollary that parity, integer multiplication, graph transitive closure and integer sorting cannot be computed in constant time by a CRCW PRAM with a polynomial number of processors. This is the first nonconstant lower bound on the parallel time required to solve these problems by a CRCW PRAM with a polynomial number of processors. We also state and outline the proof of a similar result, due to W. L. Ruzzo and M. Tompa, that relates time and processor bounds for CRCW PRAM's to alternation and space bounds for alternating Turing machines.},
  language = {en},
  number = {2},
  journal = {SIAM Journal on Computing},
  author = {Stockmeyer, Larry and Vishkin, Uzi},
  month = may,
  year = {1984},
  pages = {409-422},
  file = {/Users/billlai/Zotero/storage/MTUN4L7L/Stockmeyer and Vishkin - 1984 - Simulation of Parallel Random Access Machines by C.pdf}
}

@article{GilyenQuantumsingularvalue2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.01838},
  primaryClass = {quant-ph},
  title = {Quantum Singular Value Transformation and beyond: Exponential Improvements for Quantum Matrix Arithmetics},
  shorttitle = {Quantum Singular Value Transformation and Beyond},
  abstract = {Quantum computing is powerful because unitary operators describing the time-evolution of a quantum system have exponential size in terms of the number of qubits present in the system. We develop a new "Singular value transformation" algorithm capable of harnessing this exponential advantage, that can apply polynomial transformations to the singular values of a block of a unitary, generalizing the optimal Hamiltonian simulation results of Low and Chuang. The proposed quantum circuits have a very simple structure, often give rise to optimal algorithms and have appealing constant factors, while usually only use a constant number of ancilla qubits. We show that singular value transformation leads to novel algorithms. We give an efficient solution to a certain "non-commutative" measurement problem and propose a new method for singular value estimation. We also show how to exponentially improve the complexity of implementing fractional queries to unitaries with a gapped spectrum. Finally, as a quantum machine learning application we show how to efficiently implement principal component regression. "Singular value transformation" is conceptually simple and efficient, and leads to a unified framework of quantum algorithms incorporating a variety of quantum speed-ups. We illustrate this by showing how it generalizes a number of prominent quantum algorithms, including: optimal Hamiltonian simulation, implementing the Moore-Penrose pseudoinverse with exponential precision, fixed-point amplitude amplification, robust oblivious amplitude amplification, fast QMA amplification, fast quantum OR lemma, certain quantum walk results and several quantum machine learning algorithms. In order to exploit the strengths of the presented method it is useful to know its limitations too, therefore we also prove a lower bound on the efficiency of singular value transformation, which often gives optimal bounds.},
  journal = {arXiv:1806.01838 [quant-ph]},
  author = {Gily\'en, Andr\'as and Su, Yuan and Low, Guang Hao and Wiebe, Nathan},
  month = jun,
  year = {2018},
  keywords = {Quantum Physics,Computer Science - Emerging Technologies},
  file = {/Users/billlai/Zotero/storage/EFS8XHW9/Gilyén et al. - 2018 - Quantum singular value transformation and beyond .pdf;/Users/billlai/Zotero/storage/NUR5ALA6/1806.html}
}

@article{MoserConstructiveProofGeneral2010,
  title = {A {{Constructive Proof}} of the {{General Lov\'ASz Local Lemma}}},
  volume = {57},
  issn = {0004-5411},
  doi = {10.1145/1667053.1667060},
  abstract = {The Lov\'asz Local Lemma discovered by Erd{\H o}s and Lov\'asz in 1975 is a powerful tool to non-constructively prove the existence of combinatorial objects meeting a prescribed collection of criteria. In 1991, J\'ozsef Beck was the first to demonstrate that a constructive variant can be given under certain more restrictive conditions, starting a whole line of research aimed at improving his algorithm's performance and relaxing its restrictions. In the present article, we improve upon recent findings so as to provide a method for making almost all known applications of the general Local Lemma algorithmic.},
  number = {2},
  journal = {J. ACM},
  author = {Moser, Robin A. and Tardos, G\'abor},
  month = feb,
  year = {2010},
  keywords = {Constructive proof,Lovász local lemma,parallelization},
  pages = {11:1--11:15},
  file = {/Users/billlai/Zotero/storage/PSK9499C/Moser and Tardos - 2010 - A Constructive Proof of the General LovÁSz Local L.pdf}
}

@book{VaziraniApproximationAlgorithms2003,
  address = {Berlin Heidelberg},
  title = {Approximation {{Algorithms}}},
  isbn = {978-3-540-65367-7},
  abstract = {This book covers the dominant theoretical approaches to the approximate solution of hard combinatorial optimization and enumeration problems. It contains elegant combinatorial theory, useful and interesting algorithms, and deep results about the intrinsic complexity of combinatorial problems. Its clarity of exposition and excellent selection of exercises will make it accessible and appealing to all those with a taste for mathematics and algorithms. Richard Karp,University Professor, University of California at Berkeley Following the development of basic combinatorial optimization techniques in the 1960s and 1970s, a main open question was to develop a theory of approximation algorithms. In the 1990s, parallel developments in techniques for designing approximation algorithms as well as methods for proving hardness of approximation results have led to a beautiful theory. The need to solve truly large instances of computationally hard problems, such as those arising from the Internet or the human genome project, has also increased interest in this theory. The field is currently very active, with the toolbox of approximation algorithm design techniques getting always richer. It is a pleasure to recommend Vijay Vazirani's well-written and comprehensive book on this important and timely topic. I am sure the reader will find it most useful both as an introduction to approximability as well as a reference to the many aspects of approximation algorithms. L\'aszl\'o Lov\'asz, Senior Researcher, Microsoft Research},
  language = {en},
  publisher = {{Springer-Verlag}},
  author = {Vazirani, Vijay V.},
  year = {2003},
  file = {/Users/billlai/Zotero/storage/46ZRFCA3/Approximation Algorithms.pdf;/Users/billlai/Zotero/storage/86VHAA28/9783540653677.html}
}

@book{VaziraniApproximationAlgorithms2001,
  address = {Berlin, Heidelberg},
  title = {Approximation {{Algorithms}}},
  isbn = {978-3-540-65367-7},
  abstract = {This book will be of interest to the scientific community at large and, in particular, to students and researchers in Computer Science, Operation Research, and Discrete Mathematics. It can be used both as a text in a graduate course on approximation algorithms and as a supplementary text in basic undergraduate and graduate courses on algorithms.},
  publisher = {{Springer-Verlag}},
  author = {Vazirani, Vijay V.},
  year = {2001}
}

@article{AmbainisQuantumLovASzLocal2012,
  title = {A {{Quantum Lov\'ASz Local Lemma}}},
  volume = {59},
  issn = {0004-5411},
  doi = {10.1145/2371656.2371659},
  abstract = {The Lov\'asz Local Lemma (LLL) is a powerful tool in probability theory to show the existence of combinatorial objects meeting a prescribed collection of ``weakly dependent'' criteria. We show that the LLL extends to a much more general geometric setting, where events are replaced with subspaces and probability is replaced with relative dimension, which allows to lower bound the dimension of the intersection of vector spaces under certain independence conditions. Our result immediately applies to the k-qsat problem (quantum analog of k-sat): For instance we show that any collection of rank-1 projectors, with the property that each qubit appears in at most 2k/(e \.{c} k) of them, has a joint satisfiable state. We then apply our results to the recently studied model of random k-qsat. Recent works have shown that the satisfiable region extends up to a density of 1 in the large k limit, where the density is the ratio of projectors to qubits. Using a hybrid approach building on work by Laumann et al. [2009, 2010] we greatly extend the known satisfiable region for random k-qsat to a density of $\Omega$(2k/k2). Since our tool allows us to show the existence of joint satisfying states without the need to construct them, we are able to penetrate into regions where the satisfying states are conjectured to be entangled, avoiding the need to construct them, which has limited previous approaches to product states.},
  number = {5},
  journal = {J. ACM},
  author = {Ambainis, Andris and Kempe, Julia and Sattath, Or},
  month = nov,
  year = {2012},
  keywords = {Local lemma,probabilistic method,quantum computation,quanum SAT,random quantum SAT},
  pages = {24:1--24:24},
  file = {/Users/billlai/Zotero/storage/RQGEGTU2/Ambainis et al. - 2012 - A Quantum LovÁSz Local Lemma.pdf}
}

@inproceedings{CharikarNearoptimalAlgorithmsMaximum2007,
  address = {Philadelphia, PA, USA},
  series = {SODA '07},
  title = {Near-Optimal {{Algorithms}} for {{Maximum Constraint Satisfaction Problems}}},
  isbn = {978-0-89871-624-5},
  abstract = {In this paper we present approximation algorithms for the maximum constraint satisfaction problem with k variables in each constraint (MAX k-CSP). Given a (1 - $\epsilon$) satisfiable 2CSP our first algorithm finds an assignment of variables satisfying a 1 - O($\surd\epsilon$) fraction of all constraints. The best previously known result, due to Zwick, was 1 - O($\epsilon$1/3). The second algorithm finds a ck/2k approximation for the MAX k-CSP problem (where c $>$ 0.44 is an absolute constant). This result improves the previously best known algorithm by Hast, which had an approximation guarantee of $\Omega$(k/(2klog k)). Both results are optimal assuming the Unique Games Conjecture and are based on rounding natural semidefinite programming relaxations. We also believe that our algorithms and their analysis are simpler than those previously known.},
  booktitle = {Proceedings of the {{Eighteenth Annual ACM}}-{{SIAM Symposium}} on {{Discrete Algorithms}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Charikar, Moses and Makarychev, Konstantin and Makarychev, Yury},
  year = {2007},
  pages = {62--68},
  file = {/Users/billlai/Zotero/storage/KXNXC9M5/Charikar et al. - 2007 - Near-optimal Algorithms for Maximum Constraint Sat.pdf}
}

@article{AradLinearTimeAlgorithm2016,
  title = {Linear {{Time Algorithm}} for {{Quantum 2SAT}}},
  doi = {10.4230/lipics.icalp.2016.15},
  abstract = {A canonical result about satisfiability theory is that the 2-SAT problem can be solved in linear time, despite the NP-hardness of the 3-SAT problem. In the quantum 2-SAT problem, we are given a family of 2-qubit projectors Qij on a system of n qubits, and the task is to decide whether the Hamiltonian H = Qij has a 0-eigenvalue, or it is larger than 1/nc for some c = O(1). The problem is not only a natural extension of the classical 2-SAT problem to the quantum case, but is also equivalent to the problem of finding the ground state of 2-local frustration-free Hamiltonians of spin 1/2, a well-studied model believed to capture certain key properties in modern condensed matter physics. While Bravyi has shown that the quantum 2-SAT problem has a classical polynomial-time algorithm, the running time of his algorithm is O(n4). In this paper we give a classical algorithm with linear running time in the number of local projectors, therefore achieving the best possible complexity.},
  language = {en},
  author = {Arad, Itai and Santha, Miklos and Sundaram, Aarthi and Zhang, Shengyu},
  editor = {Herbstritt, Marc},
  year = {2016},
  file = {/Users/billlai/Zotero/storage/QSIDIW63/Arad et al. - 2016 - Linear Time Algorithm for Quantum 2SAT.pdf}
}

@article{FruendIntroductionSemidefiniteProgramming,
  title = {Introduction to {{Semidefinite Programming}}},
  language = {en},
  author = {Fruend, Robert},
  pages = {51},
  file = {/Users/billlai/Zotero/storage/TW5FQQB8/Fruend - Introduction to Semidefinite Programming.pdf}
}

@article{Barrus1uniquenessdensecritical2018,
  title = {On 1-Uniqueness and Dense Critical Graphs for Tree-Depth},
  volume = {341},
  issn = {0012-365X},
  doi = {10.1016/j.disc.2018.03.026},
  abstract = {The tree-depth of G is the smallest value of k for which a labeling of the vertices of G with elements from \{1,\ldots,k\} exists such that any path joining two vertices with the same label contains a vertex having a higher label. The graph G is k-critical if it has tree-depth k and every proper minor of G has smaller tree-depth. Motivated by a conjecture on the maximum degree of k-critical graphs, we consider the property of 1-uniqueness, wherein any vertex of a critical graph can be the unique vertex receiving label 1 in an optimal labeling. Contrary to an earlier conjecture, we construct examples of critical graphs that are not 1-unique and show that 1-unique graphs can have arbitrarily many more edges than certain critical spanning subgraphs. We also show that (n-1)-critical graphs on n vertices are 1-unique and use 1-uniqueness to show that the Andr\'asfai graphs are critical with respect to tree-depth.},
  number = {7},
  journal = {Discrete Mathematics},
  author = {Barrus, Michael D. and Sinkovic, John},
  month = jul,
  year = {2018},
  keywords = {Andrásfai graph,Graph minor,Tree-depth,Vertex ranking},
  pages = {1973-1982},
  file = {/Users/billlai/Zotero/storage/VIMF4NRE/S0012365X18300979.html}
}

@article{FellowsFPTCharacterizedUseful2014,
  title = {{{FPT}} Is {{Characterized}} by {{Useful Obstruction Sets}}: {{Connecting Algorithms}}, {{Kernels}}, and {{Quasi}}-Orders},
  volume = {6},
  issn = {1942-3454},
  shorttitle = {{{FPT}} Is {{Characterized}} by {{Useful Obstruction Sets}}},
  doi = {10.1145/2635820},
  abstract = {Many graph problems were first shown to be fixed-parameter tractable using the results of Robertson and Seymour on graph minors. We show that the combination of finite, computable obstruction sets and efficient order tests is not just one way of obtaining strongly uniform FPT algorithms, but that all of FPT may be captured in this way. Our new characterization of FPT has a strong connection to the theory of kernelization, as we prove that problems with polynomial kernels can be characterized by obstruction sets whose elements have polynomial size. Consequently we investigate the interplay between the sizes of problem kernels and the sizes of the elements of such obstruction sets, obtaining several examples of how results in one area yield new insights in the other. We show how exponential-size minor-minimal obstructions for pathwidth k form the crucial ingredient in a novel or-cross-composition for k-Pathwidth, complementing the trivial and-composition that is known for this problem. In the other direction, we show that or-cross-compositions into a parameterized problem can be used to rule out the existence of efficiently generated quasi-orders on its instances that characterize the no-instances by polynomial-size obstructions.},
  number = {4},
  journal = {ACM Trans. Comput. Theory},
  author = {Fellows, Michael R. and Jansen, Bart M. P.},
  month = aug,
  year = {2014},
  keywords = {pathwidth,parameterized complexity,Kernelization,well-quasi-ordering},
  pages = {16:1--16:26},
  file = {/Users/billlai/Zotero/storage/74HFYWMQ/Fellows and Jansen - 2014 - FPT is Characterized by Useful Obstruction Sets C.pdf}
}

@article{BarrusClassescriticalgraphs2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.05277},
  primaryClass = {math},
  title = {Classes of Critical Graphs for Tree-Depth},
  abstract = {A k-ranking of a graph G is a labeling of the vertices of G with values from \{1,...,k\} such that any path joining two vertices with the same label contains a vertex having a higher label. The tree-depth of G is the smallest value of k for which a k-ranking of G exists. The graph G is k-critical if it has tree-depth k and any proper minor of G has smaller tree-depth, and it is 1-unique if for every vertex v in G, there exists an optimal ranking of G in which v is the unique vertex with label 1. We present several classes of graphs that are both k-critical and 1-unique, providing examples that satisfy conjectures on critical graphs discussed in [M.D. Barrus and J. Sinkovic, Uniqueness and minimal obstructions for tree-depth, submitted].},
  journal = {arXiv:1502.05277 [math]},
  author = {Barrus, Michael D. and Sinkovic, John},
  month = feb,
  year = {2015},
  keywords = {Mathematics - Combinatorics},
  file = {/Users/billlai/Zotero/storage/H7NRP66T/Barrus and Sinkovic - 2015 - Classes of critical graphs for tree-depth.pdf;/Users/billlai/Zotero/storage/PZ5Z4V7V/1502.html}
}

@inproceedings{KawarabayashiPolynomialExcludedminorApproximation2018,
  address = {Philadelphia, PA, USA},
  series = {SODA '18},
  title = {A {{Polynomial Excluded}}-Minor {{Approximation}} of {{Treedepth}}},
  isbn = {978-1-61197-503-1},
  abstract = {Treedepth is a well-studied graph invariant in the family of "width measures" that includes treewidth and pathwidth. Understanding these invariants in terms of excluded minors has been an active area of research. The recent Grid Minor Theorem of Chekuri and Chuzhoy [12] establishes that treewidth is polynomially approximated by the largest k x k grid minor. In this paper, we give a similar polynomial excluded-minor approximation for treedepth in terms of three basic obstructions: grids, tree, and paths. Specifically, we show that there is a constant c such that every graph of treedepth $\geq$ kc contains one of the following minors (each of treedepth $\geq$ k): \textbullet{} the k x k grid, \textbullet{} the complete binary tree of height k, \textbullet{} the path of order 2k. Let us point out that we cannot drop any of the above graphs for our purpose. Moreover, given a graph G we can, in randomized polynomial time, find either an embedding of one of these minors or conclude that treedepth of G is at most kc. This result has potential applications in a variety of settings where bounded treedepth plays a role. In addition to some graph structural applications, we describe a surprising application in circuit complexity and finite model theory from recent work of the second author [28].},
  booktitle = {Proceedings of the {{Twenty}}-{{Ninth Annual ACM}}-{{SIAM Symposium}} on {{Discrete Algorithms}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Kawarabayashi, Ken-ichi and Rossman, Benjamin},
  year = {2018},
  pages = {234--246},
  file = {/Users/billlai/Zotero/storage/CCJZ2KFE/Kawarabayashi and Rossman - 2018 - A Polynomial Excluded-minor Approximation of Treed.pdf}
}

@article{Barrus1uniquenessdensecritical2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.07311},
  primaryClass = {math},
  title = {On 1-Uniqueness and Dense Critical Graphs for Tree-Depth},
  abstract = {The tree-depth of \$G\$ is the smallest value of \$k\$ for which a labeling of the vertices of \$G\$ with elements from \$$\backslash$\{1,$\backslash$dots,k$\backslash$\}\$ exists such that any path joining two vertices with the same label contains a vertex having a higher label. The graph \$G\$ is \$k\$-critical if it has tree-depth \$k\$ and every proper minor of \$G\$ has smaller tree-depth. Motivated by a conjecture on the maximum degree of \$k\$-critical graphs, we consider the property of 1-uniqueness, wherein any vertex of a critical graph can be the unique vertex receiving label 1 in an optimal labeling. Contrary to an earlier conjecture, we construct examples of critical graphs that are not 1-unique and show that 1-unique graphs can have arbitrarily many more edges than certain critical spanning subgraphs. We also show that \$(n-1)\$-critical graphs are 1-unique and use 1-uniqueness to show that the Andr$\backslash$'\{a\}sfai graphs are critical with respect to tree-depth.},
  journal = {arXiv:1704.07311 [math]},
  author = {Barrus, Michael D. and Sinkovic, John},
  month = apr,
  year = {2017},
  keywords = {Mathematics - Combinatorics,05C75 (05C15; 05C78; 05C83)},
  file = {/Users/billlai/Zotero/storage/3T7QZ8DL/Barrus and Sinkovic - 2017 - On 1-uniqueness and dense critical graphs for tree.pdf;/Users/billlai/Zotero/storage/Q9GGWWVY/1704.html}
}

@inproceedings{MoserConstructiveProofLovaSz2009,
  address = {New York, NY, USA},
  series = {STOC '09},
  title = {A {{Constructive Proof}} of the {{Lov\'aSz Local Lemma}}},
  isbn = {978-1-60558-506-2},
  doi = {10.1145/1536414.1536462},
  abstract = {The Lovasz Local Lemma [2] is a powerful tool to prove the existence of combinatorial objects meeting a prescribed collection of criteria. The technique can directly be applied to the satisfiability problem, yielding that a k-CNF formula in which each clause has common variables with at most 2(k-2) other clauses is always satisfiable. All hitherto known proofs of the Local Lemma are non-constructive and do thus not provide a recipe as to how a satisfying assignment to such a formula can be efficiently found. In his breakthrough paper [3], Beck demonstrated that if the neighbourhood of each clause be restricted to O(2(k/48)), a polynomial time algorithm for the search problem exists. Alon simplified and randomized his procedure and improved the bound to O(2(k/8)) [4]. Srinivasan presented in [9] a variant that achieves a bound of essentially O(2(k/4)). In [11], we improved this to O(2(k/2)). In the present paper, we give a randomized algorithm that finds a satisfying assignment to every k-CNF formula in which each clause has a neighbourhood of at most the asymptotic optimum of 2(k-5)-1 other clauses and that runs in expected time polynomial in the size of the formula, irrespective of k. If k is considered a constant, we can also give a deterministic variant. In contrast to all previous approaches, our analysis does not anymore invoke the standard non-constructive versions of the Local Lemma and can therefore be considered an alternative, constructive proof of it.},
  booktitle = {Proceedings of the {{Forty}}-First {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Moser, Robin A.},
  year = {2009},
  keywords = {derandomization,bounded occurrence sat instances,hypergraph colouring,Lovasz local lemma},
  pages = {343--350},
  file = {/Users/billlai/Zotero/storage/EQUXY3YV/Moser - 2009 - A Constructive Proof of the LováSz Local Lemma.pdf}
}

@article{KratochvilOneMoreOccurrence1993,
  title = {One {{More Occurrence}} of {{Variables Makes Satisfiability Jump}} from {{Trivial}} to {{NP}}-{{Complete}}},
  volume = {22},
  issn = {0097-5397},
  doi = {10.1137/0222015},
  abstract = {A Boolean formula in a conjunctive normal form is called a \$(k,s)\$ \textendash{} formula if every clause contains exactly k variables and every variable occurs in at most s clauses. The \$(k,s)\$\textendash\$\{$\backslash$text\{SAT\}\}\$ problem is the SATISFIABILITY problem restricted to \$(k,s)\$\textendash{}formulas. It is proved that for every \$k $\backslash$geqslant 3\$ there is an integer \$f(k)\$ such that \$(k,s)\$\textendash\$\{$\backslash$text\{SAT\}\}\$ is trivial for \$s $\backslash$leqslant f(k)\$ (because every \$(k,s)\$\textendash{}formula is satisfiable) and is NP-complete for \$s $\backslash$geqslant f(k) + 1\$. Moreover, \$f(k)\$ grows exponentially with k, namely, \$$\backslash$lfloor \{\{\{2\^k \} / \{ek\}\}\} $\backslash$rfloor  $\backslash$leqslant f(k) $\backslash$leqslant 2\^\{k - 1\}  - 2\^\{k - 4\}  - 1\$ for \$k $\backslash$geqslant 4\$.},
  number = {1},
  journal = {SIAM Journal on Computing},
  author = {Kratochv\'il, J. and Savick\'y, P. and Tuza, Z.},
  month = feb,
  year = {1993},
  pages = {203-210},
  file = {/Users/billlai/Zotero/storage/U5CMZJK9/Kratochvíl et al. - 1993 - One More Occurrence of Variables Makes Satisfiabil.pdf;/Users/billlai/Zotero/storage/SLKHW524/0222015.html}
}

@incollection{GebauerLovaszLocalLemma2009,
  address = {Berlin, Heidelberg},
  title = {The {{Lov\'asz Local Lemma}} and {{Satisfiability}}},
  volume = {5760},
  isbn = {978-3-642-03455-8 978-3-642-03456-5},
  abstract = {We consider boolean formulas in conjunctive normal form (CNF). If all clauses are large, it needs many clauses to obtain an unsatisfiable formula; moreover, these clauses have to interleave. We review quantitative results for the amount of interleaving required, many of which rely on the Lov\'asz Local Lemma, a probabilistic lemma with many applications in combinatorics.},
  language = {en},
  booktitle = {Efficient {{Algorithms}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Gebauer, Heidi and Moser, Robin A. and Scheder, Dominik and Welzl, Emo},
  editor = {Albers, Susanne and Alt, Helmut and N\"aher, Stefan},
  year = {2009},
  pages = {30-54},
  file = {/Users/billlai/Zotero/storage/H6ZDBBRP/Gebauer et al. - 2009 - The Lovász Local Lemma and Satisfiability.pdf},
  doi = {10.1007/978-3-642-03456-5_3}
}

@article{HIRSCHSATLocalSearch,
  title = {{{SAT Local Search Algorithms}}: {{Worst}}-{{Case Study}}},
  abstract = {Recent experiments demonstrated that local search algorithms (e.g. GSAT) are able to find satisfying assignments for many ``hard'' Boolean formulas. A wide experimental study of these algorithms demonstrated their good performance on some inportant classes of formulas as well as poor performance on some other ones. In contrast, theoretical knowledge of their worst-case behavior is very limited. However, many worst-case upper and lower bounds of the form 2$\alpha$n ($\alpha$ $<$ 1 is a constant) are known for other SAT algorithms, for example, resolution-like algorithms. In the present paper we prove both upper and lower bounds of this form for local search algorithms. The class of linear-size formulas we consider for the upper bound covers most of the DIMACS benchmarks; the satisfiability problem for this class of formulas is N P -complete.},
  language = {en},
  author = {HIRSCH, EDWARD A},
  pages = {17},
  file = {/Users/billlai/Zotero/storage/JKKBLD87/HIRSCH - SAT Local Search Algorithms Worst-Case Study.pdf}
}

@article{AlekhnovichExponentialLowerBounds2006,
  title = {Exponential {{Lower Bounds}} for the {{Running Time}} of {{DPLL Algorithms}} on {{Satisfiable Formulas}}},
  volume = {35},
  issn = {0168-7433, 1573-0670},
  doi = {10.1007/s10817-005-9006-x},
  abstract = {DPLL (for Davis, Putnam, Logemann, and Loveland) algorithms form the largest family of contemporary algorithms for SAT (the propositional satisfiability problem) and are widely used in applications. The recursion trees of DPLL algorithm executions on unsatisfiable formulas are equivalent to treelike resolution proofs. Therefore, lower bounds for treelike resolution (known since the 1960s) apply to them. However, these lower bounds say nothing about the behavior of such algorithms on satisfiable formulas. Proving exponential lower bounds for them in the most general setting is impossible without proving P m NP; therefore, to prove lower bounds, one has to restrict the power of branching heuristics. In this paper, we give exponential lower bounds for two families of DPLL algorithms: generalized myopic algorithms, which read up to n1j of clauses at each step and see the remaining part of the formula without negations, and drunk algorithms, which choose a variable using any complicated rule and then pick its value at random.},
  language = {en},
  number = {1-3},
  journal = {Journal of Automated Reasoning},
  author = {Alekhnovich, Michael and Hirsch, Edward A. and Itsykson, Dmitry},
  month = oct,
  year = {2006},
  pages = {51-72},
  file = {/Users/billlai/Zotero/storage/62URA5FE/Alekhnovich et al. - 2006 - Exponential Lower Bounds for the Running Time of D.pdf}
}

@article{NikolenkoHardsatisfiableformulas2003,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {cs/0301012},
  title = {Hard Satisfiable Formulas for {{DPLL}}-Type Algorithms},
  abstract = {We address lower bounds on the time complexity of algorithms solving the propositional satisfiability problem. Namely, we consider two DPLL-type algorithms, enhanced with the unit clause and pure literal heuristics. Exponential lower bounds for solving satisfiability on provably satisfiable formulas are proven.},
  journal = {arXiv:cs/0301012},
  author = {Nikolenko, Sergey I.},
  month = jan,
  year = {2003},
  keywords = {Computer Science - Computational Complexity,F.2.2},
  file = {/Users/billlai/Zotero/storage/M9JFD3ZA/0301012.html}
}

@article{NikolenkoHardsatisfiableformulas2003a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {cs/0301012},
  title = {Hard Satisfiable Formulas for {{DPLL}}-Type Algorithms},
  abstract = {We address lower bounds on the time complexity of algorithms solving the propositional satisfiability problem. Namely, we consider two DPLL-type algorithms, enhanced with the unit clause and pure literal heuristics. Exponential lower bounds for solving satisfiability on provably satisfiable formulas are proven.},
  journal = {arXiv:cs/0301012},
  author = {Nikolenko, Sergey I.},
  month = jan,
  year = {2003},
  keywords = {Computer Science - Computational Complexity,F.2.2},
  file = {/Users/billlai/Zotero/storage/FSWXC9YK/Nikolenko - 2003 - Hard satisfiable formulas for DPLL-type algorithms.pdf;/Users/billlai/Zotero/storage/YQYV9X73/0301012.html}
}

@article{BookatzQMAcompleteproblems2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1212.6312},
  primaryClass = {quant-ph},
  title = {{{QMA}}-Complete Problems},
  abstract = {In this paper we give an overview of the quantum computational complexity class QMA and a description of known QMA-complete problems to date. Such problems are believed to be difficult to solve, even with a quantum computer, but have the property that if a purported solution to the problem is given, a quantum computer would easily be able to verify whether it is correct. An attempt has been made to make this paper as self-contained as possible so that it can be accessible to computer scientists, physicists, mathematicians, and quantum chemists. Problems of interest to all of these professions can be found here.},
  journal = {arXiv:1212.6312 [quant-ph]},
  author = {Bookatz, Adam D.},
  month = dec,
  year = {2012},
  keywords = {Quantum Physics},
  file = {/Users/billlai/Zotero/storage/JDGG485R/Bookatz - 2012 - QMA-complete problems.pdf;/Users/billlai/Zotero/storage/Y3P8RIZA/1212.html}
}

@article{GharibianQuantumHamiltonianComplexity2015,
  title = {Quantum {{Hamiltonian Complexity}}},
  volume = {10},
  issn = {1551-305X, 1551-3068},
  doi = {10.1561/0400000066},
  language = {en},
  number = {3},
  journal = {Foundations and Trends\textregistered{} in Theoretical Computer Science},
  author = {Gharibian, Sevag and Huang, Yichen and Landau, Zeph and Shin, Seung Woo},
  year = {2015},
  pages = {159-282},
  file = {/Users/billlai/Zotero/storage/Z8RMMDQD/Gharibian et al. - 2015 - Quantum Hamiltonian Complexity.pdf}
}

@inproceedings{ZwickFindingAlmostsatisfyingAssignments1998,
  address = {New York, NY, USA},
  series = {STOC '98},
  title = {Finding {{Almost}}-Satisfying {{Assignments}}},
  isbn = {978-0-89791-962-3},
  doi = {10.1145/276698.276869},
  booktitle = {Proceedings of the {{Thirtieth Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Zwick, Uri},
  year = {1998},
  pages = {551--560},
  file = {/Users/billlai/Zotero/storage/DYBRYGHE/Zwick - 1998 - Finding Almost-satisfying Assignments.pdf}
}

@inproceedings{ZwickFindingalmostsatisfyingassignments1998,
  title = {Finding Almost-Satisfying Assignments},
  isbn = {978-0-89791-962-3},
  doi = {10.1145/276698.276869},
  language = {en},
  publisher = {{ACM Press}},
  author = {Zwick, Uri},
  year = {1998},
  pages = {551-560},
  file = {/Users/billlai/Zotero/storage/IY2MI9RJ/Zwick - 1998 - Finding almost-satisfying assignments.pdf}
}

@inproceedings{KolipakaMoserTardosMeet2011,
  address = {New York, NY, USA},
  series = {STOC '11},
  title = {Moser and {{Tardos Meet Lov\'aSz}}},
  isbn = {978-1-4503-0691-1},
  doi = {10.1145/1993636.1993669},
  abstract = {Beck's early work [3] gave an efficient version of the Lov\'asz Local Lemma(LLL) with significant compromise in the parameters. Following several improvements [1,7,4,13], Moser [8], and Moser and Tardos [9] obtained asymptotically optimal results in terms of the maximal degree. For a fixed dependency graph G the exact criterion under which LLL applies is given by Shearer in [12]. For a dependency structure G, let LO(G) be the set of those probability assignments to the nodes of G for which the Lov\'asz Local Lemma holds. We show that: Both the sequential and parallel ersions of the Moser-Tardos algorithm are efficient up to the Shearer's bound, by giving a tighter analysis. We also prove that, whenever p $\in$ LO(G)/(1+$\epsilon$), the expected running times of the sequential and parallel versions are at most n/$\epsilon$ and O(1/$\epsilon$ log n/$\epsilon$), the later when $\epsilon$ $<$ 1. Here n is the number of nodes in G. By adding a few lines to our analysis we can reprove Shearer's result (for the general LLL). Our alternative proof for the Shearer's bound not only highlights the connection between the variable and general versions of LLL, but also illustrates that variants of the Moser-Tardos algorithm can be useful in existence proofs. We obtain new formulas for phase transitions in the hardcore lattice gas model, non-trivially equivalent to the ones studied by Scott and Sokal [10]. We prove that if p $\in$ LO(G)/(1+$\epsilon$), the running time of the Moser-Tardos algorithm is polynomial not only in the number of events, but also in the number of variables. This extends one of the results from the more recent work of Haeupler, Saha, and Srinivasan [6]. Our new formulas immediately give a majorizing lemma that connects LLL bounds on different graphs. We show that the LLL bound for the (special case of the) variable version is sometimes larger than for the general version. This is the first known separation between the variable and the general versions of LLL.},
  booktitle = {Proceedings of the {{Forty}}-Third {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Kolipaka, Kashyap Babu Rao and Szegedy, Mario},
  year = {2011},
  keywords = {Lovász local lemma,hardcore lattice gas,independent set polynomial,matrix iterative analysis},
  pages = {235--244},
  file = {/Users/billlai/Zotero/storage/G55UPFXM/Kolipaka and Szegedy - 2011 - Moser and Tardos Meet LováSz.pdf}
}

@incollection{SzegedyLovaszLocalLemma2013,
  address = {Berlin, Heidelberg},
  title = {The {{Lov\'asz Local Lemma}} \textendash{} {{A Survey}}},
  volume = {7913},
  isbn = {978-3-642-38535-3 978-3-642-38536-0},
  booktitle = {Computer {{Science}} \textendash{} {{Theory}} and {{Applications}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Szegedy, Mario},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Bulatov, Andrei A. and Shur, Arseny M.},
  year = {2013},
  pages = {1-11},
  file = {/Users/billlai/Zotero/storage/TRJU5D7U/Szegedy - 2013 - The Lovász Local Lemma – A Survey.pdf},
  doi = {10.1007/978-3-642-38536-0_1}
}

@article{Pachnumberrootedtrees2009,
  title = {The Number of Rooted Trees of given Depth},
  language = {en},
  author = {Pach, Peter Pal and Pluhar, Gabriella and Pongracz, Andras and Szabo, Csaba},
  year = {2009},
  pages = {11},
  file = {/Users/billlai/Zotero/storage/YPSBDFH8/Pach et al. - 2009 - The number of rooted trees of given depth.pdf}
}

@article{GharibianQuantumHamiltonianComplexity2015a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.3916},
  title = {Quantum {{Hamiltonian Complexity}}},
  volume = {10},
  issn = {1551-305X, 1551-3068},
  doi = {10.1561/0400000066},
  abstract = {Constraint satisfaction problems are a central pillar of modern computational complexity theory. This survey provides an introduction to the rapidly growing field of Quantum Hamiltonian Complexity, which includes the study of quantum constraint satisfaction problems. Over the past decade and a half, this field has witnessed fundamental breakthroughs, ranging from the establishment of a "Quantum Cook-Levin Theorem" to deep insights into the structure of 1D low-temperature quantum systems via so-called area laws. Our aim here is to provide a computer science-oriented introduction to the subject in order to help bridge the language barrier between computer scientists and physicists in the field. As such, we include the following in this survey: (1) The motivations and history of the field, (2) a glossary of condensed matter physics terms explained in computer-science friendly language, (3) overviews of central ideas from condensed matter physics, such as indistinguishable particles, mean field theory, tensor networks, and area laws, and (4) brief expositions of selected computer science-based results in the area. For example, as part of the latter, we provide a novel information theoretic presentation of Bravyi's polynomial time algorithm for Quantum 2-SAT.},
  number = {3},
  journal = {Foundations and Trends\textregistered{} in Theoretical Computer Science},
  author = {Gharibian, Sevag and Huang, Yichen and Landau, Zeph and Shin, Seung Woo},
  year = {2015},
  keywords = {Computer Science - Computational Complexity,Quantum Physics,Condensed Matter - Strongly Correlated Electrons},
  pages = {159-282},
  file = {/Users/billlai/Zotero/storage/T48GC8NU/Gharibian et al. - 2015 - Quantum Hamiltonian Complexity.pdf;/Users/billlai/Zotero/storage/VWAIXG33/1401.html}
}

@book{Kayeintroductionquantumcomputing2007,
  address = {Oxford},
  title = {An Introduction to Quantum Computing},
  isbn = {978-0-19-857000-4 978-0-19-857049-3},
  language = {en},
  publisher = {{Oxford Univ. Press}},
  author = {Kaye, Phillip and Laflamme, Raymond and Mosca, Michele},
  year = {2007},
  file = {/Users/billlai/Zotero/storage/QMDILY3T/Kaye et al. - 2007 - An introduction to quantum computing.pdf},
  note = {OCLC: 255412511}
}

@article{AradLinearTimeAlgorithm2016a,
  title = {Linear {{Time Algorithm}} for {{Quantum 2SAT}}},
  doi = {10.4230/lipics.icalp.2016.15},
  abstract = {A canonical result about satisfiability theory is that the 2-SAT problem can be solved in linear time, despite the NP-hardness of the 3-SAT problem. In the quantum 2-SAT problem, we are given a family of 2-qubit projectors Qij on a system of n qubits, and the task is to decide whether the Hamiltonian H = Qij has a 0-eigenvalue, or it is larger than 1/nc for some c = O(1). The problem is not only a natural extension of the classical 2-SAT problem to the quantum case, but is also equivalent to the problem of finding the ground state of 2-local frustration-free Hamiltonians of spin 1/2, a well-studied model believed to capture certain key properties in modern condensed matter physics. While Bravyi has shown that the quantum 2-SAT problem has a classical polynomial-time algorithm, the running time of his algorithm is O(n4). In this paper we give a classical algorithm with linear running time in the number of local projectors, therefore achieving the best possible complexity.},
  language = {en},
  author = {Arad, Itai and Santha, Miklos and Sundaram, Aarthi and Zhang, Shengyu},
  editor = {Herbstritt, Marc},
  year = {2016},
  file = {/Users/billlai/Zotero/storage/KQ2G5ZQ2/Arad et al. - 2016 - Linear Time Algorithm for Quantum 2SAT.pdf}
}

@article{AradLineartimealgorithm2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1508.06340},
  primaryClass = {quant-ph},
  title = {Linear Time Algorithm for Quantum {{2SAT}}},
  abstract = {A canonical result about satisfiability theory is that the 2-SAT problem can be solved in linear time, despite the NP-hardness of the 3-SAT problem. In the quantum 2-SAT problem, we are given a family of 2-qubit projectors \$$\backslash$Pi\_\{ij\}\$ on a system of \$n\$ qubits, and the task is to decide whether the Hamiltonian \$H=$\backslash$sum $\backslash$Pi\_\{ij\}\$ has a 0-eigenvalue, or it is larger than \$1/n\^$\backslash$alpha\$ for some \$$\backslash$alpha=O(1)\$. The problem is not only a natural extension of the classical 2-SAT problem to the quantum case, but is also equivalent to the problem of finding the ground state of 2-local frustration-free Hamiltonians of spin \$$\backslash$frac\{1\}\{2\}\$, a well-studied model believed to capture certain key properties in modern condensed matter physics. While Bravyi has shown that the quantum 2-SAT problem has a classical polynomial-time algorithm, the running time of his algorithm is \$O(n\^4)\$. In this paper we give a classical algorithm with linear running time in the number of local projectors, therefore achieving the best possible complexity.},
  journal = {arXiv:1508.06340 [quant-ph]},
  author = {Arad, Itai and Santha, Miklos and Sundaram, Aarthi and Zhang, Shengyu},
  month = aug,
  year = {2015},
  keywords = {Computer Science - Computational Complexity,Quantum Physics},
  file = {/Users/billlai/Zotero/storage/4BK3QTN2/Arad et al. - 2015 - Linear time algorithm for quantum 2SAT.pdf;/Users/billlai/Zotero/storage/XPC355GB/1508.html}
}

@article{ChenNogoTheoremOneway2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1004.3787},
  title = {No-Go {{Theorem}} for {{One}}-Way {{Quantum Computing}} on {{Naturally Occurring Two}}-Level {{Systems}}},
  volume = {83},
  issn = {1050-2947, 1094-1622},
  doi = {10.1103/PhysRevA.83.050301},
  abstract = {One-way quantum computing achieves the full power of quantum computation by performing single particle measurements on some many-body entangled state, known as the resource state. As single particle measurements are relatively easy to implement, the preparation of the resource state becomes a crucial task. An appealing approach is simply to cool a strongly correlated quantum many-body system to its ground state. In addition to requiring the ground state of the system to be universal for one-way quantum computing, we also want the Hamiltonian to have non-degenerate ground state protected by a fixed energy gap, to involve only two-body interactions, and to be frustration-free so that measurements in the course of the computation leave the remaining particles in the ground space. Recently, significant efforts have been made to the search of resource states that appear naturally as ground states in spin lattice systems. The approach is proved to be successful in spin-5/2 and spin-3/2 systems. Yet, it remains an open question whether there could be such a natural resource state in a spin-1/2, i.e., qubit system. Here, we give a negative answer to this question by proving that it is impossible for a genuinely entangled qubit states to be a non-degenerate ground state of any two-body frustration-free Hamiltonian. What is more, we prove that every spin-1/2 frustration-free Hamiltonian with two-body interaction always has a ground state that is a product of single- or two-qubit states, a stronger result that is interesting independent of the context of one-way quantum computing.},
  number = {5},
  journal = {Physical Review A},
  author = {Chen, Jianxin and Chen, Xie and Duan, Runyao and Ji, Zhengfeng and Zeng, Bei},
  month = may,
  year = {2011},
  keywords = {Quantum Physics},
  file = {/Users/billlai/Zotero/storage/E7SQ5XP2/Chen et al. - 2011 - No-go Theorem for One-way Quantum Computing on Nat.pdf;/Users/billlai/Zotero/storage/KJDM55YA/1004.html}
}

@phdthesis{SundaramClassicalQuantumConstraint,
  title = {On {{Classical}} and {{Quantum Constraint Satisfaction Problems}} in the {{Trial}} and {{Error Model}}},
  language = {en},
  author = {Sundaram, Aarthi},
  file = {/Users/billlai/Zotero/storage/PJWL7BM4/Sundaram - On Classical and Quantum Constraint Satisfaction P.pdf}
}

@book{deWolfQuantumComputingLecture2018,
  title = {Quantum {{Computing}}: {{Lecture Notes}}},
  author = {{de Wolf}, Ronald},
  year = {2018},
  file = {/Users/billlai/Zotero/storage/2XDL4X89/qcnotes.pdf}
}

@article{LiuClassicalapproximationalgorithms,
  title = {Classical Approximation Algorithms for Quantum Constraint Satisfaction Problems},
  language = {en},
  author = {Liu, Sevag Gharibian Yi-Kai},
  pages = {81},
  file = {/Users/billlai/Zotero/storage/FAF66ULP/Liu - Classical approximation algorithms for quantum con.pdf}
}

@article{GharibianApproximationAlgorithmsQMAComplete2012,
  title = {Approximation {{Algorithms}} for {{QMA}}-{{Complete Problems}}},
  volume = {41},
  issn = {0097-5397},
  doi = {10.1137/110842272},
  abstract = {Approximation algorithms for classical constraint satisfaction problems are one of the main research areas in theoretical computer science. Here we define a natural approximation version of the QMA-complete local Hamiltonian problem (where QMA stands for Quantum Merlin Arthur) and initiate its study. We present two main results. The first shows that a nontrivial approximation ratio can be obtained in the class NP using product states. The second result (which builds on the first one) gives a polynomial time (classical) algorithm providing a similar approximation ratio  for dense instances of the problem. The latter result is based on an adaptation of the ``exhaustive sampling method'' by Arora, Karger, and Karpinski [J. Comput. System Sci., 58 (1999), p. 193] to the quantum setting and might be of independent interest.},
  number = {4},
  journal = {SIAM Journal on Computing},
  author = {Gharibian, S. and Kempe, J.},
  month = jan,
  year = {2012},
  pages = {1028-1050},
  file = {/Users/billlai/Zotero/storage/JE39KEMA/Gharibian and Kempe - 2012 - Approximation Algorithms for QMA-Complete Problems.pdf;/Users/billlai/Zotero/storage/98PV4A5V/110842272.html}
}

@article{BannachComputingKernelsParallel2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.03604},
  primaryClass = {cs},
  title = {Computing {{Kernels}} in {{Parallel}}: {{Lower}} and {{Upper Bounds}}},
  shorttitle = {Computing {{Kernels}} in {{Parallel}}},
  abstract = {Parallel fixed-parameter tractability studies how parameterized problems can be solved in parallel. A surprisingly large number of parameterized problems admit a high level of parallelization, but this does not mean that we can also efficiently compute small problem kernels in parallel: known kernelization algorithms are typically highly sequential. In the present paper, we establish a number of upper and lower bounds concerning the sizes of kernels that can be computed in parallel. An intriguing finding is that there are complex trade-offs between kernel size and the depth of the circuits needed to compute them: For the vertex cover problem, an exponential kernel can be computed by AC\$\^0\$-circuits, a quadratic kernel by TC\$\^0\$-circuits, and a linear kernel by randomized NC-circuits with derandomization being possible only if it is also possible for the matching problem. Other natural problems for which similar (but quantitatively different) effects can be observed include tree decomposition problems parameterized by the vertex cover number, the undirected feedback vertex set problem, the matching problem, or the point line cover problem. We also present natural problems for which computing kernels is inherently sequential.},
  journal = {arXiv:1807.03604 [cs]},
  author = {Bannach, Max and Tantau, Till},
  month = jul,
  year = {2018},
  keywords = {Computer Science - Computational Complexity},
  file = {/Users/billlai/Zotero/storage/X5TUNDB8/Bannach and Tantau - 2018 - Computing Kernels in Parallel Lower and Upper Bou.pdf;/Users/billlai/Zotero/storage/ILVUUVFR/1807.html}
}

@article{Kwonlowrankwidthcolorings2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.03304},
  primaryClass = {cs, math},
  title = {On Low Rank-Width Colorings},
  abstract = {We introduce the concept of low rank-width colorings, generalising the notion of low tree-depth colorings introduced by Ne$\backslash$v\{s\}et$\backslash$v\{r\}il and Ossona de Mendez in [Grad and classes with bounded expansion I. Decompositions. EJC, 2008]. We say that a class \$$\backslash$mathcal\{C\}\$ of graphs admits low rank-width colourings if there exist functions \$N$\backslash$colon $\backslash$mathbb\{N\}$\backslash$rightarrow$\backslash$mathbb\{N\}\$ and \$Q$\backslash$colon $\backslash$mathbb\{N\}$\backslash$rightarrow$\backslash$mathbb\{N\}\$ such that for all \$p$\backslash$in $\backslash$mathbb\{N\}\$, every graph \$G$\backslash$in $\backslash$mathcal\{C\}\$ can be vertex colored with at most \$N(p)\$ colors such that the union of any \$i$\backslash$leq p\$ color classes induces a subgraph of rank-width at most \$Q(i)\$. Graph classes admitting low rank-width colorings strictly generalize graph classes admitting low tree-depth colorings and graph classes of bounded rank-width. We prove that for every graph class \$$\backslash$mathcal\{C\}\$ of bounded expansion and every positive integer \$r\$, the class \$$\backslash$\{G\^r$\backslash$colon G$\backslash$in $\backslash$mathcal\{C\}$\backslash$\}\$ of \$r\$th powers of graphs from \$$\backslash$mathcal\{C\}\$, as well as the classes of unit interval graphs and bipartite permutation graphs admit low rank-width colorings. All of these classes have unbounded rank-width and do not admit low tree-depth colorings. We also show that the classes of interval graphs and permutation graphs do not admit low rank-width colorings. As interesting side properties, we prove that every graph class admitting low rank-width colorings has the Erd$\backslash$H\{o\}s-Hajnal property and is \$$\backslash$chi\$-bounded.},
  journal = {arXiv:1703.03304 [cs, math]},
  author = {Kwon, O.-joung and Pilipczuk, Micha\l{} and Siebertz, Sebastian},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Combinatorics},
  file = {/Users/billlai/Zotero/storage/N8I45BKD/Kwon et al. - 2017 - On low rank-width colorings.pdf;/Users/billlai/Zotero/storage/EUZA9N6F/1703.html}
}

@article{Kwonlowrankwidthcolorings2017a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.03304},
  primaryClass = {cs, math},
  title = {On Low Rank-Width Colorings},
  abstract = {We introduce the concept of low rank-width colorings, generalising the notion of low tree-depth colorings introduced by Ne$\backslash$v\{s\}et$\backslash$v\{r\}il and Ossona de Mendez in [Grad and classes with bounded expansion I. Decompositions. EJC, 2008]. We say that a class \$$\backslash$mathcal\{C\}\$ of graphs admits low rank-width colourings if there exist functions \$N$\backslash$colon $\backslash$mathbb\{N\}$\backslash$rightarrow$\backslash$mathbb\{N\}\$ and \$Q$\backslash$colon $\backslash$mathbb\{N\}$\backslash$rightarrow$\backslash$mathbb\{N\}\$ such that for all \$p$\backslash$in $\backslash$mathbb\{N\}\$, every graph \$G$\backslash$in $\backslash$mathcal\{C\}\$ can be vertex colored with at most \$N(p)\$ colors such that the union of any \$i$\backslash$leq p\$ color classes induces a subgraph of rank-width at most \$Q(i)\$. Graph classes admitting low rank-width colorings strictly generalize graph classes admitting low tree-depth colorings and graph classes of bounded rank-width. We prove that for every graph class \$$\backslash$mathcal\{C\}\$ of bounded expansion and every positive integer \$r\$, the class \$$\backslash$\{G\^r$\backslash$colon G$\backslash$in $\backslash$mathcal\{C\}$\backslash$\}\$ of \$r\$th powers of graphs from \$$\backslash$mathcal\{C\}\$, as well as the classes of unit interval graphs and bipartite permutation graphs admit low rank-width colorings. All of these classes have unbounded rank-width and do not admit low tree-depth colorings. We also show that the classes of interval graphs and permutation graphs do not admit low rank-width colorings. As interesting side properties, we prove that every graph class admitting low rank-width colorings has the Erd$\backslash$H\{o\}s-Hajnal property and is \$$\backslash$chi\$-bounded.},
  journal = {arXiv:1703.03304 [cs, math]},
  author = {Kwon, O.-joung and Pilipczuk, Micha\l{} and Siebertz, Sebastian},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Combinatorics},
  file = {/Users/billlai/Zotero/storage/5BVH5INC/Kwon et al. - 2017 - On low rank-width colorings.pdf;/Users/billlai/Zotero/storage/PW5VWGMD/1703.html}
}

@article{ChenWidthdepthspace2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.00945},
  primaryClass = {cs},
  title = {Width, Depth and Space},
  abstract = {The width measure treedepth, also known as vertex ranking, centered coloring and elimination tree height, is a well-established notion which has recently seen a resurgence of interest. Since graphs of bounded treedepth are more restricted than graphs of bounded tree- or pathwidth, we are interested in the algorithmic utility of this additional structure. On the negative side, we show that every dynamic programming algorithm on treedepth decompositions of depth\textasciitilde\$t\$ cannot solve Dominating Set with \$O((3-$\backslash$epsilon)\^t $\backslash$cdot $\backslash$log n)\$ space for any \$$\backslash$epsilon $>$ 0\$. This result implies the same space lower bound for dynamic programming algorithms on tree and path decompositions. We supplement this result by showing a space lower bound of \$O((3-$\backslash$epsilon)\^t $\backslash$cdot $\backslash$log n)\$ for 3-Coloring and \$O((2-$\backslash$epsilon)\^t $\backslash$cdot $\backslash$log n)\$ for Vertex Cover. This formalizes the common intuition that dynamic programming algorithms on graph decompositions necessarily consume a lot of space and complements known results of the time-complexity of problems restricted to low-treewidth classes. We then show that treedepth lends itself to the design of branching algorithms. This class of algorithms has in general distinct advantages over dynamic programming algorithms: a) They use less space than algorithms based on dynamic programming, b) they are easy to parallelize and c) they provide possible solutions before terminating. Specifically, we design for Dominating Set a pure branching algorithm that runs in time \$t\^\{O(t\^2)\}$\backslash$cdot n\$ and uses space \$O(t\^3 $\backslash$log t + t $\backslash$log n)\$ and a hybrid of branching and dynamic programming that achieves a running time of \$O(3\^t $\backslash$log t $\backslash$cdot n)\$ while using \$O(2\^t t $\backslash$log t + t $\backslash$log n)\$ space. Algorithms for 3-Coloring and Vertex Cover with space complexity \$O(t $\backslash$cdot $\backslash$log n)\$ and time complexity \$O(3\^t $\backslash$cdot n)\$ and \$O(2\^t$\backslash$cdot n)\$, respectively, are included for completeness.},
  journal = {arXiv:1607.00945 [cs]},
  author = {Chen, Li-Hsuan and Reidl, Felix and Rossmanith, Peter and Villaamil, Fernando S\'anchez},
  month = jul,
  year = {2016},
  keywords = {Computer Science - Computational Complexity},
  file = {/Users/billlai/Zotero/storage/Z7NGGIUW/Chen et al. - 2016 - Width, depth and space.pdf;/Users/billlai/Zotero/storage/ZS27XSK9/1607.html}
}

@article{EldarLocalHamiltoniansApproximationRobust2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.02269},
  primaryClass = {quant-ph},
  title = {Local {{Hamiltonians}} with {{Approximation}}-{{Robust Entanglement}}},
  abstract = {Quantum entanglement is considered, by and large, to be a very delicate and non-robust phenomenon that is very hard to maintain in the presence of noise, or non-zero temperatures. In recent years however, and motivated, in part, by a quest for a quantum analog of the PCP theorem researches have tried to establish whether or not we can preserve quantum entanglement at "constant" temperatures that are independent of system size. This would imply that any quantum state with energy at most, say 0.05 of the total available energy of the Hamiltonian, would be highly-entangled. To this date, no such systems were found, and moreover, it became evident that even embedding local Hamiltonians on robust, albeit "non-physical" topologies, namely expanders, does not guarantee entanglement robustness. In this study, we indicate that such robustness may be possible after all: We construct an infinite family of O(1)-local Hamiltonians, corresponding to check terms of a quantum error-correcting code with the following property of inapproximability: any quantum state with energy at most 0.05 w.r.t. the total available energy cannot be even approximately simulated by classical circuits of bounded (sub-logarithmic) depth. In a sense, this implies that even providing a "witness" to the fact that the local Hamiltonian can be "almost" satisfied, already requires some measure of long-range entanglement. Our construction is but a first step in what, we believe, is a whole range of possible entanglement - robust local Hamiltonians. A natural next step, we believe, is to devise such local Hamiltonians that resist approximation in terms of bounded-depth quantum circuits (e.g. NLTS), and even find such robust forms of entanglement that are useful for some computation.},
  journal = {arXiv:1503.02269 [quant-ph]},
  author = {Eldar, Lior},
  month = mar,
  year = {2015},
  keywords = {Quantum Physics},
  file = {/Users/billlai/Zotero/storage/GRNLKBG9/1503.html}
}

@article{BrandaoProductstateApproximationsQuantum2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1310.0017},
  title = {Product-State {{Approximations}} to {{Quantum Ground States}}},
  volume = {342},
  issn = {0010-3616, 1432-0916},
  doi = {10.1007/s00220-016-2575-1},
  abstract = {The local Hamiltonian problem consists of estimating the ground-state energy (given by the minimum eigenvalue) of a local quantum Hamiltonian. First, we show the existence of a good product-state approximation for the ground-state energy of 2-local Hamiltonians with one or more of the following properties: (1) high degree, (2) small expansion, or (3) a ground state with sublinear entanglement with respect to some partition into small pieces. The approximation based on degree is a surprising difference between quantum Hamiltonians and classical CSPs (constraint satisfaction problems), since in the classical setting, higher degree is usually associated with harder CSPs. The approximation based on low entanglement, in turn, was previously known only in the regime where the entanglement was close to zero. Since the existence of a low-energy product state can be checked in NP, the result implies that any Hamiltonian used for a quantum PCP theorem should have: (1) constant degree, (2) constant expansion, (3) a "volume law" for entanglement with respect to any partition into small parts. Second, we show that in several cases, good product-state approximations not only exist, but can be found in polynomial time: (1) 2-local Hamiltonians on any planar graph, solving an open problem of Bansal, Bravyi, and Terhal, (2) dense k-local Hamiltonians for any constant k, solving an open problem of Gharibian and Kempe, and (3) 2-local Hamiltonians on graphs with low threshold rank, via a quantum generalization of a recent result of Barak, Raghavendra and Steurer. Our work introduces two new tools which may be of independent interest. First, we prove a new quantum version of the de Finetti theorem which does not require the usual assumption of symmetry. Second, we describe a way to analyze the application of the Lasserre/Parrilo SDP hierarchy to local quantum Hamiltonians.},
  number = {1},
  journal = {Communications in Mathematical Physics},
  author = {Brand\~ao, Fernando G. S. L. and Harrow, Aram W.},
  month = feb,
  year = {2016},
  keywords = {Quantum Physics},
  pages = {47-80},
  file = {/Users/billlai/Zotero/storage/3XAUBF2U/Brandão and Harrow - 2016 - Product-state Approximations to Quantum Ground Sta.pdf;/Users/billlai/Zotero/storage/X9C7XJ9L/1310.html}
}

@phdthesis{GriloApproximationGuaranteesQuantum,
  title = {Approximation {{Guarantees}} for the {{Quantum Local Hamiltonian Problem}} and {{Limitations}} for {{Quantum PCPs}}},
  language = {en},
  author = {Grilo, Alex Bredariol},
  file = {/Users/billlai/Zotero/storage/L52GLC4N/AARONSON - Dirigé par Iordanis KERENIDIS.pdf}
}

@article{BrandaoQuantumSpeedupsSemidefinite2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.05537},
  primaryClass = {quant-ph},
  title = {Quantum {{Speed}}-Ups for {{Semidefinite Programming}}},
  abstract = {We give a quantum algorithm for solving semidefinite programs (SDPs). It has worst-case running time \$n\^\{$\backslash$frac\{1\}\{2\}\} m\^\{$\backslash$frac\{1\}\{2\}\} s\^2 $\backslash$text\{poly\}($\backslash$log(n), $\backslash$log(m), R, r, 1/$\backslash$delta)\$, with \$n\$ and \$s\$ the dimension and row-sparsity of the input matrices, respectively, \$m\$ the number of constraints, \$$\backslash$delta\$ the accuracy of the solution, and \$R, r\$ a upper bounds on the size of the optimal primal and dual solutions. This gives a square-root unconditional speed-up over any classical method for solving SDPs both in \$n\$ and \$m\$. We prove the algorithm cannot be substantially improved (in terms of \$n\$ and \$m\$) giving a \$$\backslash$Omega(n\^\{$\backslash$frac\{1\}\{2\}\}+m\^\{$\backslash$frac\{1\}\{2\}\})\$ quantum lower bound for solving semidefinite programs with constant \$s, R, r\$ and \$$\backslash$delta\$. The quantum algorithm is constructed by a combination of quantum Gibbs sampling and the multiplicative weight method. In particular it is based on a classical algorithm of Arora and Kale for approximately solving SDPs. We present a modification of their algorithm to eliminate the need for solving an inner linear program which may be of independent interest.},
  journal = {arXiv:1609.05537 [quant-ph]},
  author = {Brandao, Fernando G. S. L. and Svore, Krysta},
  month = sep,
  year = {2016},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Computational Complexity,Quantum Physics},
  file = {/Users/billlai/Zotero/storage/EZYWGRMA/Brandao and Svore - 2016 - Quantum Speed-ups for Semidefinite Programming.pdf;/Users/billlai/Zotero/storage/QJ7BFAIV/1609.html}
}

@inproceedings{BrandaoProductstateApproximationsQuantum2013,
  address = {New York, NY, USA},
  series = {STOC '13},
  title = {Product-State {{Approximations}} to {{Quantum Ground States}}},
  isbn = {978-1-4503-2029-0},
  doi = {10.1145/2488608.2488719},
  abstract = {The local Hamiltonian problem consists of estimating the ground-state energy (given by the minimum eigenvalue) of a local quantum Hamiltonian. It can be considered as a quantum generalization of constraint satisfaction problems (CSPs) and has a key role in quantum complexity theory, being the first and most natural QMA-complete problem known. An interesting regime for the local Hamiltonian problem is that of extensive error, where one is interested in estimating the mean ground-state energy to constant accuracy. The problem is NP-hard by the PCP theorem, but whether it is QMA-hard is an important open question in quantum complexity theory. A positive solution would represent a quantum analogue of the PCP theorem. A key feature that distinguishes quantum Hamiltonians from classical CSPs is that the solutions may involve complicated entangled states. In this paper, we demonstrate several large classes of Hamiltonians for which product (i.e. unentangled) states can approximate the ground state energy to within a small extensive error. First, we show the mere existence of a good product-state approximation for the ground-state energy of 2-local Hamiltonians with one of more of the following properties: (1) super-constant degree, (2) small expansion, or (3) a ground state with sublinear entanglement with respect to some partition into small pieces. The approximation based on degree is a new and surprising difference between quantum Hamiltonians and classical CSPs, since in the classical setting, higher degree is usually associated with harder CSPs. The approximation based on expansion is not new, but the approximation based on low entanglement was previously known only in the regime where the entanglement was close to zero. Since the existence of a low-energy product state can be checked in NP, this implies that any Hamiltonian used for a quantum PCP theorem should have: (1) constant degree, (2) constant expansion, (3) a ‘‘volume law'' for entanglement with respect to any partition into small parts. Second, we show that in several cases, good product-state approximations not only exist, but can be found in deterministic polynomial time: (1) 2-local Hamiltonians on any planar graph, solving an open problem of Bansal, Bravyi, and Terhal, (2) dense k-local Hamiltonians for any constant k, solving an open problem of Gharibian and Kempe, and (3) 2-local Hamiltonians on graphs with low threshold rank, via a quantum generalization of a recent result of Barak, Raghavendra and Steurer. Our work involves two new tools which may be of independent interest. First, we prove a new quantum version of the de Finetti theorem which does not require the usual assumption of symmetry. Second, we describe a way to analyze the application of the Lasserre/Parrilo SDP hierarchy to local quantum Hamiltonians.},
  booktitle = {Proceedings of the {{Forty}}-Fifth {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Brandao, Fernando G.S.L. and Harrow, Aram W.},
  year = {2013},
  keywords = {de finetti,hamiltonian,pcp,quantum},
  pages = {871--880},
  file = {/Users/billlai/Zotero/storage/GE5GH5KD/Brandao and Harrow - 2013 - Product-state Approximations to Quantum Ground Sta.pdf}
}

@article{BrandaoProductstateApproximationsQuantum2016a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1310.0017},
  title = {Product-State {{Approximations}} to {{Quantum Ground States}}},
  volume = {342},
  issn = {0010-3616, 1432-0916},
  doi = {10.1007/s00220-016-2575-1},
  abstract = {The local Hamiltonian problem consists of estimating the ground-state energy (given by the minimum eigenvalue) of a local quantum Hamiltonian. First, we show the existence of a good product-state approximation for the ground-state energy of 2-local Hamiltonians with one or more of the following properties: (1) high degree, (2) small expansion, or (3) a ground state with sublinear entanglement with respect to some partition into small pieces. The approximation based on degree is a surprising difference between quantum Hamiltonians and classical CSPs (constraint satisfaction problems), since in the classical setting, higher degree is usually associated with harder CSPs. The approximation based on low entanglement, in turn, was previously known only in the regime where the entanglement was close to zero. Since the existence of a low-energy product state can be checked in NP, the result implies that any Hamiltonian used for a quantum PCP theorem should have: (1) constant degree, (2) constant expansion, (3) a "volume law" for entanglement with respect to any partition into small parts. Second, we show that in several cases, good product-state approximations not only exist, but can be found in polynomial time: (1) 2-local Hamiltonians on any planar graph, solving an open problem of Bansal, Bravyi, and Terhal, (2) dense k-local Hamiltonians for any constant k, solving an open problem of Gharibian and Kempe, and (3) 2-local Hamiltonians on graphs with low threshold rank, via a quantum generalization of a recent result of Barak, Raghavendra and Steurer. Our work introduces two new tools which may be of independent interest. First, we prove a new quantum version of the de Finetti theorem which does not require the usual assumption of symmetry. Second, we describe a way to analyze the application of the Lasserre/Parrilo SDP hierarchy to local quantum Hamiltonians.},
  number = {1},
  journal = {Communications in Mathematical Physics},
  author = {Brand\~ao, Fernando G. S. L. and Harrow, Aram W.},
  month = feb,
  year = {2016},
  keywords = {Quantum Physics},
  pages = {47-80},
  file = {/Users/billlai/Zotero/storage/5DLPIP2D/slides - 13-product-state.pdf;/Users/billlai/Zotero/storage/G7MKQ9Z2/Brandão and Harrow - 2016 - Product-state Approximations to Quantum Ground Sta.pdf;/Users/billlai/Zotero/storage/C8JQYAXC/1310.html}
}

@article{SrinivasanLectureNewConstructive,
  title = {Lecture: {{New Constructive Aspects}} of the {{Lova}}\textasciiacute{}sz {{Local Lemma}}, and Their {{Applications}} \textemdash{} {{June}} 15, 201},
  language = {en},
  author = {Srinivasan, Aravind},
  pages = {12},
  file = {/Users/billlai/Zotero/storage/D3FG7V7R/Srinivasan - Lecture New Constructive Aspects of the Lova´sz L.pdf}
}

@article{ChaoProbabilisticAnalysisTwo1986,
  title = {Probabilistic {{Analysis}} of {{Two Heuristics}} for the 3-{{Satisfiability Problem}}},
  volume = {15},
  issn = {0097-5397},
  doi = {10.1137/0215080},
  abstract = {An algorithm for the 3-Satisfiability problem is presented and a probabilistic analysis is performed. The analysis is based on an instance distribution which is parameterized to simulate a variety of sample characteristics. The algorithm assigns values to variables appearing in a given instance of 3-Satisfiability, one at a time, using the unit clause heuristic and a maximum occurring literal selection heuristic; at each step a variable is chosen randomly from a subset of variables which is usually large. The algorithm runs in polynomial time and it is shown that the algorithm finds a solution to a random instance of 3-Satisfiability with probability bounded from below by a constant greater than zero for a range of parameter values. The heuristics studied here can be used to select variables in a Backtrack algorithm for 3-Satisfiability. Experiments have shown that for about the same range of parameters as above the Backtrack algorithm using the heuristics finds a solution in polynomial average time.},
  number = {4},
  journal = {SIAM Journal on Computing},
  author = {Chao, M. and Franco, J.},
  month = nov,
  year = {1986},
  pages = {1106-1118},
  file = {/Users/billlai/Zotero/storage/IHKXMWSY/0215080.html}
}

@incollection{Marques-SilvaImpactBranchingHeuristics1999,
  address = {Berlin, Heidelberg},
  title = {The {{Impact}} of {{Branching Heuristics}} in {{Propositional Satisfiability Algorithms}}},
  volume = {1695},
  isbn = {978-3-540-66548-9 978-3-540-48159-1},
  abstract = {This paper studies the practical impact of the branching heuristics used in Propositional Satisfiability (SAT) algorithms, when applied to solving real-world instances of SAT. In addition, different SAT algorithms are experimentally evaluated. The main conclusion of this study is that even though branching heuristics are crucial for solving SAT, other aspects of the organization of SAT algorithms are also essential. Moreover, we provide empirical evidence that for practical instances of SAT, the search pruning techniques included in the most competitive SAT algorithms may be of more fundamental significance than branching heuristics.},
  language = {en},
  booktitle = {Progress in {{Artificial Intelligence}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {{Marques-Silva}, Jo\~ao},
  editor = {Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Barahona, Pedro and Alferes, Jos\'e J.},
  year = {1999},
  pages = {62-74},
  file = {/Users/billlai/Zotero/storage/CM87WLKN/Marques-Silva - 1999 - The Impact of Branching Heuristics in Propositiona.pdf},
  doi = {10.1007/3-540-48159-1_5}
}

@article{ChaoProbabilisticAnalysisTwo1986a,
  title = {Probabilistic {{Analysis}} of {{Two Heuristics}} for the 3-{{Satisfiability Problem}}},
  volume = {15},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/0215080},
  abstract = {An algorithm for the 3-Satisfiability problem is presented and a probabilistic analysis is performed. The analysis is based on an instance distribution which is parameterized to simulate a variety of sample characteristics. The algorithm assigns values to variables appearing in a given instance of 3-Satisfiability, one at a time, using the unit clause heuristic and a maximum occurring literal selection heuristic; at each step a variable is chosen randomly from a subset of variables which is usually large. The algorithm runs in polynomial time and it is shown that the algorithm finds a solution to a random instance of 3-Satisfiability with probability bounded from below by a constant greater than zero for a range of parameter values. The heuristics studied here can be used to select variables in a Backtrack algorithm for 3-Satisfiability. Experiments have shown that for about the same range of parameters as above the Backtrack algorithm using the heuristics finds a solution in polynomial average time.},
  language = {en},
  number = {4},
  journal = {SIAM Journal on Computing},
  author = {Chao, Ming-Te and Franco, John},
  month = nov,
  year = {1986},
  pages = {1106-1118},
  file = {/Users/billlai/Zotero/storage/H4L5DRCV/Chao and Franco - 1986 - Probabilistic Analysis of Two Heuristics for the 3.pdf}
}

@article{PipatsrisawatModernClauseLearningSatisfiability2010,
  title = {On {{Modern Clause}}-{{Learning Satisfiability Solvers}}},
  volume = {44},
  issn = {0168-7433, 1573-0670},
  doi = {10.1007/s10817-009-9156-3},
  abstract = {In this paper, we present a perspective on modern clause-learning SAT solvers that highlights the roles of, and the interactions between, decision making and clause learning in these solvers. We discuss two limitations of these solvers from this perspective and discuss techniques for dealing with them. We show empirically that the proposed techniques significantly improve state-of-the-art solvers.},
  language = {en},
  number = {3},
  journal = {Journal of Automated Reasoning},
  author = {Pipatsrisawat, Knot and Darwiche, Adnan},
  month = mar,
  year = {2010},
  pages = {277-301},
  file = {/Users/billlai/Zotero/storage/CC63L8ID/Pipatsrisawat and Darwiche - 2010 - On Modern Clause-Learning Satisfiability Solvers.pdf}
}

@phdthesis{TraubProgramAnalysisProbabilistic,
  title = {Program {{Analysis}} and {{Probabilistic SAT}}-Solving},
  language = {en},
  author = {Traub, Johannes Frederik Jesper},
  file = {/Users/billlai/Zotero/storage/FZKTJTWI/Traub - All the work contained within this thesis, except .pdf}
}

@inproceedings{GongsurveySATsolver2017,
  title = {A Survey of {{SAT}} Solver},
  doi = {10.1063/1.4981999},
  abstract = {In Computer Science, the Boolean Satisfiability Problem(SAT) is the problem of determining if there exists an interpretation that satisfies a given Boolean formula. SAT is one of the first problems that was proven to be NP-complete, which is also fundamental to artificial intelligence, algorithm and hardware design. This paper reviews the main algorithms of the SAT solver in recent years, including serial SAT algorithms, parallel SAT algorithms, SAT algorithms based on GPU, and SAT algorithms based on FPGA. The development of SAT is analyzed comprehensively in this paper. Finally, several possible directions for the development of the SAT problem are proposed.},
  language = {en},
  author = {Gong, Weiwei and Zhou, Xu},
  year = {2017},
  pages = {020059},
  file = {/Users/billlai/Zotero/storage/XEUQZWCB/Gong and Zhou - 2017 - A survey of SAT solver.pdf}
}

@article{AudemardGlucoseSATSolver2018,
  title = {On the {{Glucose SAT Solver}}},
  volume = {27},
  issn = {0218-2130, 1793-6349},
  doi = {10.1142/S0218213018400018},
  language = {en},
  number = {01},
  journal = {International Journal on Artificial Intelligence Tools},
  author = {Audemard, Gilles and Simon, Laurent},
  month = feb,
  year = {2018},
  pages = {1840001},
  file = {/Users/billlai/Zotero/storage/XGPDZ669/Audemard and Simon - 2018 - On the Glucose SAT Solver.pdf}
}

@incollection{Marques-SilvaPropositionalSATSolving2018,
  address = {Cham},
  title = {Propositional {{SAT Solving}}},
  isbn = {978-3-319-10574-1 978-3-319-10575-8},
  abstract = {The Boolean Satisfiability Problem (SAT) is well known in computational complexity, representing the first decision problem to be proved NP-complete. SAT is also often the subject of work in proof complexity. Besides its theoretical interest, SAT finds a wide range of practical applications. Moreover, SAT solvers have been the subject of remarkable efficiency improvements since the mid-1990s, motivating their widespread use in many practical applications including Bounded and Unbounded Model Checking. The success of SAT solvers has also motivated the development of algorithms for natural extensions of SAT, including Quantified Boolean Formulas (QBF), Pseudo-Boolean constraints (PB), Maximum Satisfiability (MaxSAT) and Satisfiability Modulo Theories (SMT) which also see application in the model-checking context. This chapter first covers the organization of modern conflict-driven clause learning (CDCL) SAT solvers, which are used in the vast majority of practical applications of SAT. It then reviews the techniques shown to be effective in modern SAT solvers.},
  language = {en},
  booktitle = {Handbook of {{Model Checking}}},
  publisher = {{Springer International Publishing}},
  author = {{Marques-Silva}, Joao and Malik, Sharad},
  editor = {Clarke, Edmund M. and Henzinger, Thomas A. and Veith, Helmut and Bloem, Roderick},
  year = {2018},
  pages = {247-275},
  file = {/Users/billlai/Zotero/storage/QP4TQ327/Marques-Silva and Malik - 2018 - Propositional SAT Solving.pdf},
  doi = {10.1007/978-3-319-10575-8_9}
}

@article{LiangMapleCOMSPSMapleCOMSPSLRB,
  title = {{{MapleCOMSPS}}, {{MapleCOMSPS LRB}}, {{MapleCOMSPS CHB}}},
  abstract = {This document describes the SAT solvers MapleCOMSPS, MapleCOMSPS LRB, and MapleCOMSPS CHB, three solvers implementing our machine learning branching heuristics called the learning rate branching heuristic (LRB) and conflict history-based branching heuristic (CHB).},
  language = {en},
  author = {Liang, Jia Hui and Oh, Chanseok and Ganesh, Vijay and Czarnecki, Krzysztof and Poupart, Pascal},
  pages = {2},
  file = {/Users/billlai/Zotero/storage/VR34NIQP/Liang et al. - MapleCOMSPS, MapleCOMSPS LRB, MapleCOMSPS CHB.pdf}
}

@article{GableskeSATSolvingMessage,
  title = {{{SAT Solving}} with {{Message Passing}}},
  language = {en},
  author = {Gableske, Oliver},
  pages = {428},
  file = {/Users/billlai/Zotero/storage/XVIN423S/Gableske - SAT Solving with Message Passing.pdf}
}

@article{nikMiniSatSATAlgorithms,
  title = {{{MiniSat}} - {{SAT Algorithms}} and {{Applications}}},
  language = {en},
  author = {{nik}, Niklas S\"orensson},
  pages = {67},
  file = {/Users/billlai/Zotero/storage/VUT5GC2U/nik - MiniSat - SAT Algorithms and Applications.pdf}
}

@article{SorenssonMiniSatv113,
  title = {{{MiniSat}} v1.13 \textendash{} {{A SAT Solver}} with {{Conflict}}-{{Clause Minimization}}},
  abstract = {In this poster we summarize the features of the MiniSat version entering the SAT Competition 2005. The main new feature is a resolution based conflict clause minimization technique based on self-subsuming resolution. Experiments show that on industrial examples, it is not unusual for more than 30\% of the literals in a conflict clause to be redundant. Removing these literals reduces memory consumption and produce stronger clauses which may propagate under fewer decisions in the DPLL search procedure.},
  language = {en},
  author = {Sorensson, Niklas and Een, Niklas},
  pages = {2},
  file = {/Users/billlai/Zotero/storage/BQY7BMWU/Sorensson and Een - MiniSat v1.13 – A SAT Solver with Conﬂict-Clause M.pdf}
}

@incollection{LiangLearningRateBased2016,
  address = {Cham},
  title = {Learning {{Rate Based Branching Heuristic}} for {{SAT Solvers}}},
  volume = {9710},
  isbn = {978-3-319-40969-6 978-3-319-40970-2},
  abstract = {In this paper, we propose a framework for viewing solver branching heuristics as optimization algorithms where the objective is to maximize the learning rate, defined as the propensity for variables to generate learnt clauses. By viewing online variable selection in SAT solvers as an optimization problem, we can leverage a wide variety of optimization algorithms, especially from machine learning, to design effective branching heuristics. In particular, we model the variable selection optimization problem as an online multi-armed bandit, a special-case of reinforcement learning, to learn branching variables such that the learning rate of the solver is maximized. We develop a branching heuristic that we call learning rate branching or LRB, based on a well-known multiarmed bandit algorithm called exponential recency weighted average and implement it as part of MiniSat and CryptoMiniSat. We upgrade the LRB technique with two additional novel ideas to improve the learning rate by accounting for reason side rate and exploiting locality. The resulting LRB branching heuristic is shown to be faster than the VSIDS and conflict history-based (CHB) branching heuristics on 1975 application and hard combinatorial instances from 2009 to 2014 SAT Competitions. We also show that CryptoMiniSat with LRB solves more instances than the one with VSIDS. These experiments show that LRB improves on state-of-the-art.},
  language = {en},
  booktitle = {Theory and {{Applications}} of {{Satisfiability Testing}} \textendash{} {{SAT}} 2016},
  publisher = {{Springer International Publishing}},
  author = {Liang, Jia Hui and Ganesh, Vijay and Poupart, Pascal and Czarnecki, Krzysztof},
  editor = {Creignou, Nadia and Le Berre, Daniel},
  year = {2016},
  pages = {123-140},
  file = {/Users/billlai/Zotero/storage/8SMGNUP5/Liang et al. - 2016 - Learning Rate Based Branching Heuristic for SAT So.pdf},
  doi = {10.1007/978-3-319-40970-2_9}
}

@inproceedings{ChenFineClassificationConjunctive2013,
  address = {New York, NY, USA},
  series = {PODS '13},
  title = {The {{Fine Classification}} of {{Conjunctive Queries}} and {{Parameterized Logarithmic Space Complexity}}},
  isbn = {978-1-4503-2066-5},
  doi = {10.1145/2463664.2463669},
  abstract = {We perform a fundamental investigation of the complexity of conjunctive query evaluation from the perspective of parameterized complexity. We classify sets of boolean conjunctive queries according to the complexity of this problem. Previous work showed that a set of conjunctive queries is fixed-parameter tractable precisely when the set is equivalent to a set of queries having bounded treewidth. We present a fine classification of query sets up to parameterized logarithmic space reduction. We show that, in the bounded treewidth regime, there are three complexity degrees and that the properties that determine the degree of a query set are bounded pathwidth and bounded tree depth. We also engage in a study of the two higher degrees via logarithmic space machine characterizations and complete problems. Our work yields a significantly richer perspective on the complexity of conjunctive queries and, at the same time, suggests new avenues of research in parameterized complexity.},
  booktitle = {Proceedings of the {{32Nd ACM SIGMOD}}-{{SIGACT}}-{{SIGAI Symposium}} on {{Principles}} of {{Database Systems}}},
  publisher = {{ACM}},
  author = {Chen, Hubie and M\"uller, Moritz},
  year = {2013},
  keywords = {graph minors,parameterized complexity,conjunctive queries,logarithmic space},
  pages = {309--320},
  file = {/Users/billlai/Zotero/storage/K5VMG2HX/Chen and Müller - 2013 - The Fine Classification of Conjunctive Queries and.pdf}
}

@book{NesetrilSparsitygraphsstructures2012,
  address = {Heidelberg ; New York},
  series = {Algorithms and combinatorics},
  title = {Sparsity: Graphs, Structures, and Algorithms},
  isbn = {978-3-642-27874-7},
  lccn = {QA164 .N47 2012},
  shorttitle = {Sparsity},
  language = {en},
  number = {28},
  publisher = {{Springer}},
  author = {Ne{\v s}et{\v r}il, Jaroslav and {Ossona de Mendez}, Patrice},
  year = {2012},
  keywords = {Sparse matrices},
  file = {/Users/billlai/Zotero/storage/725IK4QJ/Nešetřil and Ossona de Mendez - 2012 - Sparsity graphs, structures, and algorithms.pdf},
  note = {OCLC: ocn773019512}
}

@techreport{TichyClauseLearningSAT2006,
  title = {Clause {{Learning}} in {{SAT}}: {{Seminar Automatic Problem Solving}}},
  author = {Tichy, Richard and Glase, Thomas},
  year = {2006},
  file = {/Users/billlai/Zotero/storage/MZUY6XC8/SAT_learning_clauses.pdf}
}

@inproceedings{RaghavendraOptimalalgorithmsinapproximability2008,
  title = {Optimal Algorithms and Inapproximability Results for Every {{CSP}}?},
  isbn = {978-1-60558-047-0},
  doi = {10.1145/1374376.1374414},
  abstract = {Semidefinite Programming(SDP) is one of the strongest algorithmic techniques used in the design of approximation algorithms. In recent years, Unique Games Conjecture(UGC) has proved to be intimately connected to the limitations of Semidefinite Programming.},
  language = {en},
  publisher = {{ACM Press}},
  author = {Raghavendra, Prasad},
  year = {2008},
  pages = {245},
  file = {/Users/billlai/Zotero/storage/EIY9IZEL/Raghavendra - 2008 - Optimal algorithms and inapproximability results f.pdf;/Users/billlai/Zotero/storage/I5WHNJRH/optimalcsp.pptx}
}

@techreport{SafraPCPHardnessApproximation,
  title = {{{PCP}} and {{Hardness}} of {{Approximation}}},
  author = {Safra, Muli},
  file = {/Users/billlai/Zotero/storage/H75EE6AH/PCP.pdf}
}

@article{Changrandomoraclehypothesis1994,
  title = {The Random Oracle Hypothesis Is False},
  volume = {49},
  issn = {00220000},
  doi = {10.1016/S0022-0000(05)80084-4},
  abstract = {The Random Oracle Hypothesis, attributed to Bennett and Gill, essentially states that the relationships between complexity classes which hold for almost all relativized worlds must also hold in the unrelativized case. Although this paper is not the first to provide a counterexample to the Random Oracle Hypothesis, it does provide a most compelling counterexample by showing that for almost all oracles A, IPA = PSPACEA. If the Random Oracle Hypothesis were true, it would contradict Shamir's result that IP = PSPACE. In fact, it is shown that for almost all oracles A, co-NPA $\subseteq$ IPA. These results extend to the multi-prover proof systems of Ben-Or, Goldwasser, Kilian and Wigderson. In addition, this paper shows that the Random Oracle Hypothesis is sensitive to small changes in the definition. A class IPP, similar to IP, is defined. Surprisingly, the IPP = PSPACE result holds for all oracle worlds.},
  language = {en},
  number = {1},
  journal = {Journal of Computer and System Sciences},
  author = {Chang, Richard and Chor, Benny and Goldreich, Oded and Hartmanis, Juris and H\aa{}stad, Johan and Ranjan, Desh and Rohatgi, Pankaj},
  month = aug,
  year = {1994},
  pages = {24-39},
  file = {/Users/billlai/Zotero/storage/F4M2YLU5/Chang et al. - 1994 - The random oracle hypothesis is false.pdf}
}

@article{GoldreichTheorycomputingscientific1996,
  title = {Theory of Computing: A Scientific Perspective},
  volume = {28},
  shorttitle = {Theory of Computing},
  number = {4es},
  journal = {ACM Computing Surveys (CSUR)},
  author = {Goldreich, Oded and Wigderson, Avi},
  year = {1996},
  pages = {218},
  file = {/Users/billlai/Zotero/storage/TPVB2TAI/toc-sp2.pdf;/Users/billlai/Zotero/storage/G26K9R72/citation.html}
}

@article{FarhiQuantumVersionSch2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.06985},
  primaryClass = {quant-ph},
  title = {A {{Quantum Version}} of {{Sch}}$\backslash$"oning's {{Algorithm Applied}} to {{Quantum}} 2-{{SAT}}},
  abstract = {We study a quantum algorithm that consists of a simple quantum Markov process, and we analyze its behavior on restricted versions of Quantum 2-SAT. We prove that the algorithm solves this decision problem with high probability for n qubits, L clauses, and promise gap c in time O(n\^2 L\^2 c\^\{-2\}). If the Hamiltonian is additionally polynomially gapped, our algorithm efficiently produces a state that has high overlap with the satisfying subspace. The Markov process we study is a quantum analogue of Sch$\backslash$"oning's probabilistic algorithm for k-SAT.},
  journal = {arXiv:1603.06985 [quant-ph]},
  author = {Farhi, Edward and Kimmel, Shelby and Temme, Kristan},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Computational Complexity,Quantum Physics},
  file = {/Users/billlai/Zotero/storage/ACN8RUYH/Farhi et al. - 2016 - A Quantum Version of Schoning's Algorithm Applie.pdf;/Users/billlai/Zotero/storage/FLXRNQR6/1603.html}
}

@article{GoemansImprovedapproximationalgorithms1995,
  title = {Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming},
  volume = {42},
  issn = {00045411},
  doi = {10.1145/227683.227684},
  abstract = {We present randomized approximation algorithms for the maximum cut (MAX CUT) and maximum 2-satisfiability (MAX 2SAT) problems that always deliver solutions of expected value at least .87856 times the optimal value. These algorithms use a simple and elegant technique that randomly rounds the solution to a nonlinear programming relaxation. This relaxation can be interpreted both as a semidefinite program and as an eigenvalue minimization problem. The best previously known approximation algorithms for these problems had perfc\textasciitilde{}rmance guarantees of \textasciitilde{} for MAX CUT and \textasciitilde{} for MAX 2SAT. Slight extensions of our analysis lead to a .79607-approximation algorithm for the maximum directed cut problem (MAX DICUT) and a .758-approximation algorithm for MAX SAT, where the best previously known approxim ation algorithms had performance guarantees of \textasciitilde{} and \textasciitilde, respectively. Our algorithm gives the first substantial progress in approximating MAX CUT in nearly twenty years, and represents the first use of :semidefinite programming in the design of approximation algorithms.},
  language = {en},
  number = {6},
  journal = {Journal of the ACM},
  author = {Goemans, Michel X. and Williamson, David P.},
  month = nov,
  year = {1995},
  pages = {1115-1145},
  file = {/Users/billlai/Zotero/storage/393RITEC/Goemans and Williamson - 1995 - Improved approximation algorithms for maximum cut .pdf}
}

@article{MahajanDerandomizingApproximationAlgorithms1999,
  title = {Derandomizing {{Approximation Algorithms Based}} on {{Semidefinite Programming}}},
  volume = {28},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/S0097539796309326},
  abstract = {Remarkable breakthroughs have been made recently in obtaining approximate solutions to some fundamental NP-hard problems, namely Max-Cut, Max k-Cut, Max-Sat, Max-Dicut, Max-bisection, k-vertex coloring, maximum independent set, etc. All these breakthroughs involve polynomial time randomized algorithms based upon semidefinite programming, a technique pioneered by Goemans and Williamson.},
  language = {en},
  number = {5},
  journal = {SIAM Journal on Computing},
  author = {Mahajan, Sanjeev and Ramesh, H.},
  month = jan,
  year = {1999},
  pages = {1641-1663},
  file = {/Users/billlai/Zotero/storage/G25XXJQN/Mahajan and Ramesh - 1999 - Derandomizing Approximation Algorithms Based on Se.pdf}
}

@article{BrietGrothendieckinequalitiessemidefinite2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1011.1754},
  title = {Grothendieck Inequalities for Semidefinite Programs with Rank Constraint},
  volume = {10},
  issn = {1557-2862},
  doi = {10.4086/toc.2014.v010a004},
  abstract = {Grothendieck inequalities are fundamental inequalities which are frequently used in many areas of mathematics and computer science. They can be interpreted as upper bounds for the integrality gap between two optimization problems: a difficult semidefinite program with rank-1 constraint and its easy semidefinite relaxation where the rank constrained is dropped. For instance, the integrality gap of the Goemans-Williamson approximation algorithm for MAX CUT can be seen as a Grothendieck inequality. In this paper we consider Grothendieck inequalities for ranks greater than 1 and we give two applications: approximating ground states in the n-vector model in statistical mechanics and XOR games in quantum information theory.},
  number = {1},
  journal = {Theory of Computing},
  author = {Briet, Jop and Filho, Fernando Mario de Oliveira and Vallentin, Frank},
  year = {2014},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Combinatorics,68W25; 90C22,Mathematics - Functional Analysis,Mathematics - Optimization and Control},
  pages = {77-105},
  file = {/Users/billlai/Zotero/storage/MAWRQDZD/Briet et al. - 2014 - Grothendieck inequalities for semidefinite program.pdf;/Users/billlai/Zotero/storage/DSZXT4L3/1011.html}
}

@incollection{GebauerLocalLemmaTight2011,
  address = {Philadelphia, PA},
  title = {The {{Local Lemma}} Is {{Tight}} for {{SAT}}},
  isbn = {978-0-89871-993-2 978-1-61197-308-2},
  language = {en},
  booktitle = {Proceedings of the {{Twenty}}-{{Second Annual ACM}}-{{SIAM Symposium}} on {{Discrete Algorithms}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Gebauer, H. and Szab\'o, T. and Tardos, G.},
  editor = {Randall, Dana},
  month = jan,
  year = {2011},
  pages = {664-674},
  file = {/Users/billlai/Zotero/storage/PK7SQ5WA/Gebauer et al. - 2011 - The Local Lemma is Tight for SAT.pdf},
  doi = {10.1137/1.9781611973082.52}
}

@article{BravyiApproximationalgorithmsquantum2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.01734},
  primaryClass = {quant-ph},
  title = {Approximation Algorithms for Quantum Many-Body Problems},
  abstract = {We discuss classical algorithms for approximating the largest eigenvalue of quantum spin and fermionic Hamiltonians based on semidefinite programming relaxation methods. First, we consider traceless \$2\$-local Hamiltonians \$H\$ describing a system of \$n\$ qubits. We give an efficient algorithm that outputs a separable state whose energy is at least \$$\backslash$lambda\_\{$\backslash$max\}/O($\backslash$log n)\$, where \$$\backslash$lambda\_\{$\backslash$max\}\$ is the maximum eigenvalue of \$H\$. We also give a simplified proof of a theorem due to Lieb that establishes the existence of a separable state with energy at least \$$\backslash$lambda\_\{$\backslash$max\}/9\$. Secondly, we consider a system of \$n\$ fermionic modes and traceless Hamiltonians composed of quadratic and quartic fermionic operators. We give an efficient algorithm that outputs a fermionic Gaussian state whose energy is at least \$$\backslash$lambda\_\{$\backslash$max\}/O(n$\backslash$log n)\$. Finally, we show that Gaussian states can vastly outperform Slater determinant states commonly used in the Hartree-Fock method. We give a simple family of Hamiltonians for which Gaussian states and Slater determinants approximate \$$\backslash$lambda\_\{$\backslash$max\}\$ within a fraction \$1-O(n\^\{-1\})\$ and \$O(n\^\{-1\})\$ respectively.},
  journal = {arXiv:1808.01734 [quant-ph]},
  author = {Bravyi, Sergey and Gosset, David and Koenig, Robert and Temme, Kristan},
  month = aug,
  year = {2018},
  keywords = {Quantum Physics},
  file = {/Users/billlai/Zotero/storage/I4DHJ5DK/Bravyi et al. - 2018 - Approximation algorithms for quantum many-body pro.pdf;/Users/billlai/Zotero/storage/K937RHDM/1808.html}
}

@article{BravyiApproximationalgorithmsquantum2018a,
  title = {Approximation Algorithms for Quantum Many-Body Problems},
  language = {en},
  author = {Bravyi, Sergey and Gosset, David and Koenig, Robert and Temme, Kristan},
  month = aug,
  year = {2018},
  file = {/Users/billlai/Zotero/storage/NAQU89FK/Bravyi et al. - 2018 - Approximation algorithms for quantum many-body pro.pdf;/Users/billlai/Zotero/storage/TWSZ3W54/1808.html}
}

@inproceedings{CatarataMoserTardosResamplealgorithm2017,
  title = {The {{Moser}}-{{Tardos Resample}} Algorithm: {{Where}} Is the Limit? (An Experimental Inquiry)},
  isbn = {978-1-61197-476-8},
  shorttitle = {The {{Moser}}-{{Tardos Resample}} Algorithm},
  doi = {10.1137/1.9781611974768.13},
  abstract = {The celebrated Lova\textasciiacute{}sz Local Lemma (LLL) guarantees that locally sparse systems always have solutions. The MoserTardos Resample algorithm does not only find such a solution in linear time, but its beautiful analysis has greatly enhanced LLL related research [9, 10]. Nevertheless two major questions remain open.},
  language = {en},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Catarata, Jan Dean and Corbett, Scott and Stern, Harry and Szegedy, Mario and Vyskocil, Tomas and Zhang, Zheng},
  month = jan,
  year = {2017},
  pages = {159-171},
  file = {/Users/billlai/Zotero/storage/7T52AGGU/Catarata et al. - 2017 - The Moser-Tardos Resample algorithm Where is the .pdf}
}

@inproceedings{CatarataMoserTardosResamplealgorithm2017a,
  title = {The {{Moser}}-{{Tardos Resample}} Algorithm: {{Where}} Is the Limit? (An Experimental Inquiry)},
  isbn = {978-1-61197-476-8},
  shorttitle = {The {{Moser}}-{{Tardos Resample}} Algorithm},
  doi = {10.1137/1.9781611974768.13},
  abstract = {The celebrated Lova\textasciiacute{}sz Local Lemma (LLL) guarantees that locally sparse systems always have solutions. The MoserTardos Resample algorithm does not only find such a solution in linear time, but its beautiful analysis has greatly enhanced LLL related research [9, 10]. Nevertheless two major questions remain open.},
  language = {en},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Catarata, Jan Dean and Corbett, Scott and Stern, Harry and Szegedy, Mario and Vyskocil, Tomas and Zhang, Zheng},
  month = jan,
  year = {2017},
  pages = {159-171},
  file = {/Users/billlai/Zotero/storage/BBUX77ND/Catarata et al. - 2017 - The Moser-Tardos Resample algorithm Where is the .pdf}
}

@article{GebauerDisproofneighborhoodconjecture2012,
  title = {Disproof of the Neighborhood Conjecture with Implications to {{SAT}}},
  volume = {32},
  issn = {0209-9683, 1439-6912},
  doi = {10.1007/s00493-012-2679-y},
  language = {en},
  number = {5},
  journal = {Combinatorica},
  author = {Gebauer, Heidi},
  month = may,
  year = {2012},
  pages = {573-587},
  file = {/Users/billlai/Zotero/storage/DD7RW4FW/Gebauer - 2012 - Disproof of the neighborhood conjecture with impli.pdf}
}

@inproceedings{CharikarMaximizingQuadraticPrograms2004,
  title = {Maximizing {{Quadratic Programs}}: {{Extending Grothendieck}}'s {{Inequality}}},
  isbn = {978-0-7695-2228-9},
  shorttitle = {Maximizing {{Quadratic Programs}}},
  doi = {10.1109/FOCS.2004.39},
  abstract = {This paper considers the following type of quadratic programming problem. Given an arbitrary matrix A, whose diagonal elements are zero, find x $\in$ \{-1, 1\}n such that xTAx is maximized. Our approximation algorithm for this problem uses the canonical semidefinite relaxation and returns a solution whose ratio to the optimum is in $\Omega$(1/ log n). This quadratic programming problem can be seen as an extension to that of maximizing xTAy (where y's components are also $\pm$1). Grothendieck's inequality states that the ratio of the optimum value of the latter problem to the optimum of its canonical semidefinite relaxation is bounded below by a constant.},
  language = {en},
  publisher = {{IEEE}},
  author = {Charikar, M. and Wirth, A.},
  year = {2004},
  pages = {54-60},
  file = {/Users/billlai/Zotero/storage/URJ55CAX/Charikar and Wirth - 2004 - Maximizing Quadratic Programs Extending Grothendi.pdf}
}

@article{LozinChordalbipartitegraphs2004a,
  title = {Chordal Bipartite Graphs of Bounded Tree- and Clique-Width},
  volume = {283},
  issn = {0012365X},
  doi = {10.1016/j.disc.2004.02.008},
  abstract = {A bipartite graph is chordal bipartite if every cycle of length at least six has a chord. In the class of chordal bipartite graphs the tree-width and the clique-width are unbounded. Our main results are that chordal bipartite graphs of bounded vertex degree have bounded tree-width and that k-fork-free chordal bipartite graphs have bounded clique-width, where a k-fork is the graph arising from a K1;k+1 by subdividing one edge once. (Note that a bipartite graph has vertex degree at most k if and only if it is K1;k+1-free.) This implies polynomial-time solvability for a variety of algorithmical problems for these graphs.},
  language = {en},
  number = {1-3},
  journal = {Discrete Mathematics},
  author = {Lozin, V. and Rautenbach, D.},
  month = jun,
  year = {2004},
  pages = {151-158},
  file = {/Users/billlai/Zotero/storage/K2MWQRL4/Lozin and Rautenbach - 2004 - Chordal bipartite graphs of bounded tree- and cliq.pdf}
}

@article{SCHEDERUNSATISFIABLELINEARCNF,
  title = {{{UNSATISFIABLE LINEAR CNF FORMULAS ARE LARGE AND COMPLEX}}},
  language = {en},
  author = {SCHEDER, DOMINIK},
  pages = {13},
  file = {/Users/billlai/Zotero/storage/56DMLZ3M/SCHEDER - UNSATISFIABLE LINEAR CNF FORMULAS ARE LARGE AND CO.pdf}
}

@inproceedings{FlemingRandomThlog2017,
  title = {Random {{$\Theta$}}(Log n)-{{CNFs Are Hard}} for {{Cutting Planes}}},
  isbn = {978-1-5386-3464-6},
  doi = {10.1109/FOCS.2017.19},
  abstract = {The random k-SAT model is the most important and well-studied distribution over k-SAT instances. It is closely connected to statistical physics and is a benchmark for satisfiability algorithms. We show that when k = $\Theta$(log n), any Cutting Planes refutation for random k-SAT requires exponential size in the interesting regime where the number of clauses guarantees that the formula is unsatisfiable with high probability.},
  language = {en},
  publisher = {{IEEE}},
  author = {Fleming, Noah and Pankratov, Denis and Pitassi, Toniann and Robere, Robert},
  month = oct,
  year = {2017},
  pages = {109-120},
  file = {/Users/billlai/Zotero/storage/N2LRBYSA/Fleming et al. - 2017 - Random Θ(log n)-CNFs Are Hard for Cutting Planes.pdf}
}

@article{IgnatievTacklingLimitsResolution2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.01477},
  primaryClass = {cs},
  title = {On {{Tackling}} the {{Limits}} of {{Resolution}} in {{SAT Solving}}},
  abstract = {The practical success of Boolean Satisfiability (SAT) solvers stems from the CDCL (Conflict-Driven Clause Learning) approach to SAT solving. However, from a propositional proof complexity perspective, CDCL is no more powerful than the resolution proof system, for which many hard examples exist. This paper proposes a new problem transformation, which enables reducing the decision problem for formulas in conjunctive normal form (CNF) to the problem of solving maximum satisfiability over Horn formulas. Given the new transformation, the paper proves a polynomial bound on the number of MaxSAT resolution steps for pigeonhole formulas. This result is in clear contrast with earlier results on the length of proofs of MaxSAT resolution for pigeonhole formulas. The paper also establishes the same polynomial bound in the case of modern core-guided MaxSAT solvers. Experimental results, obtained on CNF formulas known to be hard for CDCL SAT solvers, show that these can be efficiently solved with modern MaxSAT solvers.},
  journal = {arXiv:1705.01477 [cs]},
  author = {Ignatiev, Alexey and Morgado, Antonio and {Marques-Silva}, Joao},
  month = may,
  year = {2017},
  keywords = {Computer Science - Logic in Computer Science},
  file = {/Users/billlai/Zotero/storage/3MKCM59F/Ignatiev et al. - 2017 - On Tackling the Limits of Resolution in SAT Solvin.pdf;/Users/billlai/Zotero/storage/V95PH8ED/1705.html}
}

@article{Goldschlagermaximumflowproblem1982,
  title = {The Maximum Flow Problem Is Log Space Complete for {{P}}},
  volume = {21},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(82)90092-5},
  abstract = {The space complexity of the maximum flow problem is investigated. It is shown that the problem is log space complete for deterministic polynomial time. Thus the maximum flow problem probably has no algorithm which needs only O(logkn) storage space for any constant k. Another consequence is that there is probably no fast parallel algorithm for the maximum flow problem.},
  number = {1},
  journal = {Theoretical Computer Science},
  author = {Goldschlager, Leslie M. and Shaw, Ralph A. and Staples, John},
  month = oct,
  year = {1982},
  pages = {105-111},
  file = {/Users/billlai/Zotero/storage/FVHNCMGG/Goldschlager et al. - 1982 - The maximum flow problem is log space complete for.pdf;/Users/billlai/Zotero/storage/GSDNMXTC/0304397582900925.html}
}

@article{JohnsonParallelAlgorithmComputing1995,
  title = {A {{Parallel Algorithm}} for {{Computing Minimum Spanning Trees}}},
  volume = {19},
  issn = {0196-6774},
  doi = {10.1006/jagm.1995.1043},
  abstract = {present a simple and implementable algorithm that computes a minimum spanning tree of an undirected weighted graph G = (V, E) of n = |V| vertices and m = |E| edges on an EREW PRAM in O(log3/2n) time using n + m processors. This represents a substantial improvement in the running time over the previous results for this problem using at the same time the weakest of the PRAM models. It also implies the existence of algorithms having the same complexity bounds for the EREW PRAM, for connectivity, ear decomposition, biconnectivity, strong orientation, st-numbering and Euler tours problems.},
  number = {3},
  journal = {Journal of Algorithms},
  author = {Johnson, D. B. and Metaxas, P.},
  month = nov,
  year = {1995},
  pages = {383-401},
  file = {/Users/billlai/Zotero/storage/GV8M7L3E/S0196677485710437.html}
}

@article{DvorakForbiddengraphstreedepth2012,
  title = {Forbidden Graphs for Tree-Depth},
  volume = {33},
  issn = {01956698},
  doi = {10.1016/j.ejc.2011.09.014},
  language = {en},
  number = {5},
  journal = {European Journal of Combinatorics},
  author = {Dvo{\v r}\'ak, Zden{\v e}k and Giannopoulou, Archontia C. and Thilikos, Dimitrios M.},
  month = jul,
  year = {2012},
  pages = {969-979},
  file = {/Users/billlai/Folder/Papers/Dvořák et al. - 2012 - Forbidden graphs for tree-depth.pdf}
}

@article{LauriaCliquesenumerationtreelike2018,
  title = {Cliques Enumeration and Tree-like Resolution Proofs},
  volume = {135},
  issn = {0020-0190},
  doi = {10.1016/j.ipl.2018.03.001},
  abstract = {We show the close connection between the enumeration of cliques in a k-clique free graph G, the running time of DPLL-style algorithms for k-clique problem, and the length of tree-like resolution refutations for formula Clique(G,k), which claims that G has a k-clique. The length of any such tree-like refutation is within a ``fixed parameter tractable'' factor from the number of cliques in the graph. We then proceed to drastically simplify the proofs of the lower bounds for the length of tree-like resolution refutations of Clique(G,k) shown in Beyersdorff et at. 2013, Lauria et al. 2017, which now reduce to a simple estimate of said quantity.},
  journal = {Information Processing Letters},
  author = {Lauria, Massimo},
  month = jul,
  year = {2018},
  keywords = {Clique,Decision tree,Resolution,Theory of computation},
  pages = {62-67},
  file = {/Users/billlai/Zotero/storage/TYDXN5MF/S0020019018300486.html}
}

@article{EppsteinNCAlgorithmsPerfect2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.00084},
  primaryClass = {cs},
  title = {{{NC Algorithms}} for {{Perfect Matching}} and {{Maximum Flow}} in {{One}}-{{Crossing}}-{{Minor}}-{{Free Graphs}}},
  abstract = {In 1988, Vazirani gave an NC algorithm for computing the number of perfect matchings in \$K\_\{3,3\}\$-minor-free graphs by building on Kasteleyn's scheme for planar graphs, and stated that this "opens up the possibility of obtaining an NC algorithm for finding a perfect matching in \$K\_\{3,3\}\$-free graphs." In this paper, we finally settle this 30-year-old open problem. Building on the recent breakthrough result of Anari and Vazirani giving an NC algorithm for finding a perfect matching in planar graphs and graphs of bounded genus, we also obtain NC algorithms for any minor-closed graph family that forbids a one-crossing graph. The class contains several well-studied graph families including the \$K\_\{3,3\}\$-minor-free graphs and \$K\_5\$-minor-free graphs. Graphs in these classes not only have unbounded genus, but also can have genus as high as \$O(n)\$. In particular, we obtain NC algorithms for: * Determining whether a one-crossing-minor-free graph has a perfect matching and if so, finding one. * Finding a minimum weight perfect matching in a one-crossing-minor-free graph, assuming that the edge weights are polynomially bounded. * Finding a maximum \$st\$-flow in a one-crossing-minor-free flow network, with arbitrary capacities. The main new idea enabling our results is the definition and use of matching-mimicking networks, small replacement networks that behave the same, with respect to matching problems involving a fixed set of terminals, as the larger network they replace.},
  journal = {arXiv:1802.00084 [cs]},
  author = {Eppstein, David and Vazirani, Vijay V.},
  month = jan,
  year = {2018},
  keywords = {Computer Science - Data Structures and Algorithms,F.2.2},
  file = {/Users/billlai/Zotero/storage/CP2LXH84/Eppstein and Vazirani - 2018 - NC Algorithms for Perfect Matching and Maximum Flo.pdf;/Users/billlai/Zotero/storage/IZB7QS35/1802.html}
}

@article{EickmeyerSuccinctnessOrderInvariantLogics2017,
  title = {Succinctness of {{Order}}-{{Invariant Logics}} on {{Depth}}-{{Bounded Structures}}},
  volume = {18},
  issn = {15293785},
  doi = {10.1145/3152770},
  language = {en},
  number = {4},
  journal = {ACM Transactions on Computational Logic},
  author = {Eickmeyer, Kord and Elberfeld, Michael and Harwath, Frederik},
  month = dec,
  year = {2017},
  pages = {1-25},
  file = {/Users/billlai/Zotero/storage/LEYNRSDD/Eickmeyer et al. - 2017 - Succinctness of Order-Invariant Logics on Depth-Bo.pdf}
}

@article{ElberfeldSpaceCircuitComplexity2015,
  title = {On the {{Space}} and {{Circuit Complexity}} of {{Parameterized Problems}}: {{Classes}} and {{Completeness}}},
  volume = {71},
  issn = {0178-4617, 1432-0541},
  shorttitle = {On the {{Space}} and {{Circuit Complexity}} of {{Parameterized Problems}}},
  doi = {10.1007/s00453-014-9944-y},
  abstract = {The parameterized complexity of a problem is generally considered ``settled'' once it has been shown to be fixed-parameter tractable or to be complete for a class in a parameterized hierarchy such as the weft hierarchy. Several natural parameterized problems have, however, resisted such a classification. In the present paper we argue that, in some cases, this is due to the fact that the parameterized complexity of these problems can be better understood in terms of their parameterized space or parameterized circuit complexity. This includes well-studied, natural problems like the feedback vertex set problem, the associative generability problem, or the longest common subsequence problem. We show that these problems lie in and may even be complete for different parameterized space classes, leading to new insights into the problems' complexity. The classes we study are defined in terms of different forms of bounded nondeterminism and simultaneous time\textendash{}space bounds.},
  language = {en},
  number = {3},
  journal = {Algorithmica},
  author = {Elberfeld, Michael and Stockhusen, Christoph and Tantau, Till},
  month = mar,
  year = {2015},
  pages = {661-701},
  file = {/Users/billlai/Zotero/storage/SKA4DN43/10.html}
}

@article{Caiprobabilityonerandom1989,
  title = {With Probability One, a Random Oracle Separates {{PSPACE}} from the Polynomial-Time Hierarchy},
  volume = {38},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(89)90033-0},
  abstract = {We consider how much error a fixed depth Boolean circuit must make in computing the parity function. We show that with an exponential bound of the form exp(n$\lambda$) on the size of the circuits, they make a 50\% error on all possible inputs, asymptotically and uniformly. As a consequence, we show that a random oracle set A separates PSPACE from the entire polynomial-time hierarchy with probability one.},
  number = {1},
  journal = {Journal of Computer and System Sciences},
  author = {Cai, Jin-Yi},
  month = feb,
  year = {1989},
  pages = {68-85},
  file = {/Users/billlai/Zotero/storage/GYNHPPC7/Cai - 1989 - With probability one, a random oracle separates PS.pdf;/Users/billlai/Zotero/storage/WTGNLNEP/0022000089900330.html}
}

@article{ImpagliazzoHardnessRandomnesssurvey,
  title = {Hardness as {{Randomness}} : A Survey of Universal Derandomization},
  abstract = {We survey recent developments in the study of probabilistic complexity classes. While the evidence seems to support the conjecture that probabilism can be deterministically simulated with relatively low overhead, i.e., that P = BP P , it also indicates that this may be a difficult question to resolve. In fact, proving that probalistic algorithms have non-trivial deterministic simulations is basically equivalent to proving circuit lower bounds, either in the algebraic or Boolean models.},
  language = {en},
  author = {Impagliazzo, Russell},
  pages = {11},
  file = {/Users/billlai/Zotero/storage/XRRCSXK6/Impagliazzo - Hardness as Randomness  a survey of universal der.pdf}
}

@article{BussPolynomialSizeProofs1987,
  title = {Polynomial {{Size Proofs}} of the {{Propositional Pigeonhole Principle}}},
  volume = {52},
  issn = {0022-4812},
  doi = {10.2307/2273826},
  abstract = {Cook and Reckhow defined a propositional formulation of the pigeonhole principle. This paper shows that there are Frege proofs of this propositional pigeonhole principle of polynomial size. This together with a result of Haken gives another proof of Urquhart's theorem that Frege systems have an exponential speedup over resolution. We also discuss connections to provability in theories of bounded arithmetic.},
  number = {4},
  journal = {The Journal of Symbolic Logic},
  author = {Buss, Samuel R.},
  year = {1987},
  pages = {916-927}
}

@book{BiereHandbookSatisfiability2009,
  title = {Handbook of {{Satisfiability}}},
  isbn = {978-1-58603-929-5},
  abstract = {A collection of papers on various theoretical and practical aspects of SAT solving. It is suitable for students and researchers.},
  language = {en},
  publisher = {{IOS Press}},
  author = {Biere, Armin},
  year = {2009},
  keywords = {Computers / Intelligence (AI) \& Semantics},
  file = {/Users/billlai/Zotero/storage/VW76BFFG/Biere - 2009 - Handbook of Satisfiability.pdf}
}

@phdthesis{LaiIncidencetreewidthresolution2017,
  address = {Shanghai, China},
  type = {{Bachelor Dissertation}},
  title = {{Incidence treewidth and resolution of CNF formulas}},
  language = {Chinese},
  school = {Fudan University},
  author = {Lai, Wenxing},
  year = {2017}
}

@article{KolaitisConjunctiveQueryContainmentConstraint2000,
  title = {Conjunctive-{{Query Containment}} and {{Constraint Satisfaction}}},
  volume = {61},
  issn = {0022-0000},
  doi = {10.1006/jcss.2000.1713},
  abstract = {Conjunctive-query containment is recognized as a fundamental problem in database query evaluation and optimization. At the same time, constraint satisfaction is recognized as a fundamental problem in artificial intelligence. What do conjunctive-query containment and constraint satisfaction have in common? Our main conceptual contribution in this paper is to point out that, despite their very different formulation, conjunctive-query containment and constraint satisfaction are essentially the same problem. The reason is that they can be recast as the following fundamental algebraic problem: given two finite relational structures A and B, is there a homomorphism h:A\textquestiondown{}B? As formulated above, the homomorphism problem is uniform in the sense that both relational structures A and B are part of the input. By fixing the structure B, one obtains the following nonuniform problem: given a finite relational structure A, is there a homomorphism h:A\textquestiondown{}B? In general, nonuniform tractability results do not uniformize. Thus, it is natural to ask: which tractable cases of nonuniform tractability results for constraint satisfaction and conjunctive-query containment do uniformize? Our main technical contribution in this paper is to show that several cases of tractable nonuniform constraint-satisfaction problems do indeed uniformize. We exhibit three nonuniform tractability results that uniformize and, thus, give rise to polynomial-time solvable cases of constraint satisfaction and conjunctive-query containment. We begin by examining the tractable cases of Boolean constraint-satisfaction problems and show that they do uniformize. This can be applied to conjunctive-query containment via Booleanization; in particular, it yields one of the known tractable cases of conjunctive-query containment. After this, we show that tractability results for constraint-satisfaction problems that can be expressed using Datalog programs with bounded number of distinct variables also uniformize. Finally, we provide a new proof for the fact that tractability results for queries with bounded treewidth uniformize as well, via a connection with first-order logic with a bounded number of distinct variables.},
  number = {2},
  journal = {J. Comput. Syst. Sci.},
  author = {Kolaitis, Phokion G. and Vardi, Moshe Y.},
  month = oct,
  year = {2000},
  pages = {302--332},
  file = {/Users/billlai/Zotero/storage/LU5HTPEJ/Kolaitis and Vardi - 2000 - Conjunctive-Query Containment and Constraint Satis.pdf}
}


